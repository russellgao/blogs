[{"categories":null,"content":"Alpine 容器内安装命令时遇到坑","date":"2021-02-19","objectID":"/linux-alpine-apk/","tags":["linux","alpine","apk","docker"],"title":"Alpine 容器内安装命令时遇到坑","uri":"/linux-alpine-apk/"},{"categories":null,"content":"导读 记录在 Alpine 容器内安装命令时遇到的一些问题，本篇文章会持续更新。 问题背景是在docker 容器内执行 docker 命令，执行时发现缺少某些依赖包，在安装依赖包时遇到一些难以解决的问题。 ","date":"2021-02-19","objectID":"/linux-alpine-apk/:1:0","tags":["linux","alpine","apk","docker"],"title":"Alpine 容器内安装命令时遇到坑","uri":"/linux-alpine-apk/"},{"categories":null,"content":"在容器内执行 docker 命令 在 docker 容器执行 docker 命令，如启动新的容器，需要把主机的 docker sock 套接字映射到容器内 。 具体方法为: docker run -v /usr/bin/docker:/bin/docker \\ -v /var/run/docker.sock:/var/run/docker.sock \\ xxx ","date":"2021-02-19","objectID":"/linux-alpine-apk/:2:0","tags":["linux","alpine","apk","docker"],"title":"Alpine 容器内安装命令时遇到坑","uri":"/linux-alpine-apk/"},{"categories":null,"content":"error while loading shared libraries: libltdl.so.7 如果执行 docker 命令报错如下: error while loading shared libraries: libltdl.so.7: cannot open shared object file: No such file or directory 说明缺少对应的依赖库，安装方法为 : apk add --no-cache libltdl 如果是 Centos 容器: yum install libtool-ltdl -y 如果是 Ubantu 容器: sudo apt-get update sudo apt-get install libltdl-dev ","date":"2021-02-19","objectID":"/linux-alpine-apk/:3:0","tags":["linux","alpine","apk","docker"],"title":"Alpine 容器内安装命令时遇到坑","uri":"/linux-alpine-apk/"},{"categories":null,"content":"2 errors; 50 MiB in 38 packages 如果安装 libltdl 时报错如下： apk add --no-cache libltdl fetch http://mirrors.ustc.edu.cn/alpine/v3.4/main/x86_64/APKINDEX.tar.gz fetch http://mirrors.ustc.edu.cn/alpine/v3.4/community/x86_64/APKINDEX.tar.gz 2 errors; 50 MiB in 38 packages 说明没有安装成功，需要继续定位错误方法。这时候可以执行 apk update 更新一下本地索引 : apk update fetch http://mirrors.ustc.edu.cn/alpine/v3.4/main/x86_64/APKINDEX.tar.gz ERROR: http://mirrors.ustc.edu.cn/alpine/v3.4/main: Bad file descriptor WARNING: Ignoring APKINDEX.0d9a6724.tar.gz: Bad file descriptor fetch http://mirrors.ustc.edu.cn/alpine/v3.4/community/x86_64/APKINDEX.tar.gz ERROR: http://mirrors.ustc.edu.cn/alpine/v3.4/community: Bad file descriptor WARNING: Ignoring APKINDEX.6a82a2a6.tar.gz: Bad file descriptor 2 errors; 38 distinct packages available 可以看到又报错了，这个错误说明文件描述符有问题 ，解决方法删除本地缓存目录并重新创建即可 : rm -fr /var/cache/apk mkdir -p mkdir /var/cache/apk 这时候再执行 apk update 就可以了 apk update -v fetch http://mirrors.ustc.edu.cn/alpine/v3.4/main/x86_64/APKINDEX.tar.gz fetch http://mirrors.ustc.edu.cn/alpine/v3.4/community/x86_64/APKINDEX.tar.gz v3.4.6-316-g63ea6d0 [http://mirrors.ustc.edu.cn/alpine/v3.4/main] v3.4.6-160-g14ad2a3 [http://mirrors.ustc.edu.cn/alpine/v3.4/community] OK: 5984 distinct packages available 再次执行 apk add --no-cache libltdl 如果还是报错则可以执行 apk fix 进行修复 : apk fix (1/2) Reinstalling busybox (1.24.2-r14) Executing busybox-1.24.2-r14.post-upgrade (2/2) [APK unavailable, skipped] Reinstalling glibc-bin (2.25-r0) Executing busybox-1.24.2-r14.trigger 1 errors; 164 MiB in 56 packages bash-4.3# bash-4.3# apk fix -v (1/1) [APK unavailable, skipped] Reinstalling glibc-bin (2.25-r0) 1 errors; 56 packages, 333 dirs, 7131 files, 164 MiB 从上面的信息可以看出，在 fix 阶段 fix glibc-bin 失败了，那就需要手动删除再重新安装了。 apk del glibc-bin World updated, but the following packages are not removed due to: glibc-bin: glibc-i18n 1 errors; 164 MiB in 56 packages 可以如果要删除 glibc-bin 需要先删除 glibc-i18n apk del glibc-i18n (1/2) Purging glibc-i18n (2.25-r0) (2/2) Purging glibc-bin (2.25-r0) OK: 151 MiB in 54 packages apk del glibc-bin 这时候重新 fix : apk fix -v OK: 54 packages, 256 dirs, 6543 files, 151 MiB 再次安装 libltdl apk add -v --no-cache libltdl fetch http://mirrors.ustc.edu.cn/alpine/v3.4/main/x86_64/APKINDEX.tar.gz fetch http://mirrors.ustc.edu.cn/alpine/v3.4/community/x86_64/APKINDEX.tar.gz OK: 54 packages, 256 dirs, 6543 files, 151 MiB 可以看到问题完美解决 。 ","date":"2021-02-19","objectID":"/linux-alpine-apk/:4:0","tags":["linux","alpine","apk","docker"],"title":"Alpine 容器内安装命令时遇到坑","uri":"/linux-alpine-apk/"},{"categories":null,"content":"总结 记录在 alpine 容器内安装库时遇到的坑，此篇文章持续更新 。 ","date":"2021-02-19","objectID":"/linux-alpine-apk/:5:0","tags":["linux","alpine","apk","docker"],"title":"Alpine 容器内安装命令时遇到坑","uri":"/linux-alpine-apk/"},{"categories":null,"content":"交叉打印","date":"2021-02-18","objectID":"/argorithm/go-goroutine-print/","tags":["多线程","goroutine","go"],"title":"交叉打印","uri":"/argorithm/go-goroutine-print/"},{"categories":null,"content":"交叉打印 数字和字母交叉打印，打印两个字母，接着打印一个数字，再接着打印两个字母，一直从 a 打印到 z，以字母结束。输出示例： a b 1 c d 2 ... z 用 go语言的多线程实现 ","date":"2021-02-18","objectID":"/argorithm/go-goroutine-print/:0:0","tags":["多线程","goroutine","go"],"title":"交叉打印","uri":"/argorithm/go-goroutine-print/"},{"categories":null,"content":"实现原理 用两个 goroutine 实现，一个打印字母，一个打印数字，通过一个 chan 控制打印顺序。 ","date":"2021-02-18","objectID":"/argorithm/go-goroutine-print/:1:0","tags":["多线程","goroutine","go"],"title":"交叉打印","uri":"/argorithm/go-goroutine-print/"},{"categories":null,"content":"实现 package main import \"fmt\" func main() { done := make(chan bool) c := make(chan bool) go func() { defer func() { done \u003c- true }() letter := []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"} for i, item := range letter { if i%2 == 0 || i == 25 { c \u003c- false } else { c \u003c- true } fmt.Println(item) } close(c) }() go func() { count := 1 for item := range c { if item { fmt.Println(count) count++ } } }() \u003c-done } ","date":"2021-02-18","objectID":"/argorithm/go-goroutine-print/:2:0","tags":["多线程","goroutine","go"],"title":"交叉打印","uri":"/argorithm/go-goroutine-print/"},{"categories":null,"content":"交叉打印","date":"2021-02-18","objectID":"/go-goroutine-print/","tags":["多线程","goroutine","go"],"title":"交叉打印","uri":"/go-goroutine-print/"},{"categories":null,"content":"交叉打印 数字和字母交叉打印，打印两个字母，接着打印一个数字，再接着打印两个字母，一直从 a 打印到 z，以字母结束。输出示例： a b 1 c d 2 ... z 用 go语言的多线程实现 ","date":"2021-02-18","objectID":"/go-goroutine-print/:0:0","tags":["多线程","goroutine","go"],"title":"交叉打印","uri":"/go-goroutine-print/"},{"categories":null,"content":"实现原理 用两个 goroutine 实现，一个打印字母，一个打印数字，通过一个 chan 控制打印顺序。 ","date":"2021-02-18","objectID":"/go-goroutine-print/:1:0","tags":["多线程","goroutine","go"],"title":"交叉打印","uri":"/go-goroutine-print/"},{"categories":null,"content":"实现 package main import \"fmt\" func main() { done := make(chan bool) c := make(chan bool) go func() { defer func() { done \u003c- true }() letter := []string{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"} for i, item := range letter { if i%2 == 0 || i == 25 { c \u003c- false } else { c \u003c- true } fmt.Println(item) } close(c) }() go func() { count := 1 for item := range c { if item { fmt.Println(count) count++ } } }() \u003c-done } ","date":"2021-02-18","objectID":"/go-goroutine-print/:2:0","tags":["多线程","goroutine","go"],"title":"交叉打印","uri":"/go-goroutine-print/"},{"categories":null,"content":"在本地如何玩转kubernetes? - kind","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"导读 kubernetes 现在已经走进了大众的视野，很多同学都对此比较好奇，从其他渠道或多或少都了解了一些，但是苦于没有kubernetes环境，不能身临其境的感受， 毕竟如果完整搭建一套kubernetes环境是需要资源的。 今天介绍一款工具（kind），让大家可以本地也可以构建起 kubernetes 环境，愉快的在本地玩转 kubernetes。 kind 全称 是 kubernetes in docker ，把 kubernetes 控制面的组件全部运行在一个docker 容器中，在本地通过 127.0.0.1 进行通信。这种玩法只能在本地体验， 不可用于生产环境，特别适用于新人在本地体验、开发 kubernetes 相关组件时在本地进行调试等，如开始 operator 时可以在 kind 进行调试 。 详细用法可以参考官方文档。 ","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:1:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"kind 参考资料 官网: https://kind.sigs.k8s.io/ 官方文档: https://kind.sigs.k8s.io/docs/user/quick-start/ github : https://github.com/kubernetes-sigs/kind ","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:2:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"安装 ","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:3:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"有 golang 环境 如果有 golang 环境，可以通过如下命令安装 : GO111MODULE=\"on\" go get sigs.k8s.io/kind@v0.10.0 如果下载的比较慢可以设置代理，增加一个环境变量即可: GOPROXY=\"https://goproxy.cn\" ","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:3:1","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"无 golang 环境 Linux : curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind Mac (homebrew) brew install kind 或者 : curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-darwin-amd64 Windows: curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.10.0/kind-windows-amd64 Move-Item .\\kind-windows-amd64.exe c:\\some-dir-in-your-PATH\\kind.exe ","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:3:2","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"基本命令 通过 kind --help 看看支持哪些命令 kind --help kind creates and manages local Kubernetes clusters using Docker container 'nodes' Usage: kind [command] Available Commands: build Build one of [node-image] completion Output shell completion code for the specified shell (bash, zsh or fish) create Creates one of [cluster] delete Deletes one of [cluster] export Exports one of [kubeconfig, logs] get Gets one of [clusters, nodes, kubeconfig] help Help about any command load Loads images into nodes version Prints the kind CLI version Flags: -h, --help help for kind --loglevel string DEPRECATED: see -v instead -q, --quiet silence all stderr output -v, --verbosity int32 info log verbosity --version version for kind Use \"kind [command] --help\" for more information about a command. 可以看出支持3种类型的命令，cluster 相关、image 相关、通用命令 。 cluster 相关的有 create, delete 等，主要用于创建和删除 kubernetes 集群。 image 相关的有 build， load 等，主要用于本地调试时，本地可以 build镜像直接load 到集群中，而不需要推送到镜像仓库再通过集群去 pull 。 通用命令如 get ，version 等。 kind –version kind --version kind version 0.9.0 本篇文章以 kind 0.9.0 进行介绍 。下面是比较精彩的部分，仔细看哦 👀 ","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:4:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"创建 kubernetes 集群 创建一个 kubernetes 集群 ： kind create cluster Creating cluster \"kind\" ... ✓ Ensuring node image (kindest/node:v1.19.1) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Thanks for using kind! 😊 一条命令就已经启动好了一个集群 ，可以通过 kind get clusters 查看已经创建的集群。 kind get clusters kind 既然是 kubernetes in docker ，那就看看启动了哪些容器 : docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fdb88a476bb0 kindest/node:v1.19.1 \"/usr/local/bin/entr…\" 3 minutes ago Up 2 minutes 127.0.0.1:43111-\u003e6443/tcp kind-control-plane 可以看到有一个控制面的容器启动了，进到容器中看看都有什么 [root@iZuf685opgs9oyozju9i2bZ ~]# docker exec -it kind-control-plane bash root@kind-control-plane:/# root@kind-control-plane:/# root@kind-control-plane:/# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 02:49 ? 00:00:00 /sbin/init root 126 1 0 02:49 ? 00:00:00 /lib/systemd/systemd-journald root 145 1 1 02:49 ? 00:00:06 /usr/local/bin/containerd root 257 1 0 02:49 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id c1a5e2c868b9a744f4f78a85a8d660950bb76103a38e7 root 271 1 0 02:49 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 3549ecade28e2dccbad5ed15a4cd2b6e6a886cd3e10ab root 297 1 0 02:49 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 379ed27442f35696d488dd5a63cc61dc474bfa9bd08a9 root 335 1 0 02:49 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id e4eae33bf489c617c7133ada7dbd92129f3f817cb74b7 root 343 271 0 02:49 ? 00:00:00 /pause root 360 257 0 02:49 ? 00:00:00 /pause root 365 297 0 02:49 ? 00:00:00 /pause root 377 335 0 02:49 ? 00:00:00 /pause root 443 335 0 02:49 ? 00:00:01 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/ root 468 297 4 02:49 ? 00:00:17 kube-apiserver --advertise-address=172.18.0.2 --allow-privileged=true --authorization-mode=Node,RBAC --cli root 496 271 1 02:49 ? 00:00:05 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller- root 540 257 1 02:49 ? 00:00:05 etcd --advertise-client-urls=https://172.18.0.2:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --cli root 580 1 1 02:49 ? 00:00:06 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernete root 673 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id b0965a6f77f58c46cfe7b30dd84ddf4bc37516ba60e6e root 695 673 0 02:50 ? 00:00:00 /pause root 709 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id aedf0f1fd02baf1cf2b253ad11e33e396d97cc7c53114 root 738 709 0 02:50 ? 00:00:00 /pause root 789 673 0 02:50 ? 00:00:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=kind-control-plane root 798 709 0 02:50 ? 00:00:00 /bin/kindnetd root 1011 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id aa554aa998c3091a70eacbc3e4a2f275a1e680a585d69 root 1024 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 7373488f811fc5d638c2b3f5f79d953573e30a42ff52f root 1048 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 5ab6c3ef1715623e2c28fbfdecd5f4e6e2616fc20a387 root 1079 1011 0 02:50 ? 00:00:00 /pause root 1088 1024 0 02:50 ? 00:00:00 /pause root 1095 1048 0 02:50 ? 00:00:00 /pause root 1152 1011 0 02:50 ? 00:00:00 /coredns -conf /etc/coredns/Corefile root 1196 1024 0 02:50 ? 00:00:00 /coredns -conf /etc/coredns/Corefile root 1205 1048 0 02:50 ? 00:00:00 local-path-provisioner --debug start --helper-image k8s.gcr.io/build-image/debian-base:v2.1.0 --config /et root 1961 0 0 02:56 pts/1 00:00:00 bash root 1969 1961 0 02:56 pts/1 00:00:00 ps -ef root@kind-control-plane:/# 可以看到容器中有很多进程，仔细梳理一下看看有什么组件 kube-apiserver … : api-serve","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:5:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"使用集群 关于 kubernetes 的使用已经有很多文章来介绍了，所以这里不作为重点介绍，简单演示一下。可以通过 api 或者 kubectl 与kuberntes 进行交互， 这里选择用 kubectl 进行演示。 如果本地没有 kubectl 需要进行安装，安装文档参见： https://kubernetes.io/docs/tasks/tools/install-kubectl/ kubectl 的基本用法可以参考我之前的文章 ：kubectl 常用命令 以 部署 logstash 为例，我们会创建如下资源 ： Namespace Deployment Configmap Hpa Service 具体的 yaml 文件如下 : cat logstash.yaml ---# setting NamespaceapiVersion:v1kind:Namespacemetadata:name:logging---# setting ConfigMapkind:ConfigMapapiVersion:v1metadata:name:logstash-confnamespace:loggingdata:logstash.conf:| input {http{host=\u003e\"0.0.0.0\"# default: 0.0.0.0port =\u003e 8080 # default:8080response_headers=\u003e{\"Content-Type\"=\u003e\"text/plain\"\"Access-Control-Allow-Origin\"=\u003e\"*\"\"Access-Control-Allow-Methods\"=\u003e\"GET, POST, DELETE, PUT\"\"Access-Control-Allow-Headers\"=\u003e\"authorization, content-type\"\"Access-Control-Allow-Credentials\"=\u003etrue}}}output{stdout{codec=\u003erubydebug}}---# setting DepolymentapiVersion:apps/v1kind:Deploymentmetadata:name:logstashnamespace:loggingspec:replicas:1selector:matchLabels:app:logstashtemplate:metadata:labels:app:logstashspec:volumes:- name:configconfigMap:name:logstash-confhostname:logstashcontainers:- name:logstashimage:russellgao/logstash:7.2.0args:[\"-f\",\"/usr/share/logstash/pipeline/logstash.conf\",]imagePullPolicy:IfNotPresentvolumeMounts:- name:configmountPath:\"/usr/share/logstash/pipeline/logstash.conf\"readOnly:truesubPath:logstash.confresources:requests:cpu:1memory:2048Milimits:cpu:3memory:3072MireadinessProbe:tcpSocket:port:8080initialDelaySeconds:5periodSeconds:10livenessProbe:tcpSocket:port:8080initialDelaySeconds:15periodSeconds:20timeoutSeconds:15imagePullSecrets:- name:harbor---apiVersion:autoscaling/v2beta1kind:HorizontalPodAutoscalermetadata:name:logstash-hpanamespace:loggingspec:scaleTargetRef:apiVersion:apps/v1beta2kind:Deploymentname:logstashminReplicas:1maxReplicas:10metrics:- type:Resourceresource:name:cputargetAverageUtilization:80---apiVersion:v1kind:Servicemetadata:name:logstash-custeripnamespace:loggingspec:selector:app:logstashtype:ClusterIPports:- name:'port'protocol:TCPport:8080targetPort:8080 执行 kubectl apply -f logstash.yaml kubectl apply -f logstash.yaml namespace/logging created configmap/logstash-conf created deployment.apps/logstash created horizontalpodautoscaler.autoscaling/logstash-hpa created service/logstash-custerip created 可以看到具体的资源已经被创建出来，下面来观察具体的资源 : 查看 ConfigMap : kubectl -n logging get configmap NAME DATA AGE logstash-conf 1 4m 查看 Deployment : kubectl -n logging get deployment NAME READY UP-TO-DATE AVAILABLE AGE logstash 1/1 1 1 4m 查看 Pod : kubectl -n logging get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES logstash-64d58c4b98-nqk4v 1/1 Running 0 93s 10.244.0.9 kind-control-plane \u003cnone\u003e \u003cnone\u003e 这里需要注意的是 Pod 所在的 node 是 kind-control-plane ，而非本机，说明 kubernetes node 就是这个容器，在本地 curl 10.244.0.9:8080 这个地址是不通，说明是在集群外， 进到容器内再 curl 就是通的 ： curl 10.244.0.9:8080 -v * About to connect() to 10.244.0.9 port 8080 (#0) * Trying 10.244.0.9... ^C [root@iZuf685opgs9oyozju9i2bZ k8s]# [root@iZuf685opgs9oyozju9i2bZ k8s]# [root@iZuf685opgs9oyozju9i2bZ k8s]# docker exec -it kind-control-plane bash root@kind-control-plane:/# curl 10.244.0.9:8080 -v * Trying 10.244.0.9:8080... * TCP_NODELAY set * Connected to 10.244.0.9 (10.244.0.9) port 8080 (#0) \u003e GET / HTTP/1.1 \u003e Host: 10.244.0.9:8080 \u003e User-Agent: curl/7.68.0 \u003e Accept: */* \u003e * Mark bundle as not supporting multiuse \u003c HTTP/1.1 200 OK \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Methods: GET, POST, DELETE, PUT \u003c Access-Control-Allow-Headers: authorization, content-type \u003c Access-Control-Allow-Credentials: true \u003c content-length: 2 \u003c content-type: text/plain \u003c * Connection #0 to host 10.244.0.9 left intact okroot@kind-control-plane:/# 查看 service : kubectl -n logging get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE logstash-custerip ClusterIP 10.96.234.144 \u003cnone\u003e 8080/TCP 5m pod 和 service 的原理是一样的，通过 CLUSTER-IP 访问只能在容器内进行访问。 在 pod 内进行访问 [root@iZuf685opgs9oyozju9i2bZ k8s]# kubectl -n logging exec -it logst","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:6:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"环境清理 在本地体验完或者测试完成之后，为了节省资源，可以把刚刚启动的集群进行删除，下次需要时再创建即可 。 kind delete cluster Deleting cluster \"kind\" ... [root@iZuf685opgs9oyozju9i2bZ k8s]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4ec800c3ec10 russellgao/openresty:1.17.8.2-5-alpine \"/usr/local/openrest…\" 8 weeks ago Up 7 days 0.0.0.0:80-\u003e80/tcp, 0.0.0.0:443-\u003e443/tcp openresty-app-1 [root@iZuf685opgs9oyozju9i2bZ k8s]# kubectl -n logging get po The connection to the server localhost:8080 was refused - did you specify the right host or port? 通过上面的命令可以看出 : 当执行 kind delete cluster 命令之后会把控制面的容器(kind-control-plane) 删除 当再次执行 kubectl 命令是已经无法找到对应的 api-server地址，可以查看 .kube/config 文件，发现已经删除了关于集群的配置信息。 ","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:7:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"总结 本篇介绍了 kind(kubernetes in docker) 的基本用法，可以在本地快速构建起kubernetes 环境，适合新人快速入门、调试k8s 相关组件，测试operator 等。 ","date":"2021-02-06","objectID":"/kubernetes/k8s-kind/:8:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/kubernetes/k8s-kind/"},{"categories":null,"content":"在本地如何玩转kubernetes? - kind","date":"2021-02-06","objectID":"/k8s-kind/","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"导读 kubernetes 现在已经走进了大众的视野，很多同学都对此比较好奇，从其他渠道或多或少都了解了一些，但是苦于没有kubernetes环境，不能身临其境的感受， 毕竟如果完整搭建一套kubernetes环境是需要资源的。 今天介绍一款工具（kind），让大家可以本地也可以构建起 kubernetes 环境，愉快的在本地玩转 kubernetes。 kind 全称 是 kubernetes in docker ，把 kubernetes 控制面的组件全部运行在一个docker 容器中，在本地通过 127.0.0.1 进行通信。这种玩法只能在本地体验， 不可用于生产环境，特别适用于新人在本地体验、开发 kubernetes 相关组件时在本地进行调试等，如开始 operator 时可以在 kind 进行调试 。 详细用法可以参考官方文档。 ","date":"2021-02-06","objectID":"/k8s-kind/:1:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"kind 参考资料 官网: https://kind.sigs.k8s.io/ 官方文档: https://kind.sigs.k8s.io/docs/user/quick-start/ github : https://github.com/kubernetes-sigs/kind ","date":"2021-02-06","objectID":"/k8s-kind/:2:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"安装 ","date":"2021-02-06","objectID":"/k8s-kind/:3:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"有 golang 环境 如果有 golang 环境，可以通过如下命令安装 : GO111MODULE=\"on\" go get sigs.k8s.io/kind@v0.10.0 如果下载的比较慢可以设置代理，增加一个环境变量即可: GOPROXY=\"https://goproxy.cn\" ","date":"2021-02-06","objectID":"/k8s-kind/:3:1","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"无 golang 环境 Linux : curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind Mac (homebrew) brew install kind 或者 : curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-darwin-amd64 Windows: curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.10.0/kind-windows-amd64 Move-Item .\\kind-windows-amd64.exe c:\\some-dir-in-your-PATH\\kind.exe ","date":"2021-02-06","objectID":"/k8s-kind/:3:2","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"基本命令 通过 kind --help 看看支持哪些命令 kind --help kind creates and manages local Kubernetes clusters using Docker container 'nodes' Usage: kind [command] Available Commands: build Build one of [node-image] completion Output shell completion code for the specified shell (bash, zsh or fish) create Creates one of [cluster] delete Deletes one of [cluster] export Exports one of [kubeconfig, logs] get Gets one of [clusters, nodes, kubeconfig] help Help about any command load Loads images into nodes version Prints the kind CLI version Flags: -h, --help help for kind --loglevel string DEPRECATED: see -v instead -q, --quiet silence all stderr output -v, --verbosity int32 info log verbosity --version version for kind Use \"kind [command] --help\" for more information about a command. 可以看出支持3种类型的命令，cluster 相关、image 相关、通用命令 。 cluster 相关的有 create, delete 等，主要用于创建和删除 kubernetes 集群。 image 相关的有 build， load 等，主要用于本地调试时，本地可以 build镜像直接load 到集群中，而不需要推送到镜像仓库再通过集群去 pull 。 通用命令如 get ，version 等。 kind –version kind --version kind version 0.9.0 本篇文章以 kind 0.9.0 进行介绍 。下面是比较精彩的部分，仔细看哦 👀 ","date":"2021-02-06","objectID":"/k8s-kind/:4:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"创建 kubernetes 集群 创建一个 kubernetes 集群 ： kind create cluster Creating cluster \"kind\" ... ✓ Ensuring node image (kindest/node:v1.19.1) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Thanks for using kind! 😊 一条命令就已经启动好了一个集群 ，可以通过 kind get clusters 查看已经创建的集群。 kind get clusters kind 既然是 kubernetes in docker ，那就看看启动了哪些容器 : docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fdb88a476bb0 kindest/node:v1.19.1 \"/usr/local/bin/entr…\" 3 minutes ago Up 2 minutes 127.0.0.1:43111-\u003e6443/tcp kind-control-plane 可以看到有一个控制面的容器启动了，进到容器中看看都有什么 [root@iZuf685opgs9oyozju9i2bZ ~]# docker exec -it kind-control-plane bash root@kind-control-plane:/# root@kind-control-plane:/# root@kind-control-plane:/# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 02:49 ? 00:00:00 /sbin/init root 126 1 0 02:49 ? 00:00:00 /lib/systemd/systemd-journald root 145 1 1 02:49 ? 00:00:06 /usr/local/bin/containerd root 257 1 0 02:49 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id c1a5e2c868b9a744f4f78a85a8d660950bb76103a38e7 root 271 1 0 02:49 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 3549ecade28e2dccbad5ed15a4cd2b6e6a886cd3e10ab root 297 1 0 02:49 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 379ed27442f35696d488dd5a63cc61dc474bfa9bd08a9 root 335 1 0 02:49 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id e4eae33bf489c617c7133ada7dbd92129f3f817cb74b7 root 343 271 0 02:49 ? 00:00:00 /pause root 360 257 0 02:49 ? 00:00:00 /pause root 365 297 0 02:49 ? 00:00:00 /pause root 377 335 0 02:49 ? 00:00:00 /pause root 443 335 0 02:49 ? 00:00:01 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/ root 468 297 4 02:49 ? 00:00:17 kube-apiserver --advertise-address=172.18.0.2 --allow-privileged=true --authorization-mode=Node,RBAC --cli root 496 271 1 02:49 ? 00:00:05 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller- root 540 257 1 02:49 ? 00:00:05 etcd --advertise-client-urls=https://172.18.0.2:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --cli root 580 1 1 02:49 ? 00:00:06 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernete root 673 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id b0965a6f77f58c46cfe7b30dd84ddf4bc37516ba60e6e root 695 673 0 02:50 ? 00:00:00 /pause root 709 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id aedf0f1fd02baf1cf2b253ad11e33e396d97cc7c53114 root 738 709 0 02:50 ? 00:00:00 /pause root 789 673 0 02:50 ? 00:00:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=kind-control-plane root 798 709 0 02:50 ? 00:00:00 /bin/kindnetd root 1011 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id aa554aa998c3091a70eacbc3e4a2f275a1e680a585d69 root 1024 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 7373488f811fc5d638c2b3f5f79d953573e30a42ff52f root 1048 1 0 02:50 ? 00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 5ab6c3ef1715623e2c28fbfdecd5f4e6e2616fc20a387 root 1079 1011 0 02:50 ? 00:00:00 /pause root 1088 1024 0 02:50 ? 00:00:00 /pause root 1095 1048 0 02:50 ? 00:00:00 /pause root 1152 1011 0 02:50 ? 00:00:00 /coredns -conf /etc/coredns/Corefile root 1196 1024 0 02:50 ? 00:00:00 /coredns -conf /etc/coredns/Corefile root 1205 1048 0 02:50 ? 00:00:00 local-path-provisioner --debug start --helper-image k8s.gcr.io/build-image/debian-base:v2.1.0 --config /et root 1961 0 0 02:56 pts/1 00:00:00 bash root 1969 1961 0 02:56 pts/1 00:00:00 ps -ef root@kind-control-plane:/# 可以看到容器中有很多进程，仔细梳理一下看看有什么组件 kube-apiserver … : api-serve","date":"2021-02-06","objectID":"/k8s-kind/:5:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"使用集群 关于 kubernetes 的使用已经有很多文章来介绍了，所以这里不作为重点介绍，简单演示一下。可以通过 api 或者 kubectl 与kuberntes 进行交互， 这里选择用 kubectl 进行演示。 如果本地没有 kubectl 需要进行安装，安装文档参见： https://kubernetes.io/docs/tasks/tools/install-kubectl/ kubectl 的基本用法可以参考我之前的文章 ：kubectl 常用命令 以 部署 logstash 为例，我们会创建如下资源 ： Namespace Deployment Configmap Hpa Service 具体的 yaml 文件如下 : cat logstash.yaml ---# setting NamespaceapiVersion:v1kind:Namespacemetadata:name:logging---# setting ConfigMapkind:ConfigMapapiVersion:v1metadata:name:logstash-confnamespace:loggingdata:logstash.conf:| input {http{host=\u003e\"0.0.0.0\"# default: 0.0.0.0port =\u003e 8080 # default:8080response_headers=\u003e{\"Content-Type\"=\u003e\"text/plain\"\"Access-Control-Allow-Origin\"=\u003e\"*\"\"Access-Control-Allow-Methods\"=\u003e\"GET, POST, DELETE, PUT\"\"Access-Control-Allow-Headers\"=\u003e\"authorization, content-type\"\"Access-Control-Allow-Credentials\"=\u003etrue}}}output{stdout{codec=\u003erubydebug}}---# setting DepolymentapiVersion:apps/v1kind:Deploymentmetadata:name:logstashnamespace:loggingspec:replicas:1selector:matchLabels:app:logstashtemplate:metadata:labels:app:logstashspec:volumes:- name:configconfigMap:name:logstash-confhostname:logstashcontainers:- name:logstashimage:russellgao/logstash:7.2.0args:[\"-f\",\"/usr/share/logstash/pipeline/logstash.conf\",]imagePullPolicy:IfNotPresentvolumeMounts:- name:configmountPath:\"/usr/share/logstash/pipeline/logstash.conf\"readOnly:truesubPath:logstash.confresources:requests:cpu:1memory:2048Milimits:cpu:3memory:3072MireadinessProbe:tcpSocket:port:8080initialDelaySeconds:5periodSeconds:10livenessProbe:tcpSocket:port:8080initialDelaySeconds:15periodSeconds:20timeoutSeconds:15imagePullSecrets:- name:harbor---apiVersion:autoscaling/v2beta1kind:HorizontalPodAutoscalermetadata:name:logstash-hpanamespace:loggingspec:scaleTargetRef:apiVersion:apps/v1beta2kind:Deploymentname:logstashminReplicas:1maxReplicas:10metrics:- type:Resourceresource:name:cputargetAverageUtilization:80---apiVersion:v1kind:Servicemetadata:name:logstash-custeripnamespace:loggingspec:selector:app:logstashtype:ClusterIPports:- name:'port'protocol:TCPport:8080targetPort:8080 执行 kubectl apply -f logstash.yaml kubectl apply -f logstash.yaml namespace/logging created configmap/logstash-conf created deployment.apps/logstash created horizontalpodautoscaler.autoscaling/logstash-hpa created service/logstash-custerip created 可以看到具体的资源已经被创建出来，下面来观察具体的资源 : 查看 ConfigMap : kubectl -n logging get configmap NAME DATA AGE logstash-conf 1 4m 查看 Deployment : kubectl -n logging get deployment NAME READY UP-TO-DATE AVAILABLE AGE logstash 1/1 1 1 4m 查看 Pod : kubectl -n logging get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES logstash-64d58c4b98-nqk4v 1/1 Running 0 93s 10.244.0.9 kind-control-plane \u003cnone\u003e \u003cnone\u003e 这里需要注意的是 Pod 所在的 node 是 kind-control-plane ，而非本机，说明 kubernetes node 就是这个容器，在本地 curl 10.244.0.9:8080 这个地址是不通，说明是在集群外， 进到容器内再 curl 就是通的 ： curl 10.244.0.9:8080 -v * About to connect() to 10.244.0.9 port 8080 (#0) * Trying 10.244.0.9... ^C [root@iZuf685opgs9oyozju9i2bZ k8s]# [root@iZuf685opgs9oyozju9i2bZ k8s]# [root@iZuf685opgs9oyozju9i2bZ k8s]# docker exec -it kind-control-plane bash root@kind-control-plane:/# curl 10.244.0.9:8080 -v * Trying 10.244.0.9:8080... * TCP_NODELAY set * Connected to 10.244.0.9 (10.244.0.9) port 8080 (#0) \u003e GET / HTTP/1.1 \u003e Host: 10.244.0.9:8080 \u003e User-Agent: curl/7.68.0 \u003e Accept: */* \u003e * Mark bundle as not supporting multiuse \u003c HTTP/1.1 200 OK \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Methods: GET, POST, DELETE, PUT \u003c Access-Control-Allow-Headers: authorization, content-type \u003c Access-Control-Allow-Credentials: true \u003c content-length: 2 \u003c content-type: text/plain \u003c * Connection #0 to host 10.244.0.9 left intact okroot@kind-control-plane:/# 查看 service : kubectl -n logging get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE logstash-custerip ClusterIP 10.96.234.144 \u003cnone\u003e 8080/TCP 5m pod 和 service 的原理是一样的，通过 CLUSTER-IP 访问只能在容器内进行访问。 在 pod 内进行访问 [root@iZuf685opgs9oyozju9i2bZ k8s]# kubectl -n logging exec -it logst","date":"2021-02-06","objectID":"/k8s-kind/:6:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"环境清理 在本地体验完或者测试完成之后，为了节省资源，可以把刚刚启动的集群进行删除，下次需要时再创建即可 。 kind delete cluster Deleting cluster \"kind\" ... [root@iZuf685opgs9oyozju9i2bZ k8s]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4ec800c3ec10 russellgao/openresty:1.17.8.2-5-alpine \"/usr/local/openrest…\" 8 weeks ago Up 7 days 0.0.0.0:80-\u003e80/tcp, 0.0.0.0:443-\u003e443/tcp openresty-app-1 [root@iZuf685opgs9oyozju9i2bZ k8s]# kubectl -n logging get po The connection to the server localhost:8080 was refused - did you specify the right host or port? 通过上面的命令可以看出 : 当执行 kind delete cluster 命令之后会把控制面的容器(kind-control-plane) 删除 当再次执行 kubectl 命令是已经无法找到对应的 api-server地址，可以查看 .kube/config 文件，发现已经删除了关于集群的配置信息。 ","date":"2021-02-06","objectID":"/k8s-kind/:7:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"总结 本篇介绍了 kind(kubernetes in docker) 的基本用法，可以在本地快速构建起kubernetes 环境，适合新人快速入门、调试k8s 相关组件，测试operator 等。 ","date":"2021-02-06","objectID":"/k8s-kind/:8:0","tags":["kubernetes","kind"],"title":"在本地如何玩转kubernetes? - kind","uri":"/k8s-kind/"},{"categories":null,"content":"Python 中的装饰器","date":"2021-01-30","objectID":"/python-decorator/","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python-decorator/"},{"categories":null,"content":"导读 这篇文章主要介绍了 python 当中的装饰器。 ","date":"2021-01-30","objectID":"/python-decorator/:1:0","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python-decorator/"},{"categories":null,"content":"什么是装饰器 装饰器可以理解为函数的函数，想想这么一种场景，要计算每个函数的执行时间，一种解决方法是在每个函数中做个计时，就可以拿到执行时间， 但是这样会有大量的冗余代码，显然是不可取的，这时候装饰器就派上用场了。 下面直接看代码，代码比文字更有说服力。 ","date":"2021-01-30","objectID":"/python-decorator/:2:0","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python-decorator/"},{"categories":null,"content":"装饰器有哪些类型 ","date":"2021-01-30","objectID":"/python-decorator/:3:0","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python-decorator/"},{"categories":null,"content":"无参装饰器 装饰器本身没有任务参数，通过 @ 直接使用即可。 import time from datetime import datetime def timet(func) : print(\"otter=====\") def inner(*args, **kwargs) : start_time = datetime.now() print(\"inner====\") func(*args, **kwargs) end_time = datetime.now() print(\"total cost {}\".format(end_time - start_time)) return inner @timet def p_p1(a1,a2,a3 = \"pkfjf\") : print(\"=====\") print(a1) print(a2) print(a3) print(\"======\") time.sleep(4) p_p1(\"a1111111\",\"a22222\") ","date":"2021-01-30","objectID":"/python-decorator/:3:1","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python-decorator/"},{"categories":null,"content":"有参装饰器 装饰器本身还有参数，在使用时可以传参控制。 import time from datetime import datetime def timet(*args, **kwargs) : print(\"装饰器参数 =====\") for item_a in args : print(item_a) for item_k ,item_v in kwargs.items() : print(item_k,item_v) print(\"装饰器参数------\") def outter(func) : print(\"otter=====\") def inner(*args, **kwargs) : start_time = datetime.now() print(\"inner====\") func(*args, **kwargs) end_time = datetime.now() print(\"total cost {}\".format(end_time - start_time)) return inner return outter @timet(\"p1\",\"p2\",p3=\"oirei\",p4=\"uryhd\") def p_p1(a1,a2,a3 = \"pkfjf\") : print(\"=====\") print(a1) print(a2) print(a3) print(\"======\") time.sleep(4) p_p1(\"a1111111\",\"a22222\") ","date":"2021-01-30","objectID":"/python-decorator/:3:2","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python-decorator/"},{"categories":null,"content":"Python 中的装饰器","date":"2021-01-30","objectID":"/python/python-decorator/","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python/python-decorator/"},{"categories":null,"content":"导读 这篇文章主要介绍了 python 当中的装饰器。 ","date":"2021-01-30","objectID":"/python/python-decorator/:1:0","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python/python-decorator/"},{"categories":null,"content":"什么是装饰器 装饰器可以理解为函数的函数，想想这么一种场景，要计算每个函数的执行时间，一种解决方法是在每个函数中做个计时，就可以拿到执行时间， 但是这样会有大量的冗余代码，显然是不可取的，这时候装饰器就派上用场了。 下面直接看代码，代码比文字更有说服力。 ","date":"2021-01-30","objectID":"/python/python-decorator/:2:0","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python/python-decorator/"},{"categories":null,"content":"装饰器有哪些类型 ","date":"2021-01-30","objectID":"/python/python-decorator/:3:0","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python/python-decorator/"},{"categories":null,"content":"无参装饰器 装饰器本身没有任务参数，通过 @ 直接使用即可。 import time from datetime import datetime def timet(func) : print(\"otter=====\") def inner(*args, **kwargs) : start_time = datetime.now() print(\"inner====\") func(*args, **kwargs) end_time = datetime.now() print(\"total cost {}\".format(end_time - start_time)) return inner @timet def p_p1(a1,a2,a3 = \"pkfjf\") : print(\"=====\") print(a1) print(a2) print(a3) print(\"======\") time.sleep(4) p_p1(\"a1111111\",\"a22222\") ","date":"2021-01-30","objectID":"/python/python-decorator/:3:1","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python/python-decorator/"},{"categories":null,"content":"有参装饰器 装饰器本身还有参数，在使用时可以传参控制。 import time from datetime import datetime def timet(*args, **kwargs) : print(\"装饰器参数 =====\") for item_a in args : print(item_a) for item_k ,item_v in kwargs.items() : print(item_k,item_v) print(\"装饰器参数------\") def outter(func) : print(\"otter=====\") def inner(*args, **kwargs) : start_time = datetime.now() print(\"inner====\") func(*args, **kwargs) end_time = datetime.now() print(\"total cost {}\".format(end_time - start_time)) return inner return outter @timet(\"p1\",\"p2\",p3=\"oirei\",p4=\"uryhd\") def p_p1(a1,a2,a3 = \"pkfjf\") : print(\"=====\") print(a1) print(a2) print(a3) print(\"======\") time.sleep(4) p_p1(\"a1111111\",\"a22222\") ","date":"2021-01-30","objectID":"/python/python-decorator/:3:2","tags":["python","装饰器"],"title":"Python 中的装饰器","uri":"/python/python-decorator/"},{"categories":null,"content":"设计模式六大原则","date":"2021-01-19","objectID":"/design-principle/","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"导读 重温设计模式的原则 ","date":"2021-01-19","objectID":"/design-principle/:1:0","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"六大原则 ","date":"2021-01-19","objectID":"/design-principle/:2:0","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"单一职责原则（Single Responsibility Principle） There should never be more than one reason for a class to change. 就一个类而言， 应该仅有一个引起它变化的原因。 ","date":"2021-01-19","objectID":"/design-principle/:2:1","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"开放封闭原则（Open-Closed Principle） Software entities like classes,modules and functions should be open for extension but closed for modifications. 类、模块、函数等应该是可以拓展的，但是不可修改。 开闭原则指导我们，当软件需要变化时，应该尽量通过拓展的方式来实现变化，而不是通过修改已有代码来实现。这里的“应该尽量”4个字说明OCP原则并不是说绝对不可以修改原始类的。当我们嗅到原来的代码“腐化气味”时，应该尽早地重构，以便使代码恢复到正常的“进化”过程，而不是通过集成等方式添加新的实现，这会导致类型的膨胀以及历史遗留代码的冗余。因此，在开发过程中需要自己结合具体情况进行考量，是通过修改旧代码还是通过继承使得软件系统更稳定、更灵活，在保证去除“代码腐化”的同时，也保证原有模块的正确性。 ","date":"2021-01-19","objectID":"/design-principle/:2:2","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"里式替换原则（Liskov Substitution Principle） Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it. 所有引用基类的地方必须能透明地使用其子类的对象。 里氏替换原则的核心原理是抽象，抽象又依赖于继承这个特性，在OOP中，继承的优缺点相当明显，有点如下： 代码重用，减少创建类成本，每个子类拥有父类的属性和方法； 子类和父类基本相似，但又与父类有所区别； 提高代码的可拓展性。 继承的缺点： 继承是侵入性的，只要继承就必须拥有父类的所有属性和方法； 可能造成子类代码的冗余、灵活性降低，因为子类必须拥有弗雷的属性和方法。 开闭原则和里氏替换原则往往是生死相依、不离不弃的，通过里氏替换来达到对扩展的开发，对修改的关闭效果。 ","date":"2021-01-19","objectID":"/design-principle/:2:3","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"迪米特原则(最少知识原则)（Law of Demeter） Only talk to you immediate friends. 一个软件实体应当尽可能少地与其他实体发生相互作用。 ","date":"2021-01-19","objectID":"/design-principle/:2:4","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"接口隔离原则（Interface Segregation Principle） The dependency of one class to another one should depend on the smallest possible interface. 一个类对另一个类的依赖应该建立在最小的接口上。 建立单一接口，不要建立庞大臃肿接口；尽量细化接口，接口中方法尽量少。也就是说，我们要为各个类建立专用的接口，而不要试图建立一个很庞大的接口供所有依赖它的类调用。 接口尽量小，但是要有限度。对接口进行细化可以提高程序设计的灵活性；但是如果过小，则会造成接口数量过多，使设计复杂化。所以，一定要适度。 为依赖接口的类定制服务，只暴露给调用的类需要的方法，它不需要的方法则隐蔽起来。只有专注得为一个模块提供定制服务，才能建立最小的依赖关系。 提高内聚，减少对外交互。接口方法尽量少用public修饰。接口是对外的承诺，承诺越少对系统开发越有利，变更风险也会越少。 ","date":"2021-01-19","objectID":"/design-principle/:2:5","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"依赖倒置原则（Dependence Inversion Principle） High level modules should not depends upon low level modules.Both should depend upon abstractions.Abstractions should not depend upon details.Details should depend upon abstractions. 高层模块不应该依赖于低层模块，两者都应该依赖于抽象。抽象不应该依赖于细节，细节应该依赖于抽象。 ","date":"2021-01-19","objectID":"/design-principle/:2:6","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"总结 单一职责原则告诉我们实现类要职责单一 里氏替换原则告诉我们不要破坏继承体系 依赖倒置原则告诉我们要面向接口编程 接口隔离原则告诉我们在设计接口的时候要精简单一 迪米特法则告诉我们要降低耦合。 开闭原则是总纲，告诉我们要对扩展开放，对修改关闭。 ","date":"2021-01-19","objectID":"/design-principle/:3:0","tags":["设计模式"],"title":"设计模式六大原则","uri":"/design-principle/"},{"categories":null,"content":"消息队列原理之kafka","date":"2020-12-17","objectID":"/mq-kafka/","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"导读 本文消息队列系列第二篇，上一篇讲述的是 Rabbitmq ，这篇主要介绍 Kafka 的原理与使用。 Kafka 是一个快速、可扩展的、高吞吐的、可容错的分布式“发布-订阅”消息系统， 使用 Scala 与 Java 语言编写，能够将消息从一个端点传递到另一个端点。 较之传统的消息中间件（例如 ActiveMQ、RabbitMQ），Kafka 具有高吞吐量、内置分区、支持消息副本和高容错的特性，非常适合大规模消息处理应用程序。 Kafka 官网：http://kafka.apache.org/ ","date":"2020-12-17","objectID":"/mq-kafka/:1:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka 主要设计目标如下 以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间的访问性能。 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条消息的传输。 支持 Kafka Server 间的消息分区，及分布式消费，同时保证每个 Partition 内的消息顺序传输。 同时支持离线数据处理和实时数据处理。 支持在线水平扩展。 ","date":"2020-12-17","objectID":"/mq-kafka/:2:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka 通常用于两大类应用程序 建立实时流数据管道，以可靠地在系统或应用程序之间获取数据。 构建实时流应用程序，以转换或响应数据流。 要了解 Kafka 如何执行这些操作，让我们从头开始深入研究 Kafka 的功能。 ","date":"2020-12-17","objectID":"/mq-kafka/:3:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"首先几个概念 Kafka 在一个或多个可以跨越多个数据中心的服务器上作为集群运行。 Kafka 集群将记录流存储在称为主题的类别中。 每个记录由一个键，一个值和一个时间戳组成。 ","date":"2020-12-17","objectID":"/mq-kafka/:4:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka 架构体系如下图 Kafka 的应用场景非常多, 下面我们就来举几个我们最常见的场景： 用户的活动跟踪：用户在网站的不同活动消息发布到不同的主题中心，然后可以对这些消息进行实时监测、实时处理。当然，也可以加载到 Hadoop 或离线处理数据仓库，对用户进行画像。像淘宝、天猫、京东这些大型电商平台，用户的所有活动都要进行追踪的。 日志收集如下图： 限流削峰如下图： 高吞吐率实现：Kafka 与其他 MQ 相比，最大的特点就是高吞吐率。为了增加存储能力，Kafka 将所有的消息都写入到了低速大容量的硬盘。按理说，这将导致性能损失，但实际上，Kafka 仍然可以保持超高的吞吐率，并且其性能并未受到影响。 其主要采用如下方式实现了高吞吐率： 顺序读写：Kafka 将消息写入到了分区 Partition 中，而分区中的消息又是顺序读写的。顺序读写要快于随机读写。 零拷贝：生产者、消费者对于 Kafka 中的消息是采用零拷贝实现的。 批量发送：Kafka 允许批量发送模式。 消息压缩：Kafka 允许对消息集合进行压缩。 ","date":"2020-12-17","objectID":"/mq-kafka/:5:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka 的优点 解耦：在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余（副本）：有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\"插入-获取-删除\"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。 灵活性\u0026峰值处理能力：在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka 保证一个 Partition 内的消息的有序性。 缓冲：在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行，写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。 异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 ","date":"2020-12-17","objectID":"/mq-kafka/:6:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka 与其他 MQ 对比 RabbitMQ：RabbitMQ 是使用 Erlang 编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP，SMTP，STOMP，也正因如此，它非常重量级，更适合于企业级的开发。同时实现了 Broker 构架，这意味着消息在发送给客户端时先在中心队列排队。对路由，负载均衡或者数据持久化都有很好的支持。 Redis：Redis 是一个基于 Key-Value 对的 NoSQL 数据库，开发维护很活跃。虽然它是一个 Key-Value 数据库存储系统，但它本身支持 MQ 功能，所以完全可以当做一个轻量级的队列服务来使用。对于 RabbitMQ 和 Redis 的入队和出队操作，各执行 100 万次，每 10 万次记录一次执行时间。测试数据分为 128Bytes、512Bytes、1K 和 10K 四个不同大小的数据。实验表明：入队时，当数据比较小时 Redis 的性能要高于 RabbitMQ，而如果数据大小超过了 10K，Redis 则慢的无法忍受；出队时，无论数据大小，Redis 都表现出非常好的性能，而 RabbitMQ 的出队性能则远低于 Redis。 ZeroMQ：ZeroMQ 号称最快的消息队列系统，尤其针对大吞吐量的需求场景。ZeroMQ 能够实现 RabbitMQ 不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，技术上的复杂度是对这 MQ 能够应用成功的挑战。ZeroMQ 具有一个独特的非中间件的模式，你不需要安装和运行一个消息服务器或中间件，因为你的应用程序将扮演这个服务器角色。你只需要简单的引用 ZeroMQ 程序库，可以使用 NuGet 安装，然后你就可以愉快的在应用程序之间发送消息了。但是 ZeroMQ 仅提供非持久性的队列，也就是说如果宕机，数据将会丢失。其中，Twitter 的 Storm 0.9.0 以前的版本中默认使用 ZeroMQ 作为数据流的传输（Storm 从 0.9 版本开始同时支持 ZeroMQ 和 Netty 作为传输模块）。 ActiveMQ：ActiveMQ 是 Apache 下的一个子项目。类似于 ZeroMQ，它能够以代理人和点对点的技术实现队列。同时类似于 RabbitMQ，它少量代码就可以高效地实现高级应用场景。 Kafka/Jafka：Kafka 是 Apache 下的一个子项目，是一个高性能跨语言分布式发布/订阅消息队列系统，而 Jafka 是在 Kafka 之上孵化而来的，即 Kafka 的一个升级版。 Kafka 具有以下特性: 快速持久化，可以在 O(1) 的系统开销下进行消息持久化。 高吞吐，在一台普通的服务器上既可以达到 10W/s 的吞吐速率。 完全的分布式系统，Broker、Producer、Consumer 都原生自动支持分布式，自动实现负载均衡。 支持 Hadoop 数据并行加载，对于像 Hadoop 的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。 Kafka 通过 Hadoop 的并行加载机制统一了在线和离线的消息处理。Apache Kafka 相对于 ActiveMQ 是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。 Kafka 的几种重要角色如下： Kafka 作为存储系统：任何允许发布与使用无关的消息发布的消息队列都有效地充当了运行中消息的存储系统。Kafka 的不同之处在于它是一个非常好的存储系统。写入 Kafka 的数据将写入磁盘并进行复制以实现容错功能。Kafka 允许生产者等待确认，以便直到完全复制并确保即使写入服务器失败的情况下写入也不会完成。 Kafka 的磁盘结构可以很好地扩展使用-无论服务器上有 50KB 还是 50TB 的持久数据，Kafka 都将执行相同的操作。由于认真对待存储并允许客户端控制其读取位置，因此您可以将 Kafka 视为一种专用于高性能，低延迟提交日志存储，复制和传播的专用分布式文件系统。 Kafka 作为消息传递系统：Kafka 的流概念与传统的企业消息传递系统相比如何？传统上，消息传递具有两种模型：排队和发布订阅。在队列中，一组使用者可以从服务器中读取内容，并且每条记录都将转到其中一个。在发布-订阅记录中广播给所有消费者。这两个模型中的每一个都有优点和缺点。排队的优势在于，它允许您将数据处理划分到多个使用者实例上，从而扩展处理量。 不幸的是，队列不是多用户的—一次进程读取了丢失的数据。发布-订阅允许您将数据广播到多个进程，但是由于每条消息都传递给每个订阅者，因此无法扩展处理。Kafka 的消费者群体概念概括了这两个概念。与队列一样，使用者组允许您将处理划分为一组进程（使用者组的成员）。与发布订阅一样，Kafka 允许您将消息广播到多个消费者组。 Kafka 模型的优点在于，每个主题都具有这些属性-可以扩展处理范围，并且是多订阅者，无需选择其中一个。与传统的消息传递系统相比，Kafka 还具有更强的订购保证。传统队列将记录按顺序保留在服务器上，如果多个使用者从队列中消费，则服务器将按记录的存储顺序分发记录。但是，尽管服务器按顺序分发记录，但是这些记录是异步传递给使用者的，因此它们可能在不同的使用者上乱序到达。 这实际上意味着在并行使用的情况下会丢失记录的顺序。消息传递系统通常通过“专有使用者”的概念来解决此问题，该概念仅允许一个进程从队列中使用，但是，这当然意味着在处理中没有并行性。Kafka 做得更好，通过在主题内具有并行性（即分区）的概念，Kafka 能够在用户进程池中提供排序保证和负载均衡。 这是通过将主题中的分区分配给消费者组中的消费者来实现的，以便每个分区都由组中的一个消费者完全消费。通过这样做，我们确保使用者是该分区的唯一读取器，并按顺序使用数据。由于存在许多分区，因此仍然可以平衡许多使用者实例上的负载。但是请注意，使用者组中的使用者实例不能超过分区。 Kafka 用作流处理：仅读取，写入和存储数据流是不够的，目的是实现对流的实时处理。在 Kafka 中，流处理器是指从输入主题中获取连续数据流，对该输入进行一些处理并生成连续数据流以输出主题的任何东西。例如，零售应用程序可以接受销售和装运的输入流，并输出根据此数据计算出的重新订购和价格调整流。 可以直接使用生产者和消费者 API 进行简单处理。但是，对于更复杂的转换，Kafka 提供了完全集成的 Streams API。这允许构建执行非重要处理的应用程序，这些应用程序计算流的聚合或将流连接在一起。该功能有助于解决此类应用程序所面临的难题：处理无序数据，在代码更改时重新处理输入，执行状态计算等。 流 API 建立在 Kafka 提供的核心原语之上：它使用生产者和使用者 API 进行输入，使用 Kafka 进行状态存储，并使用相同的组机制来实现流处理器实例之间的容错。 ","date":"2020-12-17","objectID":"/mq-kafka/:7:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka 中的关键术语解释 Topic： 主题。在 Kafka 中，使用一个类别属性来划分消息的所属类，划分消息的这个类称为 Topic。Topic 相当于消息的分类标签，是一个逻辑概念。物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 Broker 上但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处。 Partition： 分区。Topic 中的消息被分割为一个或多个 Partition，其是一个物理概念，对应到系统上 就是一个或若干个目录。Partition 内部的消息是有序的，但 Partition 间的消息是无序的。 Segment 段: 将 Partition 进一步细分为了若干的 Segment，每个 Segment 文件的大小相等。 Broker： Kafka 集群包含一个或多个服务器，每个服务器节点称为一个 Broker。Broker 存储 Topic 的数据。如果某 Topic 有 N 个 Partition，集群有 N 个 Broker，那么每个 Broker 存储该 Topic 的一个 Partition。 如果某 Topic 有 N 个 Partition，集群有（N+M）个 Broker，那么其中有 N 个 Broker 存储该 Topic 的一个 Partition，剩下的 M 个 Broker 不存储该 Topic 的 Partition 数据。如果某 Topic 有 N 个 Partition，集群中 Broker 数目少于 N 个，那么一个 Broker 存储该 Topic 的一个或多个 Partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致 Kafka 集群数据不均衡。 Producer： 生产者。即消息的发布者，生产者将数据发布到他们选择的主题。生产者负责选择将哪个记录分配给主题中的哪个分区。即：生产者生产的一条消息，会被写入到某一个 Partition。 Consumer： 消费者。可以从 Broker 中读取消息。一个消费者可以消费多个 Topic 的消息；一个消费者可以消费同一个 Topic 中的多个 Partition 中的消息；一个 Partiton 允许多个 Consumer 同时消费。 Consumer Group： Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制。组内可以有多个消费者，它们共享一个公共的 ID，即 Group ID。组内的所有消费者协调在一起来消费订阅主题 的所有分区。Kafka 保证同一个 Consumer Group 中只有一个 Consumer 会消费某条消息。 实际上，Kafka 保证的是稳定状态下每一个 Consumer 实例只会消费某一个或多个特定的 Partition，而某个 Partition 的数据只会被某一个特定的 Consumer 实例所消费。 下面我们用官网的一张图, 来标识 Consumer 数量和 Partition 数量的对应关系。 由两台服务器组成的 Kafka 群集，其中包含四个带有两个使用者组的分区（P0-P3）。消费者组 A 有两个消费者实例，组 B 有四个。 对于这个消费组, 以前一直搞不明白, 我自己的总结是：Topic 中的 Partitoin 到 Group 是发布订阅的通信方式。 即一条 Topic 的 Partition 的消息会被所有的 Group 消费，属于一对多模式；Group 到 Consumer 是点对点通信方式，属于一对一模式。 举个例子：不使用 Group 的话，启动 10 个 Consumer 消费一个 Topic，这 10 个 Consumer 都能得到 Topic 的所有数据，相当于这个 Topic 中的任一条消息被消费 10 次。 使用 Group 的话，连接时带上 groupid，Topic 的消息会分发到 10 个 Consumer 上，每条消息只被消费 1 次。 Replizcas of partition： 分区副本。副本是一个分区的备份，是为了防止消息丢失而创建的分区的备份。 Partition Leader： 每个 Partition 有多个副本，其中有且仅有一个作为 Leader，Leader 是当前负责消息读写 的 Partition。即所有读写操作只能发生于 Leader 分区上。 Partition Follower： 所有 Follower 都需要从 Leader 同步消息，Follower 与 Leader 始终保持消息同步。Leader 与 Follower 的关系是主备关系，而非主从关系。 ISR： ISR，In-Sync Replicas，是指副本同步列表。ISR 列表是由 Leader 负责维护。 AR，Assigned Replicas，指某个 Partition 的所有副本, 即已分配的副本列表。 OSR，Outof-Sync Replicas，即非同步的副本列表。 AR=ISR+OSR Offset： 偏移量。每条消息都有一个当前 Partition 下唯一的 64 字节的 Offset，它是相当于当前分区第一条消息的偏移量。 Broker Controller： Kafka集群的多个 Broker 中，有一个会被选举 Controller，负责管理整个集群中 Partition 和 Replicas 的状态。 只有 Broker Controller 会向 Zookeeper 中注册 Watcher，其他 Broker 及分区无需注册。即 Zookeeper 仅需监听 Broker Controller 的状态变化即可。 HW 与 LEO： HW，HighWatermark，高水位，表示 Consumer 可以消费到的最高 Partition 偏移量。HW 保证了 Kafka 集群中消息的一致性。确切地说，是保证了 Partition 的 Follower 与 Leader 间数 据的一致性。 LEO，Log End Offset，日志最后消息的偏移量。消息是被写入到 Kafka 的日志文件中的， 这是当前最后一个写入的消息在 Partition 中的偏移量。 对于 Leader 新写入的消息，Consumer 是不能立刻消费的。Leader 会等待该消息被所有 ISR 中的 Partition Follower 同步后才会更新 HW，此时消息才能被 Consumer 消费。 我相信你看完上面的概念还是懵逼的，好吧！下面我们就用图来形象话的表示两者的关系吧： Zookeeper： Zookeeper 负责维护和协调 Broker，负责 Broker Controller 的选举。在 Kafka 0.9 之前版本，Offset 是由 ZK 负责管理的。 总结：ZK 负责 Controller 的选举，Controller 负责 Leader 的选举。 Coordinator： 一般指的是运行在每个 Broker 上的 Group Coordinator 进程，用于管理 Consumer Group 中的各个成员，主要用于 Offset 位移管理和 Rebalance。一个 Coordinator 可以同时管理多个消费者组。 Rebalance： 当消费者组中的数量发生变化，或者 Topic 中的 Partition 数量发生了变化时，Partition 的所有权会在消费者间转移，即 Partition 会重新分配，这个过程称为再均衡 Rebalance。 再均衡能够给消费者组及 Broker 带来高性能、高可用性和伸缩，但在再均衡期间消费者是无法读取消息的，即整个 Broker 集群有小一段时间是不可用的。因此要避免不必要的再均衡。 Offset Commit： Consumer 从 Broker 中取一批消息写入 Buffer 进行消费，在规定的时间内消费完消息后，会自动将其消费消息的 Offset 提交给 Broker，以记录下哪些消息是消费过的。当然，若在时限内没有消费完毕，其是不会提交 Offset 的。 ","date":"2020-12-17","objectID":"/mq-kafka/:8:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka的工作原理和过程 ","date":"2020-12-17","objectID":"/mq-kafka/:9:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"消息写入算法 消息发送者将消息发送给 Broker, 并形成最终的可供消费者消费的 log，是比较复杂的过程： Producer 先从 Zookeeper 中找到该 Partition 的 Leader。 Producer将消息发送给该 Leader。 Leader 将消息接入本地的 log，并通知 ISR 的 Followers。 ISR 中的 Followers 从 Leader 中 Pull 消息, 写入本地 log 后向 Leader 发送 Ack。 Leader 收到所有 ISR 中的 Followers 的 Ack 后，增加 HW 并向 Producer 发送 Ack，表示消息写入成功。 ","date":"2020-12-17","objectID":"/mq-kafka/:9:1","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"消息路由策略 在通过 API 方式发布消息时，生产者是以 Record 为消息进行发布的。 Record 中包含 Key 与 Value，Value 才是我们真正的消息本身，而 Key 用于路由消息所要存放的 Partition。 消息要写入到哪个 Partition 并不是随机的，而是有路由策略的： 若指定了 Partition，则直接写入到指定的 Partition。 若未指定 Partition 但指定了 Key，则通过对 Key 的 Hash 值与 Partition 数量取模，该取模。 结果就是要选出的 Partition 索引。 若 Partition 和 Key 都未指定，则使用轮询算法选出一个 Partition。 ","date":"2020-12-17","objectID":"/mq-kafka/:9:2","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"HW 截断机制 如果 Partition Leader 接收到了新的消息， ISR 中其它 Follower 正在同步过程中，还未同步完毕时 leader 宕机。 此时就需要选举出新的 Leader。若没有 HW 截断机制，将会导致 Partition 中 Leader 与 Follower 数据的不一致。 当原 Leader 宕机后又恢复时，将其 LEO 回退到其宕机时的 HW，然后再与新的 Leader 进行数据同步，这样就可以保证老 Leader 与新 Leader 中数据一致了，这种机制称为 HW 截断机制。 ","date":"2020-12-17","objectID":"/mq-kafka/:9:3","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"消息发送的可靠性 生产者向 Kafka 发送消息时，可以选择需要的可靠性级别。通过 request.required.acks 参数的值进行设置。 0 值： 异步发送。生产者向 Kafka 发送消息而不需要 Kafka 反馈成功 Ack。该方式效率最高，但可靠性最低。 其可能会存在消息丢失的情况： 在传输过程中会出现消息丢失。 在 Broker 内部会出现消息丢失。 会出现写入到 Kafka 中的消息的顺序与生产顺序不一致的情况。 1 值： 同步发送。生产者发送消息给 Kafka，Broker 的 Partition Leader 在收到消息后马上发送成功 Ack（无需等等 ISR 中的 Follower 同步）。 生产者收到后知道消息发送成功，然后会再发送消息。如果一直未收到 Kafka 的 Ack，则生产者会认为消息发送失败，会重发消息。 该方式对于 Producer 来说，若没有收到 Ack，一定可以确认消息发送失败了，然后可以重发。 但是，即使收到了 ACK，也不能保证消息一定就发送成功了。故，这种情况，也可能会发生消息丢失的情况。 -1 值： 同步发送。生产者发送消息给 Kafka，Kafka 收到消息后要等到 ISR 列表中的所有副本都 同步消息完成后，才向生产者发送成功 Ack。 如果一直未收到 Kafka 的 Ack，则认为消息发送 失败，会自动重发消息。该方式会出现消息重复接收的情况。 ","date":"2020-12-17","objectID":"/mq-kafka/:9:4","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"消费者消费过程解析 生产者将消息发送到 Topitc 中，消费者即可对其进行消费，其消费过程如下： Consumer 向 Broker 提交连接请求，其所连接上的 Broker 都会向其发送Broker Controller 的通信 URL，即配置文件中的 Listeners 地址。 当 Consumer 指定了要消费的 Topic 后，会向 Broker Controller 发送消费请求。 Broker Controller 会为 Consumer 分配一个或几个 Partition Leader，并将该 Partition 的当前 Offset 发送给 Consumer。 Consumer 会按照 Broker Controller 分配的 Partition 对其中的消息进行消费。 当 Consumer 消费完该条消息后，Consumer 会向 Broker 发送一个消息已经被消费反馈，即该消息的 Offset。 在 Broker 接收到 Consumer 的 Offset 后，会更新相应的 __consumer_offset 中。 以上过程会一直重复，知道消费者停止请求消费。 Consumer 可以重置 Offset，从而可以灵活消费存储在 Broker 上的消息。 ","date":"2020-12-17","objectID":"/mq-kafka/:9:5","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Partition Leader 选举范围 当 Leader 宕机后，Broker Controller 会从 ISR 中挑选一个 Follower 成为新的 Leader。 如果 ISR 中没有其他副本怎么办？可以通过 unclean.leader.election.enable 的值来设置 Leader 选举范围。 False：必须等到 ISR 列表中所有的副本都活过来才进行新的选举。该策略可靠性有保证，但可用性低。 True：在 ISR 列表中没有副本的情况下，可以选择任意一个没有宕机的主机作为新的 Leader，该策略可用性高，但可靠性没有保证。 ","date":"2020-12-17","objectID":"/mq-kafka/:9:6","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"重复消费问题的解决方案 同一个 Consumer 重复消费：当 Consumer 由于消费能力低而引发了消费超时，则可能会形成重复消费。 在某数据刚好消费完毕，但是正准备提交 Offset 时候，消费时间超时，则 Broker 认为这条消息未消费成功。这时就会产生重复消费问题。其解决方案：延长 Offset 提交时间。 不同的 Consumer 重复消费：当 Consumer 消费了消息，但还没有提交 Offset 时宕机，则这些已经被消费过的消息会被重复消费。其解决方案：将自动提交改为手动提交。 ","date":"2020-12-17","objectID":"/mq-kafka/:9:7","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"从架构设计上解决 Kafka 重复消费的问题 我们在设计程序的时候，比如考虑到网络故障等一些异常的情况，我们都会设置消息的重试次数，可能还有其他可能出现消息重复，那我们应该如何解决呢？下面提供三个方案： 保存并查询 给每个消息都设置一个独一无二的 uuid，所有的消息，我们都要存一个 uuid。 我们在消费消息的时候，首先去持久化系统中查询一下看这个看是否以前消费过，如没有消费过，在进行消费，如果已经消费过，丢弃就好了。 下图表明了这种方案： 利用幂等 幂等（Idempotence）在数学上是这样定义的，如果一个函数 f(x) 满足：f(f(x)) = f(x)，则函数 f(x) 满足幂等性。 这个概念被拓展到计算机领域，被用来描述一个操作、方法或者服务。一个幂等操作的特点是，其任意多次执行所产生的影响均与一次执行的影响相同。 一个幂等的方法，使用同样的参数，对它进行多次调用和一次调用，对系统产生的影响是一样的。所以，对于幂等的方法，不用担心重复执行会对系统造成任何改变。 我们举个例子来说明一下。在不考虑并发的情况下，“将 X 老师的账户余额设置为 100 万元”，执行一次后对系统的影响是，X 老师的账户余额变成了 100 万元。只要提供的参数 100 万元不变，那即使再执行多少次，X 老师的账户余额始终都是 100 万元，不会变化，这个操作就是一个幂等的操作。 再举一个例子，“将 X 老师的余额加 100 万元”，这个操作它就不是幂等的，每执行一次，账户余额就会增加 100 万元，执行多次和执行一次对系统的影响（也就是账户的余额）是不一样的。 所以，通过这两个例子，我们可以想到如果系统消费消息的业务逻辑具备幂等性，那就不用担心消息重复的问题了，因为同一条消息，消费一次和消费多次对系统的影响是完全一样的。也就可以认为，消费多次等于消费一次。 那么，如何实现幂等操作呢？最好的方式就是，从业务逻辑设计上入手，将消费的业务逻辑设计成具备幂等性的操作。 但是，不是所有的业务都能设计成天然幂等的，这里就需要一些方法和技巧来实现幂等。 下面我们介绍一种常用的方法：利用数据库的唯一约束实现幂等。 例如，我们刚刚提到的那个不具备幂等特性的转账的例子：将 X 老师的账户余额加 100 万元。在这个例子中，我们可以通过改造业务逻辑，让它具备幂等性。 首先，我们可以限定，对于每个转账单每个账户只可以执行一次变更操作，在分布式系统中，这个限制实现的方法非常多，最简单的是我们在数据库中建一张转账流水表。 这个表有三个字段：转账单 ID、账户 ID 和变更金额，然后给转账单 ID 和账户 ID 这两个字段联合起来创建一个唯一约束，这样对于相同的转账单 ID 和账户 ID，表里至多只能存在一条记录。 这样，我们消费消息的逻辑可以变为：“在转账流水表中增加一条转账记录，然后再根据转账记录，异步操作更新用户余额即可。” 在转账流水表增加一条转账记录这个操作中，由于我们在这个表中预先定义了“账户 ID 转账单 ID”的唯一约束，对于同一个转账单同一个账户只能插入一条记录，后续重复的插入操作都会失败，这样就实现了一个幂等的操作。 设置前提条件 为更新的数据设置前置条件另外一种实现幂等的思路是，给数据变更设置一个前置条件，如果满足条件就更新数据，否则拒绝更新数据，在更新数据的时候，同时变更前置条件中需要判断的数据。 这样，重复执行这个操作时，由于第一次更新数据的时候已经变更了前置条件中需要判断的数据，不满足前置条件，则不会重复执行更新数据操作。 比如，刚刚我们说过，“将 X 老师的账户的余额增加 100 万元”这个操作并不满足幂等性，我们可以把这个操作加上一个前置条件，变为：“如果 X 老师的账户当前的余额为 500 万元，将余额加 100 万元”，这个操作就具备了幂等性。 对应到消息队列中的使用时，可以在发消息时在消息体中带上当前的余额，在消费的时候进行判断数据库中，当前余额是否与消息中的余额相等，只有相等才执行变更操作。 但是，如果我们要更新的数据不是数值，或者我们要做一个比较复杂的更新操作怎么办？用什么作为前置判断条件呢？ 更加通用的方法是，给你的数据增加一个版本号属性，每次更数据前，比较当前数据的版本号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等。 我们在工作中，为了保证环境的高可用，防止单点，Kafka 都是以集群的方式出现的，下面就带领大家一起搭建一套 Kafka 集群环境。 我们在官网下载 Kafka，下载地址为：http://kafka.apache.org/downloads，下载我们需要的版本，推荐使用稳定的版本。 ","date":"2020-12-17","objectID":"/mq-kafka/:9:8","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"搭建集群 下载并解压： cd /usr/local/src wget http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.4.0/kafka_2.11-2.4.0.tgz mkdir /data/servers tar xzvf kafka_2.11-2.4.0.tgz -C /data/servers/ cd /data/servers/kafka_2.11-2.4.0 修改配置文件： 确保每个机器上的 id 不一样 broker.id=0 配置服务端的监控地址 listeners=PLAINTEXT://192.168.51.128:9092 Kafka 日志目录 log.dirs=/data/servers/kafka_2.11-2.4.0/logs #Kafka 设置的 partitons 的个数 num.partitions=1 ZooKeeper 的连接地址，如果有自己的 ZooKeeper 集群，请直接使用自己搭建的 ZooKeeper 集群 zookeeper.connect=192.168.51.128:2181 因为我自己是本机做实验，所有使用的是一个主机的不同端口，在线上，就是不同的机器，大家参考即可。 我们这里使用 Kafka 的 ZooKeeper，只启动一个节点，但是正真的生产过程中，是需要 ZooKeeper 集群，自己搭建就好，后期我们也会出 ZooKeeper 的教程，大家请关注就好了。 拷贝 3 份配置文件： #创建对应的日志目录 mkdir -p /data/servers/kafka_2.11-2.4.0/logs/9092 mkdir -p /data/servers/kafka_2.11-2.4.0/logs/9093 mkdir -p /data/servers/kafka_2.11-2.4.0/logs/9094 #拷贝三份配置文件 cp server.properties server_9092.properties cp server.properties server_9093.properties cp server.properties server_9094.properties 修改不同端口对应的文件： #9092 的 id 为 0，9093 的 id 为 1，9094 的 id 为 2 broker.id=0 # 配置服务端的监控地址，在不通的配置文件中写入不同的端口 listeners=PLAINTEXT://192.168.51.128:9092 # Kafka 日志目录，目录也是对应不同的端口 log.dirs=/data/servers/kafka_2.11-2.4.0/logs/9092 # Kafka 设置的 partitons 的个数 num.partitions=1 # ZooKeeper 的连接地址，如果有自己的 ZooKeeper 集群，请直接使用自己搭建的 ZooKeeper 集群 zookeeper.connect=192.168.51.128:2181 修改 ZooKeeper 的配置文件： dataDir=/data/servers/zookeeper server.1=192.168.51.128:2888:3888 然后创建 ZooKeeper 的 myid 文件： echo \"1\"\u003e /data/servers/zookeeper/myid 启动 ZooKeeper： 使用 Kafka 内置的 ZooKeeper： cd /data/servers/kafka_2.11-2.4.0/bin zookeeper-server-start.sh -daemon ../config/zookeeper.properties netstat -anp |grep 2181 启动 Kafka： ./kafka-server-start.sh -daemon ../config/server_9092.properties ./kafka-server-start.sh -daemon ../config/server_9093.properties ./kafka-server-start.sh -daemon ../config/server_9094.properties ","date":"2020-12-17","objectID":"/mq-kafka/:10:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka 的操作 ","date":"2020-12-17","objectID":"/mq-kafka/:11:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"topic 我们先来看一下创建 Topic 常用的参数吧： –create：创建 topic –delete：删除 topic –alter：修改 topic 的名字或者 partition 个数 –list：查看 topic –describe：查看 topic 的详细信息 –topic \u003cString: topic\u003e：指定 topic 的名字 –zookeeper \u003cString: hosts\u003e：指定 Zookeeper 的连接地址参数提示并不赞成这样使用（DEPRECATED, The connection string for the zookeeper connection in the form host:port. Multiple hosts can be given to allow fail-over.） –bootstrap-server \u003cString: server to connect to\u003e：指定 Kafka 的连接地址，推荐使用这个，参数的提示信息显示（REQUIRED: The Kafka server to connect to. In case of providing this, a direct Zookeeper connection won’t be required.）。 –replication-factor \u003cInteger: replication factor\u003e：对于每个 Partiton 的备份个数。（The replication factor for each partition in the topic being created. If not supplied, defaults to the cluster default.） –partitions \u003cInteger: # of partitions\u003e：指定该 topic 的分区的个数。 示例： cd /data/servers/kafka_2.11-2.4.0/bin # 创建 topic test1 kafka-topics.sh --create --bootstrap-server=192.168.51.128:9092,10.231.128.96:9093,192.168.51.128:9094 --replication-factor 1 --partitions 1 --topic test1 # 创建 topic test2 kafka-topics.sh --create --bootstrap-server=192.168.51.128:9092,10.231.128.96:9093,192.168.51.128:9094 --replication-factor 1 --partitions 1 --topic test2 # 查看 topic kafka-topics.sh --list --bootstrap-server=192.168.51.128:9092,10.231.128.96:9093,192.168.51.128:9094 自动创建 Topic 我们在工作中，如果我们不想去管理 Topic，可以通过 Kafka 的配置文件来管理。 我们可以让 Kafka 自动创建 Topic，需要在我们的 Kafka 配置文件中加入如下配置文件： auto.create.topics.enable=true 如果删除 Topic 想达到物理删除的目的，也是需要配置的： delete.topic.enable=true ","date":"2020-12-17","objectID":"/mq-kafka/:11:1","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"发送消息 他们可以通过客户端的命令生产消息，先来看看 kafka-console-producer.sh 常用的几个参数吧： –topic \u003cString: topic\u003e：指定 topic –timeout \u003cInteger: timeout_ms\u003e：超时时间 –sync：异步发送消息 –broker-list \u003cString: broker-list\u003e：官网提示：REQUIRED: The broker list string in the form HOST1:PORT1,HOST2:PORT2. 这个参数是必须的： kafka-console-producer.sh --broker-list 192.168.51.128:9092,192.168.51.128:9093,192.168.51.128:9094 --topic test1 ","date":"2020-12-17","objectID":"/mq-kafka/:11:2","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"消费消息 我们也还是先来看看 kafka-console-consumer.sh 的参数吧： –topic \u003cString: topic\u003e：指定 topic –group \u003cString: consumer group id\u003e：指定消费者组 –from-beginning：指定从开始进行消费, 如果不指定, 就从当前进行消费 –bootstrap-server：Kafka 的连接地址‍‍ ","date":"2020-12-17","objectID":"/mq-kafka/:11:3","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka 的日志 Kafka 的日志分两种： 第一种日志是我们的 Kafka 的启动日志，就是我们排查问题，查看报错信息的日志。 第二种日志就是我们的数据日志，Kafka 是我们的数据是以日志的形式存在存盘中的，我们第二种所说的日志就是我们的 Partiton 与 Segment。 那我们就来说说备份和分区吧：我们创建一个分区，一个备份，那么 test 就应该在三台机器上或者三个数据目录只有一个 test-0。（分区的下标是从 0 开始的） 如果我们创建 N 个分区，我们就会在三个服务器上发现，test_0-n，如果我们创建 M 个备份，我们就会在发现，test_0 到 test_n 每一个都是 M 个。 kafka-console-consumer.sh --bootstrap-server 192.168.51.128:9092,192.168.51.128:9093,192.168.51.128:9094 --topic test1 ---beginning ","date":"2020-12-17","objectID":"/mq-kafka/:11:4","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Kafka API 使用 Kafka 原生的 API ","date":"2020-12-17","objectID":"/mq-kafka/:12:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"消费者自动提交 定义自己的生产者： import org.apache.kafka.clients.producer.Callback; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; import java.util.Properties; /** * @ClassName MyKafkaProducer * @Description TODO * @Author lingxiangxiang * @Date 3:37 PM * @Version 1.0 **/ public class MyKafkaProducer { private org.apache.kafka.clients.producer.KafkaProducer\u003cInteger, String\u003e producer; public MyKafkaProducer() { Properties properties = new Properties(); properties.put(\"bootstrap.servers\", \"192.168.51.128:9092,192.168.51.128:9093,192.168.51.128:9094\"); properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.IntegerSerializer\"); properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 设置批量发送 properties.put(\"batch.size\", 16384); // 批量发送的等待时间 50ms, 超过 50ms, 不足批量大小也发送 properties.put(\"linger.ms\", 50); this.producer = new org.apache.kafka.clients.producer.KafkaProducer\u003cInteger, String\u003e(properties); } public boolean sendMsg() { boolean result = true; try { // 正常发送, test2 是 topic, 0 代表的是分区, 1 代表的是 key, hello world 是发送的消息内容 final ProducerRecord\u003cInteger, String\u003e record = new ProducerRecord\u003cInteger, String\u003e(\"test2\", 0, 1, \"hello world\"); producer.send(record); // 有回调函数的调用 producer.send(record, new Callback() { @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) { System.out.println(recordMetadata.topic()); System.out.println(recordMetadata.partition()); System.out.println(recordMetadata.offset()); } }); // 自己定义一个类 producer.send(record, new MyCallback(record)); } catch (Exception e) { result = false; } return result; } } 定义生产者发送成功的回调函数： import org.apache.kafka.clients.producer.Callback; import org.apache.kafka.clients.producer.RecordMetadata; /** * @ClassName MyCallback * @Description TODO * @Author lingxiangxiang * @Date 3:51 PM * @Version 1.0 **/ public class MyCallback implements Callback { private Object msg; public MyCallback(Object msg) { this.msg = msg; } @Override public void onCompletion(RecordMetadata metadata, Exception e) { System.out.println(\"topic = \" + metadata.topic()); System.out.println(\"partiton = \" + metadata.partition()); System.out.println(\"offset = \" + metadata.offset()); System.out.println(msg); } } 生产者测试类：在生产者测试类中，自己遇到一个坑，就是最后自己没有加 sleep，就是怎么检查自己的代码都没有问题，但是最后就是没法发送成功消息，最后加了一个 sleep 就可以了。 因为主函数 main 已经执行完退出，但是消息并没有发送完成，需要进行等待一下。当然，你在生产环境中可能不会遇到这样问题，呵呵！ 代码如下： import static java.lang.Thread.sleep; /** * @ClassName MyKafkaProducerTest * @Description TODO * @Author lingxiangxiang * @Date 3:46 PM * @Version 1.0 **/ public class MyKafkaProducerTest { public static void main(String[] args) throws InterruptedException { MyKafkaProducer producer = new MyKafkaProducer(); boolean result = producer.sendMsg(); System.out.println(\"send msg \" + result); sleep(1000); } } 消费者类： import kafka.utils.ShutdownableThread; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.util.Arrays; import java.util.Collections; import java.util.Properties; /** * @ClassName MyKafkaConsumer * @Description TODO * @Author lingxiangxiang * @Date 4:12 PM * @Version 1.0 **/ public class MyKafkaConsumer extends ShutdownableThread { private KafkaConsumer\u003cInteger, String\u003e consumer; public MyKafkaConsumer() { super(\"KafkaConsumerTest\", false); Properties properties = new Properties(); properties.put(\"bootstrap.servers\", \"192.168.51.128:9092,192.168.51.128:9093,192.168.51.128:9094\"); properties.put(\"group.id\", \"mygroup\"); properties.put(\"enable.auto.commit\", \"true\"); properties.put(\"auto.commit.interval.ms\", \"1000\"); properties.put(\"session.timeout.ms\", \"30000\"); properties.put(\"heartbeat.interval.ms\", \"10000\"); properties.put(\"auto.offset.reset\", \"earliest\"); properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.IntegerDeseriali","date":"2020-12-17","objectID":"/mq-kafka/:12:1","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"消费者同步手动提交 前面的消费者都是以自动提交 Offset 的方式对 Broker 中的消息进行消费的，但自动提交 可能会出现消息重复消费的情况。 所以在生产环境下，很多时候需要对 Offset 进行手动提交， 以解决重复消费的问题。 手动提交又可以划分为同步提交、异步提交，同异步联合提交。这些提交方式仅仅是 doWork() 方法不相同，其构造器是相同的。 所以下面首先在前面消费者类的基础上进行构造器的修改，然后再分别实现三种不同的提交方式。 同步提交方式是，消费者向 Broker 提交 Offset 后等待 Broker 成功响应。若没有收到响应，则会重新提交，直到获取到响应。 而在这个等待过程中，消费者是阻塞的。其严重影响了消费者的吞吐量。 修改前面的 MyKafkaConsumer.java, 主要修改下面的配置： import kafka.utils.ShutdownableThread; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.util.Arrays; import java.util.Collections; import java.util.Properties; /** * @ClassName MyKafkaConsumer * @Description TODO * @Author lingxiangxiang * @Date 4:12 PM * @Version 1.0 **/ public class MyKafkaConsumer extends ShutdownableThread { private KafkaConsumer\u003cInteger, String\u003e consumer; public MyKafkaConsumer() { super(\"KafkaConsumerTest\", false); Properties properties = new Properties(); properties.put(\"bootstrap.servers\", \"192.168.51.128:9092,192.168.51.128:9093,192.168.51.128:9094\"); properties.put(\"group.id\", \"mygroup\"); // 这里要修改成手动提交 properties.put(\"enable.auto.commit\", \"false\"); // properties.put(\"auto.commit.interval.ms\", \"1000\"); properties.put(\"session.timeout.ms\", \"30000\"); properties.put(\"heartbeat.interval.ms\", \"10000\"); properties.put(\"auto.offset.reset\", \"earliest\"); properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.IntegerDeserializer\"); properties.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); this.consumer = new KafkaConsumer\u003cInteger, String\u003e(properties); } @Override public void doWork() { consumer.subscribe(Arrays.asList(\"test2\")); ConsumerRecords\u003cInteger, String\u003erecords = consumer.poll(1000); for (ConsumerRecord record : records) { System.out.println(\"topic = \" + record.topic()); System.out.println(\"partition = \" + record.partition()); System.out.println(\"key = \" + record.key()); System.out.println(\"value = \" + record.value()); //手动同步提交 consumer.commitSync(); } } } ","date":"2020-12-17","objectID":"/mq-kafka/:12:2","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"消费者异步手工提交 手动同步提交方式需要等待 Broker 的成功响应，效率太低，影响消费者的吞吐量。 异步提交方式是，消费者向 Broker 提交 Offset 后不用等待成功响应，所以其增加了消费者的吞吐量。 import kafka.utils.ShutdownableThread; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.util.Arrays; import java.util.Collections; import java.util.Properties; /** * @ClassName MyKafkaConsumer * @Description TODO * @Author lingxiangxiang * @Date 4:12 PM * @Version 1.0 **/ public class MyKafkaConsumer extends ShutdownableThread { private KafkaConsumer\u003cInteger, String\u003e consumer; public MyKafkaConsumer() { super(\"KafkaConsumerTest\", false); Properties properties = new Properties(); properties.put(\"bootstrap.servers\", \"192.168.51.128:9092,192.168.51.128:9093,192.168.51.128:9094\"); properties.put(\"group.id\", \"mygroup\"); // 这里要修改成手动提交 properties.put(\"enable.auto.commit\", \"false\"); // properties.put(\"auto.commit.interval.ms\", \"1000\"); properties.put(\"session.timeout.ms\", \"30000\"); properties.put(\"heartbeat.interval.ms\", \"10000\"); properties.put(\"auto.offset.reset\", \"earliest\"); properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.IntegerDeserializer\"); properties.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); this.consumer = new KafkaConsumer\u003cInteger, String\u003e(properties); } @Override public void doWork() { consumer.subscribe(Arrays.asList(\"test2\")); ConsumerRecords\u003cInteger, String\u003erecords = consumer.poll(1000); for (ConsumerRecord record : records) { System.out.println(\"topic = \" + record.topic()); System.out.println(\"partition = \" + record.partition()); System.out.println(\"key = \" + record.key()); System.out.println(\"value = \" + record.value()); //手动同步提交 // consumer.commitSync(); //手动异步提交 // consumer.commitAsync(); // 带回调公共的手动异步提交 consumer.commitAsync((offsets, e) -\u003e { if(e != null) { System.out.println(\"提交次数, offsets = \" + offsets); System.out.println(\"exception = \" + e); } }); } } } ","date":"2020-12-17","objectID":"/mq-kafka/:12:3","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"Spring Boot 使用 Kafka 现在大家的开发过程中，很多都用的是 Spring Boot 的项目，直接启动了，如果还是用原生的 API，就是有点 Low 了啊，那 Kafka 是如何和 Spring Boot 进行联合的呢？ Maven 配置： \u003c!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.kafka\u003c/groupId\u003e \u003cartifactId\u003ekafka-clients\u003c/artifactId\u003e \u003cversion\u003e2.1.1\u003c/version\u003e \u003c/dependency\u003e 添加配置文件，在 application.properties 中加入如下配置信息： Kafka 连接地址： spring.kafka.bootstrap-servers = 192.168.51.128:9092,10.231.128.96:9093,192.168.51.128:9094 生产者： spring.kafka.producer.acks = 0 spring.kafka.producer.key-serializer = org.apache.kafka.common.serialization.StringSerializer spring.kafka.producer.value-serializer = org.apache.kafka.common.serialization.StringSerializer spring.kafka.producer.retries = 3 spring.kafka.producer.batch-size = 4096 spring.kafka.producer.buffer-memory = 33554432 spring.kafka.producer.compression-type = gzip 消费者： spring.kafka.consumer.group-id = mygroup spring.kafka.consumer.auto-commit-interval = 5000 spring.kafka.consumer.heartbeat-interval = 3000 spring.kafka.consumer.key-deserializer = org.apache.kafka.common.serialization.StringDeserializer spring.kafka.consumer.value-deserializer = org.apache.kafka.common.serialization.StringDeserializer spring.kafka.consumer.auto-offset-reset = earliest spring.kafka.consumer.enable-auto-commit = true # listenner, 标识消费者监听的个数 spring.kafka.listener.concurrency = 8 # topic的名字 kafka.topic1 = topic1 生产者： import lombok.extern.slf4j.Slf4j; import org.springframework.beans.factory.annotation.Value; import org.springframework.kafka.core.KafkaTemplate; @Service @Slf4j public class MyKafkaProducerServiceImpl implements MyKafkaProducerService { @Resource private KafkaTemplate\u003cString, String\u003e kafkaTemplate; // 读取配置文件 @Value(\"${kafka.topic1}\") private String topic; @Override public void sendKafka() { kafkaTemplate.send(topic, \"hell world\"); } } 消费者： @Component @Slf4j public class MyKafkaConsumer { @KafkaListener(topics = \"${kafka.topic1}\") public void listen(ConsumerRecord\u003c?, ?\u003e record) { Optional\u003c?\u003e kafkaMessage = Optional.ofNullable(record.value()); if (kafkaMessage.isPresent()) { log.info(\"----------------- record =\" + record); log.info(\"------------------ message =\" + kafkaMessage.get()); } ","date":"2020-12-17","objectID":"/mq-kafka/:12:4","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"参考 本文系转载 https://mp.weixin.qq.com/s/R1en4V0Tlwlpt102BjotoA ","date":"2020-12-17","objectID":"/mq-kafka/:13:0","tags":["kafka","消息队列"],"title":"消息队列原理之kafka","uri":"/mq-kafka/"},{"categories":null,"content":"消息队列原理之rabbitmq","date":"2020-12-16","objectID":"/mq-rabbitmq/","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"导读 谈起消息队列，我们的脑海可能会不由自主的冒出这么几个关键词，解耦、异步化、消峰、广播等，消息队列的种类也很多，如 rabbitmq、rocketmq、activemq、kafka等还有各个云厂商提供的消息队列。 它们都有各种的特点和使用场景，所以这个系列的文章主要谈各个消息的原理，目前规划了两篇文章，rabbitmq 和 kafka ，其他的暂时还没有用到，还没有深究。 这篇主要介绍 rabbitmq 的原理和基于 golang 如何使用。 ","date":"2020-12-16","objectID":"/mq-rabbitmq/:1:0","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"介绍 RabbitMQ 是一个由 Erlang 开发的 AMQP(Advanced Message Queuing Protocol，高级消息队列协议)的开源实现，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。支持多种客户端语言。 ","date":"2020-12-16","objectID":"/mq-rabbitmq/:2:0","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"架构 整体架构对照下面的图说明 先看看图片上各个名次的解释: Broker:它提供一种传输服务，它的角色就是维护一条从生产者到消费者的路线，保证数据能按照指定的方式进行传输，简单来说就是消息队列服务器实体。 Connection: 客户端与 Rabbitmq Broker 直接的 TCP 连接，通常一个客户端与 Broker 之间只需要一个连接即可。 Channel: 消息通道，在客户端的每个连接里，可建立多个channel，最好每个线程都用独立的Channel，后续的对 Queue 和 Exchange 的操作都是在 Channel 中完成的。 Producer: 消息生产者，通过和 Broker 建立 Connection 和 Channel ，向 Exchange 发送消息。 Consumer: 消息消费者，通过和 Broker 建立 Connection 和 Channel，从 Queue 中消费消息。 Exchange: 消息交换机，按照一定的策略把 Producer 生产的消息投递到 Queue 中，等待消费者消费。 Queue: 消息队列载体，每个消息都会被投入到一个或多个队列。 Vhost: 虚拟主机，一个broker里可以开设多个vhost，用作权限分离，把不同的系统使用的rabbitmq区分开，共用一个消息队列服务器，但看上去就像各自在用不用的rabbitmq服务器一样。 Binding：绑定，它的作用就是把exchange和queue按照路由规则绑定起来，这样RabbitMQ就知道如何正确地将消息路由到指定的Queue了。 RoutingKey：路由关键字，生产者在将消息发送给Exchange的时候，一般会指定一个routing key，来指定这个消息的路由规则，而这个routing key需要与Exchange Type及binding key联合使用才能最终生效。 这里面比较难理解的概念是 RoutingKey,Exchange,Binding ，消费发送时不会直接发送给 Queue ,而是先发送给 Exchange，由 Exchange 按照一定的规则投递到与它绑定的 Queue 中，那这个规则是什么呢? 规则就与 Exchange 的 Type、Binding、RoutingKey 相关，Exchange 支持的类型有 4 种，direct,fanout,topic,headers,含义如下: direct: Queue 和 Exchange 在绑定时需要指定一个 key, 我们称为 Bindkey。Producer 往 Exchange 发送消息时，也需要指定一个 key ，这个 key 就是 Routekey。这种模式下 Exchange 会把消息投递给 Routekey 和 Bindkey 相同的队列 fanout: 类似于广播的方式，会把消息投递给和 Exchange 绑定的所有队列，不需要检查 Routekey 和 Bindkey 。 topic: 类似于组播的方式，这种模式下 Bingkey 支持模糊匹配，* 代表匹配一个任意词组，#代表匹配0个或多个词组。如 Producer 产生一条 RouteKey 为 benz.car 的消息， 同时这个 Exchange 绑定了3组队列（请注意是3组不是3个，意思是Exchange可以和同一个Queue进行多次绑定，通过Bindkey 的不同，它们之间是多对多的关系），Bindkey 分别为: car ,*.car ,benz.car ,那么会把这个消息投递到 *.car、benz.car 对应的 Queue 中。 headers: 这个类型 Routekey 和 Bindkey 的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配。 对照上面图和名次解释应该比较清晰明了了，下面我们通过几个例子说明如何使用。 ","date":"2020-12-16","objectID":"/mq-rabbitmq/:3:0","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"用法（golang） ","date":"2020-12-16","objectID":"/mq-rabbitmq/:4:0","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"direct 先看看 Rabbitmq 默认的 exchange ，其中第一个(AMQP default) 是默认的，默认绑定了所有的 Queue ，会把消息投递到 Routekey 对应的队列中，即： Routekey==QueueName 。 package main import ( \"fmt\" \"github.com/streadway/amqp\" \"log\" ) func handlerError(err error, msg string) { if err != nil { log.Fatalf(\"%s: %s\", msg, err) } } var url = \"amqp://username:password@ip:port\" func main() { conn, err := amqp.Dial(url) handlerError(err, \"Failed to connect to RabbitMQ\") defer conn.Close() channel, err := conn.Channel() handlerError(err, \"Failed to open a Channel\") defer channel.Close() queueNameCar := \"car\" if _, err := channel.QueueDeclare(queueNameCar, false, false, false, false, nil); err != nil { handlerError(err, \"Failed to decare Queue\") } if err := channel.Publish(\"\", queueNameCar, false, false, amqp.Publishing{ContentType: \"text/plain\", Body: []byte(\"test car\")}); err != nil { handlerError(err, \"Failed to publish message\") } } 这里是一个完整的 Demo， 后面只会提供main() 函数的示例代码，其他的和这里这里类似。 申明了一个名称为 car 的消息队列，并没有做任何的绑定，往 defalut exchange 发送一条消息，routekey 为 car ,可以看到和队列名相同。 为了方便演示，结果以图片的方式展现，可以看到这里有 car 的队列，并且有一条消息。 在创建队列有几个参数可以关注一下 Durability： 持久化，是否将队列持久化到磁盘，当选择持久化时当 rabbitmq 重启了，这个队列还在，否则当重启了之后这个队列就没有了，需要重新创建，这个需要设计程序时考虑到。 Auto delete: 当其中一个消费者已经完成之后，会删除这个队列并断开与其他的消费者的连接。 Arguments： x-message-ttl: 消息的过期时间，发布到队列中的消息在被丢弃之前可以存活多久。 x-expires: 队列的过期时间，一个队列在多长时间内未使用会被自动删除。 x-max-length: 队列的长度，最多剋容纳多少条消息。 x-max-length-bytes: 队列最大可以包含多大的消息。 x-dead-letter-exchange: 当消息过期或者被客户端reject 之后应该重新投递到那个exchange ，类似与一个producer发送消息时选择exchange x-dead-letter-routing-key: 当消息过期或者被客户端reject 之后重新投递时的 Routekey，类似与一个producer发送消息时设置routekey，默认是原消息的 routekey x-max-priority: 消息的优先级设置，设置可以支持的最大优先级，如设置为10,则可以在发送消息设置优先级，可以根据优先级处理消息，默认为空，当为空时则不支持优先级 x-queue-mode: 将队列设置为懒惰模式，尽可能多地将消息保留在磁盘上，以减少RAM的使用量；如果不设置，队列将保留内存中的缓存，以尽可能快地传递消息。 我们自己创建一个 direct 类型的 exchange 并绑定一些队列看看是什么效果。 func main() { conn, err := amqp.Dial(url) handlerError(err, \"Failed to connect to RabbitMQ\") defer conn.Close() channel, err := conn.Channel() handlerError(err, \"Failed to open a Channel\") defer channel.Close() directExchangeNameCar := \"direct.car\" if err := channel.ExchangeDeclare(directExchangeNameCar, \"direct\", true, false, false, false, nil); err != nil { handlerError(err, \"Failed to decalare exchange\") } queueNameCar := \"car\" queueNameBigCar := \"big-car\" queueNameMiddleCar := \"middle-car\" queueNameSmallCar := \"small-car\" channel.QueueDeclare(queueNameCar, false, false, false, false, nil) channel.QueueDeclare(queueNameBigCar, false, false, false, false, nil) channel.QueueDeclare(queueNameMiddleCar, false, false, false, false, nil) channel.QueueDeclare(queueNameSmallCar, false, false, false, false, nil) if err := channel.QueueBind(queueNameCar, \"car\", directExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameBigCar, \"car\", directExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameBigCar, \"big.car\", directExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameMiddleCar, \"car\", directExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameMiddleCar, \"middler.car\", directExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameSmallCar, \"car\", directExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameSmallCar, \"small.car\", directExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.Publish(directExchangeNameCar, \"car\", false, false, amqp.Publishing{ContentType: \"text/plain\", Body: []byte(\"test car\")}); err != nil { handlerError(err, \"Failed to publish message\") } } 代码中申明了 1 一个 Exchange ，4个 Queue，7个 Binding ,其","date":"2020-12-16","objectID":"/mq-rabbitmq/:4:1","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"fanout fanout 工作方式类似于广播，看看下面的代码 func main() { conn, err := amqp.Dial(url) handlerError(err, \"Failed to connect to RabbitMQ\") defer conn.Close() channel, err := conn.Channel() handlerError(err, \"Failed to open a Channel\") defer channel.Close() fanoutExchangeNameCar := \"fanout.car\" if err := channel.ExchangeDeclare(fanoutExchangeNameCar, \"fanout\", true, false, false, false, nil); err != nil { handlerError(err, \"Failed to decalare exchange\") } queueNameCar := \"car\" queueNameBigCar := \"big-car\" queueNameMiddleCar := \"middle-car\" queueNameSmallCar := \"small-car\" channel.QueueDeclare(queueNameCar, false, false, false, false, nil) channel.QueueDeclare(queueNameBigCar, false, false, false, false, nil) channel.QueueDeclare(queueNameMiddleCar, false, false, false, false, nil) channel.QueueDeclare(queueNameSmallCar, false, false, false, false, nil) if err := channel.QueueBind(queueNameCar, \"car\", fanoutExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameBigCar, \"car\", fanoutExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameBigCar, \"big.car\", fanoutExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameMiddleCar, \"car\", fanoutExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameMiddleCar, \"middler.car\", fanoutExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameSmallCar, \"car\", fanoutExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameSmallCar, \"small.car\", fanoutExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.Publish(fanoutExchangeNameCar, \"middle.car\", false, false, amqp.Publishing{ContentType: \"text/plain\", Body: []byte(\"test car\")}); err != nil { handlerError(err, \"Failed to publish message\") } } 这个申明了一个 fanout 类型的 exchange ，和上面的代码类似，只有 exchange 不同。 可以先在脑海中想想每个 queue 中有几条消息。 向 fanout.car 这个 exchange 发消息指定 Routekey 为 middle.car ，但是由于是广播模式，所以和 routekey 是没有关系的，每个消息队列中各有一条消息。 请注意有些 binding 指向的是同一个 queue ，那么会产生多条消息到相同的 queue 中，答案是否定的。producer 产生一条消息，根据一定的规则，每个队列只会收到一条(如何符合投递规则的话)。 ","date":"2020-12-16","objectID":"/mq-rabbitmq/:4:2","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"topic topic 比较有意思了，和之前的简单粗暴的用法不一样了，先看看下面的代码，声明了一个 topic 类型的 exchange， 4个 queue func main() { conn, err := amqp.Dial(url) handlerError(err, \"Failed to connect to RabbitMQ\") defer conn.Close() channel, err := conn.Channel() handlerError(err, \"Failed to open a Channel\") defer channel.Close() topicExchangeNameCar := \"topic.car\" if err := channel.ExchangeDeclare(topicExchangeNameCar, \"topic\", true, false, false, false, nil); err != nil { handlerError(err, \"Failed to decalare exchange\") } queueNameCar := \"car\" queueNameBigCar := \"big-car\" queueNameMiddleCar := \"middle-car\" queueNameSmallCar := \"small-car\" channel.QueueDeclare(queueNameCar, false, false, false, false, nil) channel.QueueDeclare(queueNameBigCar, false, false, false, false, nil) channel.QueueDeclare(queueNameMiddleCar, false, false, false, false, nil) channel.QueueDeclare(queueNameSmallCar, false, false, false, false, nil) if err := channel.QueueBind(queueNameCar, \"car\", topicExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameBigCar, \"car\", topicExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameBigCar, \"big.car\", topicExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameMiddleCar, \"car\", topicExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameMiddleCar, \"middler.car\", topicExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameSmallCar, \"car\", topicExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameSmallCar, \"small.car\", topicExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameSmallCar, \"*.small.car\", topicExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } if err := channel.QueueBind(queueNameSmallCar, \"#.small.car\", topicExchangeNameCar, false, nil); err != nil { handlerError(err, \"Failed to bind queue to exchange\") } } 现在思考每个 producer 产生消息之后，会有哪些 queue 会收到消息。 if err := channel.Publish(topicExchangeNameCar, \"car\", false, false, amqp.Publishing{ContentType: \"text/plain\", Body: []byte(\"test car\")}); err != nil { handlerError(err, \"Failed to publish message\") } 每个 queue 都会收到消息 if err := channel.Publish(topicExchangeNameCar, \"small.car\", false, false, amqp.Publishing{ContentType: \"text/plain\", Body: []byte(\"test car\")}); err != nil { handlerError(err, \"Failed to publish message\") } small-car 这一个队列会收到消息。 符合 Routekey 为 small.car 、*.small.car、#.small.car 的binding if err := channel.Publish(topicExchangeNameCar, \"benz.small.car\", false, false, amqp.Publishing{ContentType: \"text/plain\", Body: []byte(\"test car\")}); err != nil { handlerError(err, \"Failed to publish message\") } small-car 这一个队列会收到消息。 符合 Routekey 为 *.small.car、#.small.car 的binding if err := channel.Publish(topicExchangeNameCar, \"auto.blue.benz.small.car\", false, false, amqp.Publishing{ContentType: \"text/plain\", Body: []byte(\"test car\")}); err != nil { handlerError(err, \"Failed to publish message\") } small-car 这一个队列会收到消息。 符合 Routekey 为 #.small.car 的binding if err := channel.Publish(topicExchangeNameCar, \"bike\", false, false, amqp.Publishing{ContentType: \"text/plain\", Body: []byte(\"test car\")}); err != nil { handlerError(err, \"Failed to publish message\") } 都不会收到消息，没有符合的 routekey 。 ","date":"2020-12-16","objectID":"/mq-rabbitmq/:4:3","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"headers 这种类型很少有实际的应用场景。 ","date":"2020-12-16","objectID":"/mq-rabbitmq/:4:4","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"参考 https://www.rabbitmq.com/documentation.html ","date":"2020-12-16","objectID":"/mq-rabbitmq/:5:0","tags":["rabbitmq","消息队列"],"title":"消息队列原理之rabbitmq","uri":"/mq-rabbitmq/"},{"categories":null,"content":"深入浅出的聊聊 cpu 负载与使用率","date":"2020-12-15","objectID":"/cpu/","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"导读 在定位性能问题时，一个绕不开的话题就是 CPU ，会觉得 CPU 不够用了，或者是瓶颈了，那么怎么来确定是 CPU 的问题呢？衡量 CPU 的指标有两个，CPU 负载(load average) 和 使用率，这两者有什么关系和区别呢？ 这篇文章带大家深层次的了解一下 CPU 。 ","date":"2020-12-15","objectID":"/cpu/:1:0","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"什么是CPU CPU 就是计算机的中央处理器(Central Processing Unit)，其功能主要是解释计算机指令以及处理计算机软件中的数据。CPU是计算机中负责读取指令，对指令译码并执行指令的核心部件。中央处理器主要包括两个部分，即控制器、运算器，其中还包括高速缓冲存储器及实现它们之间联系的数据、控制的总线。电子计算机三大核心部件就是CPU、内部存储器、输入/输出设备。中央处理器的功效主要为处理指令、执行操作、控制时间、处理数据。 来自百度百科上的定义。 ","date":"2020-12-15","objectID":"/cpu/:2:0","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"CPU 结构 上面这张图片描述了现在的CPU的基本情况，一个物理核心中包含多个核心(多核结构)。这张图表示一个物理核心中包含4个核心，而现在的 CPU 都会用到超线程技术，Inter 一般是把一个核分成2个，所以上这个就代表 8 核(8=1 * 4 * 2) 。 超线程技术把多线程处理器内部的两个逻辑内核模拟成两个物理芯片，让单个处理器就能使用线程级的并行计算，进而兼容多线程操作系统和软件。超线程技术充分利用空闲CPU资源，在相同时间内完成更多工作。 超线程技术主要的出发点是，当处理器在运行一个线程，执行指令代码时，很多时候处理器并不会使用到全部的计算能力，部分计算能力就会处于空闲状态。而超线程技术就是通过多线程来进一步“压榨”处理器。举个例子，如果一个线程运行过程中，必须要等到一些数据加载到缓存中以后才能继续执行，此时CPU就可以切换到另一个线程，去执行其他指令，而不用去处于空闲状态，等待当前线程的数据加载完毕。 通常，一个传统的处理器在线程之间切换，可能需要几万个时钟周期。而一个具有HT超线程技术的处理器只需要1个时钟周期。因此就大大减小了线程之间切换的成本，从而最大限度地让处理器满负荷运转。 所以得出一个结论: 总核数 = 物理核数 * 每个物理核心的核数 * 超线程数 ","date":"2020-12-15","objectID":"/cpu/:3:0","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"cpu 信息查看 在 unix 系统下查看 CPU 详细信息的方法为 cat /proc/cpuinfo : [root@iZuf685opgs9oyozju9i2bZ ~]# cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 85 model name : Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz stepping : 7 microcode : 0x1 cpu MHz : 2499.998 cache size : 36608 KB physical id : 0 siblings : 2 core id : 0 cpu cores : 1 apicid : 0 initial apicid : 0 fpu : yes fpu_exception : yes cpuid level : 22 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni bogomips : 4999.99 clflush size : 64 cache_alignment : 64 address sizes : 46 bits physical, 48 bits virtual power management: processor : 1 vendor_id : GenuineIntel cpu family : 6 model : 85 model name : Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz stepping : 7 microcode : 0x1 cpu MHz : 2499.998 cache size : 36608 KB physical id : 0 siblings : 2 core id : 0 cpu cores : 1 apicid : 1 initial apicid : 1 fpu : yes fpu_exception : yes cpuid level : 22 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni bogomips : 4999.99 clflush size : 64 cache_alignment : 64 address sizes : 46 bits physical, 48 bits virtual power management: 上面这是一个 1一个物理核心，一个物理核心只有一个核，一个核上有两个超线程 的 CPU 详细信息。 关键指标说明: processor: 逻辑核心的序号，默认从 0 开始，对于单核处理器，则可以认为是其CPU编号，对于多核处理器则可以是物理核、或者使用超线程技术虚拟的逻辑核。 vendor_id: CPU 制造商 cpu family: CPU 产品系列代号 model: CPU属于其系列中的哪一代的代号 model name: CPU属于的名字及其编号、标称主频 stepping: CPU属于制作更新版本 cpu MHz: CPU的实际使用主频 cache size: CPU二级缓存大小 physical id: 单个CPU的标号 siblings: 单个CPU逻辑物理核数 core id: 当前物理核在其所处CPU中的编号，这个编号不一定连续 cpu cores: 该逻辑核所处CPU的物理核数 apicid: 用来区分不同逻辑核的编号，系统中每个逻辑核的此编号必然不同，此编号不一定连续 fpu: 是否具有浮点运算单元（Floating Point Unit） fpu_exception: 是否支持浮点计算异常 cpuid level : 执行cpuid指令前，eax寄存器中的值，根据不同的值cpuid指令会返回不同的内容 wp: 表明当前CPU是否在内核态支持对用户空间的写保护（Write Protection） flags: 当前CPU支持的功能 bogomips: 在系统内核启动时粗略测算的CPU速度（Million Instructions Per Second） clflush size: 每次刷新缓存的大小单位 cache_alignment: 缓存地址对齐单位 address sizes: 可访问地址空间位数 power management: 对能源管理的支持 对于 cpu 的常用操作如下: 查看 CPU 型号: [root@iZuf685opgs9oyozju9i2bZ ~]# cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 2 Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz 查看物理 CPU 个数 [root@iZuf685opgs9oyozju9i2bZ ~]# cat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l 1 查看每个物理 CPU 中 core 的个数(即核数) [root@iZuf685opgs9oyozju9i2bZ ~]# cat /proc/cpuinfo| grep \"cpu cores\"| uniq cpu cores : 1 查看逻辑 CPU 的个数 [root@iZuf685opgs9oyozju9i2bZ ~]# cat /proc/cpuinfo| grep \"processor\"| wc -l 2 查看 CPU 是运行在 32 位还是 64 位模式下 [root@iZuf685opgs9oyozju9i2bZ ~]# getconf LONG_BIT 64 ","date":"2020-12-15","objectID":"/cpu/:3:1","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"CPU 负载 在 unix 系统下可以通过 top 命令看到3个值 : [root@iZuf685opgs9oyozju9i2bZ ~]# top top - 21:18:36 up 35 days, 12:39, 1 user, load average: 0.00, 0.01, 0.05 Tasks: 99 total, 1 running, 98 sleeping, 0 stopped, 0 zombie ... load average: 0.00, 0.01, 0.05 表示系统在最近 1、5、15分钟内的平均负载。那么什么是负载呢 ? CPU 负载指的是: 系统在一段时间内正在使用和等待使用CPU的平均任务数。描述的是任务的排队情况。 借用网上的一个例子：公用电话 把CPU比作电话亭，把任务比作排队打电话的人。有一堆人排队打电话，每个人只允许打1分钟的电话，如果时间到了还没有打完还是需要重新去排队。在打电话的时候，肯定会遇到排队等待电话的人，也有打完电话走掉的人，也有新来排队的人，也有打完1分钟后没打完又重新排队的人。那这个人数的变化就相当于任务的增减。为了统计平均负载状态，每分钟统计一次，计算最近1、5、15分钟的平均值。 load低并不意味着CPU的利用率低，有的人(任务)拿起电话(CPU)一直打完1分钟，这时候 cpu 使用率为100%，有的人(任务)拿起电话(CPU)一直犹豫是否要打或者在找手机号，30秒后才拨通了电话，只有后30秒是真正在打电话，这时候cpu使用率为50% ，当然实际情况可能会有偏差。 ","date":"2020-12-15","objectID":"/cpu/:4:0","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"CPU 使用率 CPU 使用率是程序在运行期间实时占用的CPU百分比。描述的是 cpu 的繁忙情况。 cpu 使用率高不一定负载高，看看下面的代码: func main() { for { num1 := 1 num2 := 1 num3 := num1 + num2 fmt.Println(num3) } } 这个程序会一直占着 cpu ，如果是单核的，cpu 使用率为 100%，负载为1。 ","date":"2020-12-15","objectID":"/cpu/:5:0","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"负载与使用率分析 ","date":"2020-12-15","objectID":"/cpu/:6:0","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"负载高、使用率低 说明等待执行的任务很多。很可能是进程僵死了。通过命令 ps -aux 查看是否存在D状态的进程，该状态为不可中断的睡眠状，态。处于D状态的进程通常是在等待IO，通常是IO密集型任务，如果大量请求都集中于相同的IO设备，超出设备的响应能力，会造成任务在运行队列里堆积等待，也就是D状态的进程堆积，那么此时Load Average就会飙高。 ","date":"2020-12-15","objectID":"/cpu/:6:1","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"利用率高、负载低 说明任务少，但是任务执行时间长，有可能是程序本身有问题，如果没有问题那么计算完成后则利用率会下降。这种场景，通常是计算密集型任务，即大量生成耗时短的计算任务。 ","date":"2020-12-15","objectID":"/cpu/:6:2","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"使用率低、负载低、IOPS 高 通常是低频大文件读写，由于请求数量不大，所以任务都处于R状态(表示正在运行，或者处于运行队列，可以被调度运行)，负载数值反映了当前运行的任务数，不会飙升，IO设备处于满负荷工作状态，导致系统响应能力降低。 ","date":"2020-12-15","objectID":"/cpu/:6:3","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"总结 CPU 就是计算机的中央处理器(Central Processing Unit)，其功能主要是解释计算机指令以及处理计算机软件中的数据 总核数 = 物理核数 * 每个物理核心的核数 * 超线程数 CPU 负载是系统在一段时间内正在使用和等待使用CPU的平均任务数。描述的是任务的排队情况。 CPU 使用率是程序在运行期间实时占用的CPU百分比。描述的是 cpu 的繁忙情况。 CPU 负载高并不能说明 CPU 使用率高，反之亦然。 ","date":"2020-12-15","objectID":"/cpu/:7:0","tags":["linux","cpu"],"title":"深入浅出的聊聊 cpu 负载与使用率","uri":"/cpu/"},{"categories":null,"content":"git 常用命令","date":"2020-12-14","objectID":"/git-command/","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"导读 这篇文章主要记录了 git 的一些常用命令，后续会持续补充更新。 ","date":"2020-12-14","objectID":"/git-command/:1:0","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"常用命令 ","date":"2020-12-14","objectID":"/git-command/:2:0","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"检出代码 git clone url -b git_branch ","date":"2020-12-14","objectID":"/git-command/:2:1","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"查看分支 git branch -a ","date":"2020-12-14","objectID":"/git-command/:2:2","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"创建分支 git branch xxx ","date":"2020-12-14","objectID":"/git-command/:2:3","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"删除本地分支 git branch -d xxxxx ","date":"2020-12-14","objectID":"/git-command/:2:4","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"检出分支 git checkout git_branch ","date":"2020-12-14","objectID":"/git-command/:2:5","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"拉取代码 git pull ","date":"2020-12-14","objectID":"/git-command/:2:6","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"把修改文件提交到缓冲区 git add \u003cfilename\u003e ","date":"2020-12-14","objectID":"/git-command/:2:7","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"本地提交 git commit -m \"代码提交信息\" ","date":"2020-12-14","objectID":"/git-command/:2:8","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"推送代码 git push origin local_branch:remote_branch 例 : git push origin release/release:release/release ","date":"2020-12-14","objectID":"/git-command/:2:9","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"合并代码 git merge origin/remote ","date":"2020-12-14","objectID":"/git-command/:2:10","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"cherry pick git cherry-pick commit_id ","date":"2020-12-14","objectID":"/git-command/:2:11","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"跟踪 git branch --set-upstream-to=remote_branch local_branch 例 git branch --set-upstream-to=origin/release/release release/release ","date":"2020-12-14","objectID":"/git-command/:2:12","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"丢弃本地修改 git checkout -- file 例 git checkout -- test.py ","date":"2020-12-14","objectID":"/git-command/:2:13","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"取消本地提交 - git log 查找需要恢复的 commit_id - git reset --hard commit_id ","date":"2020-12-14","objectID":"/git-command/:2:14","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"本地清除git上已经删除的分支 git remote prune origin ","date":"2020-12-14","objectID":"/git-command/:2:15","tags":["git","cli"],"title":"git 常用命令","uri":"/git-command/"},{"categories":null,"content":"docker 原理之本地存储","date":"2020-12-09","objectID":"/docker-local-storage/","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"导读 在前面的文章docker 原理之存储驱动中简单的介绍了 Docker 的存储驱动，这篇文章接着讲存储，目前的 docker 版本中默认的是 overlay2 ，所以这篇文章就以 overlay2 为例带大家看看，在我们执行 docker build ，docker pull，docker run 等命令时本地存储有何变化。 这篇文章比较长，如果看不完可以收藏起来后续需要用到的时候再查阅，称的上是干货满满，作者自己整理也花了较长的时间。 ","date":"2020-12-09","objectID":"/docker-local-storage/:1:0","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"背景 查看 docker Storage Driver 可以通过 docker info | grep \"Storage Driver\"命令。 docker 的默认安装目录为： /var/lib/docker，如果要修改可以通过修改启动时的配置文件(默认为/usr/lib/systemd/system/docker.service) 中的 ExecStart， 查看 docker 启动时的配置文件: 修改 docker 的存储目录: 修改(增加) --graph 即可。 ","date":"2020-12-09","objectID":"/docker-local-storage/:2:0","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"本地目录 [root@iZuf685opgs9oyozju9i2bZ docker]# ll 总用量 48 drwx------ 2 root root 4096 11月 11 08:49 builder drwx--x--x 4 root root 4096 11月 11 08:49 buildkit drwx------ 3 root root 4096 12月 2 09:25 containers drwx------ 3 root root 4096 11月 11 08:49 image drwxr-x--- 3 root root 4096 11月 11 08:49 network drwx------ 9 root root 4096 12月 2 09:25 overlay2 drwx------ 4 root root 4096 11月 11 08:49 plugins drwx------ 2 root root 4096 11月 11 08:49 runtimes drwx------ 2 root root 4096 11月 11 08:49 swarm drwx------ 2 root root 4096 11月 11 13:32 tmp drwx------ 2 root root 4096 11月 11 08:49 trust drwx------ 2 root root 4096 11月 11 08:49 volumes 可以用 tree 进行展开 [root@iZuf685opgs9oyozju9i2bZ docker]# tree -L 2 . ├── builder │ └── fscache.db ├── buildkit │ ├── cache.db │ ├── content │ ├── executor │ ├── metadata.db │ └── snapshots.db ├── containers │ └── 9bd6ac07a8c962e2403203e1c45f4fb54733f9953cf318b34fc3f155bf2c0c59 ├── image │ └── overlay2 ├── network │ └── files ├── overlay2 │ ├── 00b65b9c288df8c0ae7cdacba531a7dc5cb006e6c768e19ee36055717b782acc │ ├── 1e53dddb1a0bb04ee4ebd24a8edb94b96e2fd471a72bf1b8608096b38cb16646 │ ├── 1e53dddb1a0bb04ee4ebd24a8edb94b96e2fd471a72bf1b8608096b38cb16646-init │ ├── 5da215c4f218cbb1d9825fa111c21bf381dc35a9e6c7c6cd5c3ea952316031e4 │ ├── 8cbfb8b74c887e780747c8e6f4b3b9223a513ff6d69770bac16abb76da4e314f │ ├── f1cf8b173467c98e08f3d276d7ccd8f9892c7c71dec2c4b335c39c6f175ae744 │ └── l ├── plugins │ ├── storage │ └── tmp ├── runtimes ├── swarm ├── tmp ├── trust └── volumes └── metadata.db 26 directories, 5 files 这篇文章以分析存储为主，涉及到的目录有 image,containers,overlay2，其他的目录放在后续的文章讨论。 在真正开始之前，先想想几个问题(这也是我自己问我自己的问题) : docker build 的过程是怎样的? docker pull 和 docker build 产生的镜像存放在哪了？ docker run 运行一个容器的时候过程是怎么样的? 带着这些问题我们以一个例子进行说明 russellgao/openresty:1.17.8.2-5-alpine ","date":"2020-12-09","objectID":"/docker-local-storage/:3:0","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"image image 目录主要存放的镜像相关的信息，我们执行 docker pull russellgao/openresty:1.17.8.2-5-alpine 看看： [root@iZuf685opgs9oyozju9i2bZ docker]# docker pull russellgao/openresty:1.17.8.2-5-alpine 1.17.8.2-5-alpine: Pulling from russellgao/openresty df20fa9351a1: Already exists 5682af42731d: Pull complete 7c6cb2b54a9d: Pull complete aa74dc345098: Pull complete Digest: sha256:224ced85b5f8b679a8664a39b69c1b8feb09f8ba4343d834bd5b69433081389e Status: Downloaded newer image for openresty/openresty:1.17.8.2-5-alpine 可以看到 pull 了 4 层下来了，我们看看 image 目录: [root@iZuf685opgs9oyozju9i2bZ docker]# tree image image └── overlay2 ├── distribution │ ├── diffid-by-digest │ │ └── sha256 │ │ ├── 5682af42731d652bd98d2456ed3da4f0595ed5d9e5b13ac8bb9590bb74f72eb8 │ │ ├── 7c6cb2b54a9d9d40c4a03dd6615b1c8e791feb5d81464a7702a9bb921f7a73e9 │ │ ├── aa74dc3450985aee599c181d650da8f8880ca1d6e2bc01a43831ca59b6e2a7b6 │ │ └── df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c │ └── v2metadata-by-diffid │ └── sha256 │ ├── 1680a9f16b18732726d0656b6d6ff9611a3c4460ca870827b537a87bbe10cc22 │ ├── 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a │ ├── 8521b614863046bf4bb604e3586feeca8b7ce1372f1d6664a5545e85ad9ca472 │ └── 9c572ba82b91e3ac35c7351bdacc6876c67f5d9bc69c5e51e8b2deeafae95e4f ├── imagedb │ ├── content │ │ └── sha256 │ │ └── 1ddc7a18ba0bcc20c61447f391bfff98ac559eea590e7ac59b5b5f588f1f47ed │ └── metadata │ └── sha256 │ └── 1ddc7a18ba0bcc20c61447f391bfff98ac559eea590e7ac59b5b5f588f1f47ed │ └── lastUpdated ├── layerdb │ ├── mounts │ │ └── 9bd6ac07a8c962e2403203e1c45f4fb54733f9953cf318b34fc3f155bf2c0c59 │ │ ├── init-id │ │ ├── mount-id │ │ └── parent │ ├── sha256 │ │ ├── 228fb92e31891f472e9857ee11d13c404ff7c88e808b05ce4ebdc80d785d71f3 │ │ │ ├── cache-id │ │ │ ├── diff │ │ │ ├── parent │ │ │ ├── size │ │ │ └── tar-split.json.gz │ │ ├── 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a │ │ │ ├── cache-id │ │ │ ├── diff │ │ │ ├── size │ │ │ └── tar-split.json.gz │ │ ├── 5f72760956a669e4c9b33aa3f2f04baa84b0f4cf1e11676049981bafcbba74da │ │ │ ├── cache-id │ │ │ ├── diff │ │ │ ├── parent │ │ │ ├── size │ │ │ └── tar-split.json.gz │ │ └── fe267088885017d5e9a4621e68617a7f35e58dc2d0d747927882da21059854e3 │ │ ├── cache-id │ │ ├── diff │ │ ├── parent │ │ ├── size │ │ └── tar-split.json.gz │ └── tmp └── repositories.json 21 directories, 33 files 看看 repositories.json 中是内容 : [root@iZuf685opgs9oyozju9i2bZ docker]# cat image/overlay2/repositories.json | jq . { \"Repositories\": { \"openresty/openresty\": { \"openresty/openresty:1.17.8.2-5-alpine\": \"sha256:1ddc7a18ba0bcc20c61447f391bfff98ac559eea590e7ac59b5b5f588f1f47ed\", \"openresty/openresty@sha256:224ced85b5f8b679a8664a39b69c1b8feb09f8ba4343d834bd5b69433081389e\": \"sha256:1ddc7a18ba0bcc20c61447f391bfff98ac559eea590e7ac59b5b5f588f1f47ed\" }, \"russellgao/openresty\": { \"russellgao/openresty:1.17.8.2-5-alpine\": \"sha256:1ddc7a18ba0bcc20c61447f391bfff98ac559eea590e7ac59b5b5f588f1f47ed\", \"russellgao/openresty@sha256:84f53dc7517e9b6695fc8fd74916a1eb5970a92fc24a984f99bfb81508f3d261\": \"sha256:1ddc7a18ba0bcc20c61447f391bfff98ac559eea590e7ac59b5b5f588f1f47ed\" } } } repositories.json 中记录了这个机器上所有的镜像，可以看到这里有两个镜像 openresty/openresty:1.17.8.2-5-alpine 和 russellgao/openresty:1.17.8.2-5-alpine ，但其实只有一个镜像，因为后面的 imageid 是相同的，这个可以 docker images 验证一下 [root@iZuf685opgs9oyozju9i2bZ docker]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE openresty/openresty 1.17.8.2-5-alpine 1ddc7a18ba0b 2 months ago 104MB russellgao/openresty 1.17.8.2-5-alpine 1ddc7a18ba0b 2 months ago 104MB openresty/openresty:1.17.8.2-5-alpine 和 russellgao/openresty:1.17.8.2-5-alpine 只是镜像 1ddc7a18ba0b 的 tag 。 那么 1ddc7a18ba0b 镜像是怎么组成的呢? image/overlay2/ 下面除了 repositories.json 还有3个目录 distribution,imagedb,layerdb，作用分别如下: distribution: 主要和镜像仓库的交互相关 imagedb: 保存了镜像的元数据 layerdb: 保存了镜像layer(层) 的数据 image/overlay2/ 保存的是数据的链接，真正的镜像数据是存放在 overlay2 目录下，先看看 distribution : ","date":"2020-12-09","objectID":"/docker-local-storage/:3:1","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"distribution [root@iZuf685opgs9oyozju9i2bZ docker]# tree image/overlay2/distribution/ image/overlay2/distribution/ ├── diffid-by-digest │ └── sha256 │ ├── 5682af42731d652bd98d2456ed3da4f0595ed5d9e5b13ac8bb9590bb74f72eb8 │ ├── 7c6cb2b54a9d9d40c4a03dd6615b1c8e791feb5d81464a7702a9bb921f7a73e9 │ ├── aa74dc3450985aee599c181d650da8f8880ca1d6e2bc01a43831ca59b6e2a7b6 │ └── df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c └── v2metadata-by-diffid └── sha256 ├── 1680a9f16b18732726d0656b6d6ff9611a3c4460ca870827b537a87bbe10cc22 ├── 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a ├── 8521b614863046bf4bb604e3586feeca8b7ce1372f1d6664a5545e85ad9ca472 └── 9c572ba82b91e3ac35c7351bdacc6876c67f5d9bc69c5e51e8b2deeafae95e4f 4 directories, 8 files 请注意看 image/overlay2/distribution/diffid-by-digest/sha256 下面，回过头再看看 docker pull 的过程，这里的就是 digestid ，docker pull 的时候也是通过 digestid 实现的，这个id对应的是 docker repository 中的 blob id，在 docker repository 的 blobs 目录下可以找到。 可以查看具体的文件，如 cat image/overlay2/distribution/diffid-by-digest/sha256/5682af42731d652bd98d2456ed3da4f0595ed5d9e5b13ac8bb9590bb74f72eb8 [root@iZuf685opgs9oyozju9i2bZ docker]# cat image/overlay2/distribution/diffid-by-digest/sha256/5682af42731d652bd98d2456ed3da4f0595ed5d9e5b13ac8bb9590bb74f72eb8 sha256:9c572ba82b91e3ac35c7351bdacc6876c67f5d9bc69c5e51e8b2deeafae95e4f 不难发现它们之间是相互引用的，可以实现 diffid 和 digest 的相互转换。 ","date":"2020-12-09","objectID":"/docker-local-storage/:3:2","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"imagedb [root@iZuf685opgs9oyozju9i2bZ docker]# tree image/overlay2/imagedb/ image/overlay2/imagedb/ ├── content │ └── sha256 │ └── 1ddc7a18ba0bcc20c61447f391bfff98ac559eea590e7ac59b5b5f588f1f47ed └── metadata └── sha256 └── 1ddc7a18ba0bcc20c61447f391bfff98ac559eea590e7ac59b5b5f588f1f47ed └── lastUpdated 5 directories, 2 files 可以看到 imagedb 是以镜像为单位进行存储的，看一下 content 下面的具体内容 : [root@iZuf685opgs9oyozju9i2bZ docker]# cat image/overlay2/imagedb/content/sha256/1ddc7a18ba0bcc20c61447f391bfff98ac559eea590e7ac59b5b5f588f1f47ed | jq . { \"architecture\": \"amd64\", \"config\": { \"Hostname\": \"\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/openresty/luajit/bin:/usr/local/openresty/nginx/sbin:/usr/local/openresty/bin\" ], \"Cmd\": [ \"/usr/local/openresty/bin/openresty\", \"-g\", \"daemon off;\" ], \"ArgsEscaped\": true, \"Image\": \"sha256:0b827067ad09ab8a0b9a73a45f5b1c408b84db1ca6883a4c544078ed43b8b5e3\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": { \"maintainer\": \"Evan Wies \u003cevan@neomantra.net\u003e\", \"resty_add_package_builddeps\": \"\", \"resty_add_package_rundeps\": \"\", \"resty_config_deps\": \"--with-pcre --with-cc-opt='-DNGX_LUA_ABORT_AT_PANIC -I/usr/local/openresty/pcre/include -I/usr/local/openresty/openssl/include' --with-ld-opt='-L/usr/local/openresty/pcre/lib -L/usr/local/openresty/openssl/lib -Wl,-rpath,/usr/local/openresty/pcre/lib:/usr/local/openresty/openssl/lib' \", \"resty_config_options\": \" --with-compat --with-file-aio --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_geoip_module=dynamic --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module=dynamic --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-http_xslt_module=dynamic --with-ipv6 --with-mail --with-mail_ssl_module --with-md5-asm --with-pcre-jit --with-sha1-asm --with-stream --with-stream_ssl_module --with-threads \", \"resty_config_options_more\": \"\", \"resty_eval_post_make\": \"\", \"resty_eval_pre_configure\": \"\", \"resty_image_base\": \"alpine\", \"resty_image_tag\": \"3.12\", \"resty_openssl_patch_version\": \"1.1.1f\", \"resty_openssl_url_base\": \"https://www.openssl.org/source\", \"resty_openssl_version\": \"1.1.1g\", \"resty_pcre_version\": \"8.44\", \"resty_version\": \"1.17.8.2\" }, \"StopSignal\": \"SIGQUIT\" }, \"container\": \"0ae35046dd1afef0f1f525360939abc524dbd469a92470c5836dfbb7dc666923\", \"container_config\": { \"Hostname\": \"0ae35046dd1a\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/openresty/luajit/bin:/usr/local/openresty/nginx/sbin:/usr/local/openresty/bin\" ], \"Cmd\": [ \"/bin/sh\", \"-c\", \"#(nop) \", \"STOPSIGNAL SIGQUIT\" ], \"ArgsEscaped\": true, \"Image\": \"sha256:0b827067ad09ab8a0b9a73a45f5b1c408b84db1ca6883a4c544078ed43b8b5e3\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": { \"maintainer\": \"Evan Wies \u003cevan@neomantra.net\u003e\", \"resty_add_package_builddeps\": \"\", \"resty_add_package_rundeps\": \"\", \"resty_config_deps\": \"--with-pcre --with-cc-opt='-DNGX_LUA_ABORT_AT_PANIC -I/usr/local/openresty/pcre/include -I/usr/local/openresty/openssl/include' --with-ld-opt='-L/usr/local/openresty/pcre/lib -L/usr/local/openresty/openssl/lib -Wl,-rpath,/usr/local/openresty/pcre/lib:/usr/local/openresty/openssl/lib' \", \"resty_config_options\": \" --with-compat --with-file-aio --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_geoip_module=dynamic --","date":"2020-12-09","objectID":"/docker-local-storage/:3:3","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"layerdb 前面我们说过，imagedb 存的是元数据，那么 layerdb 应该存的是 layer 相关信息?先看看这个目录下面有什么: [root@iZuf685opgs9oyozju9i2bZ docker]# ll image/overlay2/layerdb/ 总用量 12 drwxr-xr-x 3 root root 4096 12月 2 09:25 mounts drwxr-xr-x 6 root root 4096 11月 11 13:32 sha256 drwxr-xr-x 2 root root 4096 11月 11 13:32 tmp tmp tmp 是一个临时目录 sha256 sha256: 先看看这个下面有什么 ll image/overlay2/layerdb/sha256/ [root@iZuf685opgs9oyozju9i2bZ docker]# ll image/overlay2/layerdb/sha256/ 总用量 16 drwx------ 2 root root 4096 11月 11 13:32 228fb92e31891f472e9857ee11d13c404ff7c88e808b05ce4ebdc80d785d71f3 drwx------ 2 root root 4096 11月 11 13:31 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a drwx------ 2 root root 4096 11月 11 13:32 5f72760956a669e4c9b33aa3f2f04baa84b0f4cf1e11676049981bafcbba74da drwx------ 2 root root 4096 11月 11 13:32 fe267088885017d5e9a4621e68617a7f35e58dc2d0d747927882da21059854e3 咋一看这里和 rootfs 中的 diff_id 并不相同，只有一层是一样的(只有base layer是相同)。这里的是 chain_id ，那么什么是 chain_id呢？ diff_id: 描述的是某一层的变化 chain_id: 描述的是一系列变化 diff_id 和 chain_id 的计算公式为: ChainID(A) = DiffID(A) ChainID(A | B) = Digest(ChainID(A) + \" \" + DiffID(B)) ChainID(A | B | C) = Digest(ChainID(A | B) + \" \" + DiffID(C)) 是不是有点绕，回到我们的例子看看： rootfs 中的 diff_id 为: \"diff_ids\": [ \"sha256:50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a\", \"sha256:9c572ba82b91e3ac35c7351bdacc6876c67f5d9bc69c5e51e8b2deeafae95e4f\", \"sha256:1680a9f16b18732726d0656b6d6ff9611a3c4460ca870827b537a87bbe10cc22\", \"sha256:8521b614863046bf4bb604e3586feeca8b7ce1372f1d6664a5545e85ad9ca472\" ] chain_id 为: drwx------ 2 root root 4096 11月 11 13:32 228fb92e31891f472e9857ee11d13c404ff7c88e808b05ce4ebdc80d785d71f3 drwx------ 2 root root 4096 11月 11 13:31 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a drwx------ 2 root root 4096 11月 11 13:32 5f72760956a669e4c9b33aa3f2f04baa84b0f4cf1e11676049981bafcbba74da drwx------ 2 root root 4096 11月 11 13:32 fe267088885017d5e9a4621e68617a7f35e58dc2d0d747927882da21059854e3 根据上面的公式 base layer 的 diff_id 和 chain_id 是相同的: 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a -\u003e 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a 在继续看看下面的算法 [root@iZuf685opgs9oyozju9i2bZ docker]# echo -n \"sha256:50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a sha256:9c572ba82b91e3ac35c7351bdacc6876c67f5d9bc69c5e51e8b2deeafae95e4f\" | sha256sum fe267088885017d5e9a4621e68617a7f35e58dc2d0d747927882da21059854e3 - [root@iZuf685opgs9oyozju9i2bZ docker]# [root@iZuf685opgs9oyozju9i2bZ docker]# echo -n \"sha256:fe267088885017d5e9a4621e68617a7f35e58dc2d0d747927882da21059854e3 sha256:1680a9f16b18732726d0656b6d6ff9611a3c4460ca870827b537a87bbe10cc22\" | sha256sum 5f72760956a669e4c9b33aa3f2f04baa84b0f4cf1e11676049981bafcbba74da - [root@iZuf685opgs9oyozju9i2bZ docker]# echo -n \"sha256:5f72760956a669e4c9b33aa3f2f04baa84b0f4cf1e11676049981bafcbba74da sha256:8521b614863046bf4bb604e3586feeca8b7ce1372f1d6664a5545e85ad9ca472\" | sha256sum 228fb92e31891f472e9857ee11d13c404ff7c88e808b05ce4ebdc80d785d71f3 - 这么一演算就事情就变的清晰起来了: 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a -\u003e 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a 9c572ba82b91e3ac35c7351bdacc6876c67f5d9bc69c5e51e8b2deeafae95e4f -\u003e fe267088885017d5e9a4621e68617a7f35e58dc2d0d747927882da21059854e3 1680a9f16b18732726d0656b6d6ff9611a3c4460ca870827b537a87bbe10cc22 -\u003e 5f72760956a669e4c9b33aa3f2f04baa84b0f4cf1e11676049981bafcbba74da 8521b614863046bf4bb604e3586feeca8b7ce1372f1d6664a5545e85ad9ca472 -\u003e 228fb92e31891f472e9857ee11d13c404ff7c88e808b05ce4ebdc80d785d71f3 理清它们之间的关系就比较好办了，在看看 chain_id 目录下都有什么: [root@iZuf685opgs9oyozju9i2bZ docker]# tree image/overlay2/layerdb/sha256/ image/overlay2/layerdb/sha256/ ├── 228fb92e31891f472e9857ee11d13c404ff7c88e808b05ce4ebdc80d785d71f3 │ ├── cache-id │ ├── diff │ ├── parent │ ├── size │ └── tar-split.json.gz ├── 50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a │ ├── cache-id │ ├── diff │ ├── size │ └── tar-split.json.gz ├── 5f72760956a669e4c9b33aa3f2f04baa84b","date":"2020-12-09","objectID":"/docker-local-storage/:3:4","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"overlay2 overlay2 目录存放的每一层的具体数据，先看看目录结构: [root@iZuf685opgs9oyozju9i2bZ docker]# tree overlay2/ -L 2 [root@iZuf685opgs9oyozju9i2bZ docker]# tree -L 2 overlay2/ overlay2/ ├── 00b65b9c288df8c0ae7cdacba531a7dc5cb006e6c768e19ee36055717b782acc │ ├── committed │ ├── diff │ ├── link │ ├── lower │ └── work ├── 5da215c4f218cbb1d9825fa111c21bf381dc35a9e6c7c6cd5c3ea952316031e4 │ ├── committed │ ├── diff │ ├── link │ ├── lower │ └── work ├── 8cbfb8b74c887e780747c8e6f4b3b9223a513ff6d69770bac16abb76da4e314f │ ├── committed │ ├── diff │ └── link ├── b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc │ ├── diff │ ├── link │ ├── lower │ ├── merged │ └── work ├── b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc-init │ ├── committed │ ├── diff │ ├── link │ ├── lower │ └── work ├── f1cf8b173467c98e08f3d276d7ccd8f9892c7c71dec2c4b335c39c6f175ae744 │ ├── committed │ ├── diff │ ├── link │ ├── lower │ └── work └── l ├── 2NFRHNZBFYCUAPMTFCKUR5R4DS -\u003e ../b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc/diff ├── 33RV26M4VMX3ZUISOG26USXBKR -\u003e ../f1cf8b173467c98e08f3d276d7ccd8f9892c7c71dec2c4b335c39c6f175ae744/diff ├── BGEYC7V6ULKFOOIITWCEKITQEU -\u003e ../b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc-init/diff ├── HG76ICE67NTXFL7AYXCMI3EK4Y -\u003e ../00b65b9c288df8c0ae7cdacba531a7dc5cb006e6c768e19ee36055717b782acc/diff ├── L5XCYQVZ6DOSLJNP6HXCZQZ7A5 -\u003e ../5da215c4f218cbb1d9825fa111c21bf381dc35a9e6c7c6cd5c3ea952316031e4/diff └── MUHXHRXFSGNCBKQ2AUXDFOUDLF -\u003e ../8cbfb8b74c887e780747c8e6f4b3b9223a513ff6d69770bac16abb76da4e314f/diff 25 directories, 16 files 可以看到 overlay2 一级目录下有个特殊目录 l , l 下是各个layer 软链接，防止mount 命令太长而发生错误，所以就用短链接了。 细心的你一定发现了这么几个问题: 这个镜像只有 4 个层，这里为啥会有 6 个层(目录) 为啥具体layer 下的目录/文件结构不一样 有的 layer 为啥带了 -init 后缀，有的没有 有 6 个layer 层是因为我这里启动了一个容器，会生成两个层(读写层和这个镜像merged之后的只读层)，删除容器之后看看: [root@iZuf685opgs9oyozju9i2bZ docker]# docker rm -f openresty-app-1 openresty-app-1 [root@iZuf685opgs9oyozju9i2bZ docker]# ll overlay2/ 总用量 20 drwx------ 4 root root 4096 11月 11 13:32 00b65b9c288df8c0ae7cdacba531a7dc5cb006e6c768e19ee36055717b782acc drwx------ 4 root root 4096 11月 11 13:48 5da215c4f218cbb1d9825fa111c21bf381dc35a9e6c7c6cd5c3ea952316031e4 drwx------ 3 root root 4096 11月 11 13:32 8cbfb8b74c887e780747c8e6f4b3b9223a513ff6d69770bac16abb76da4e314f drwx------ 4 root root 4096 11月 11 13:32 f1cf8b173467c98e08f3d276d7ccd8f9892c7c71dec2c4b335c39c6f175ae744 drwx------ 2 root root 4096 12月 12 13:29 l 这下和之前讨论对上了， 和 cache-id 中的内容一一对应 [root@iZuf685opgs9oyozju9i2bZ docker]# ll image/overlay2/layerdb/sha256/*/cache-id -rw-r--r-- 1 root root 64 11月 11 13:32 image/overlay2/layerdb/sha256/228fb92e31891f472e9857ee11d13c404ff7c88e808b05ce4ebdc80d785d71f3/cache-id -rw-r--r-- 1 root root 64 11月 11 13:31 image/overlay2/layerdb/sha256/50644c29ef5a27c9a40c393a73ece2479de78325cae7d762ef3cdc19bf42dd0a/cache-id -rw-r--r-- 1 root root 64 11月 11 13:32 image/overlay2/layerdb/sha256/5f72760956a669e4c9b33aa3f2f04baa84b0f4cf1e11676049981bafcbba74da/cache-id -rw-r--r-- 1 root root 64 11月 11 13:32 image/overlay2/layerdb/sha256/fe267088885017d5e9a4621e68617a7f35e58dc2d0d747927882da21059854e3/cache-id 在看看具体镜像 layer 层的内容: diff: 这个层所做的改动，如通过 ADD、RUN 等指令对做文件系统做出的改变都在这里了。 link: 自己的link值，在刚刚的 overlay2/l 目录下可以看的到。 [root@iZuf685opgs9oyozju9i2bZ docker]# cat overlay2/00b65b9c288df8c0ae7cdacba531a7dc5cb006e6c768e19ee36055717b782acc/link HG76ICE67NTXFL7AYXCMI3EK4Y lower: 该层所依赖的层的所有link，base layer 不依赖任何层，所以也就不会有 lower 这个文件，最后一层依赖之前的所有层，如: [root@iZuf685opgs9oyozju9i2bZ docker]# cat overlay2/5da215c4f218cbb1d9825fa111c21bf381dc35a9e6c7c6cd5c3ea952316031e4/lower l/HG76ICE67NTXFL7AYXCMI3EK4Y:l/33RV26M4VMX3ZUISOG26USXBKR:l/MUHXHRXFSGNCBKQ2AUXDFOUDLF 这个镜像的最后一层依赖前面的层 merged:容器的最终视图，merge 了 镜像层加读写层 启动容器新建的两层在 containers 中详细说。 ","date":"2020-12-09","objectID":"/docker-local-storage/:3:5","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"containers 我们一直说 docker 镜像是分层的， 容器 = 镜像 + 读写层 ，如果还没有什么感觉的话不妨再来看一个图: 可以看到容器是依赖于镜像，启动容器时会先把镜像的各个 layer 联合挂载成一个统一的视图(只读层)，就是我们在 overlay2 目录中看到的 b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc-init 目录， 去掉 -init 就是对应的读写层。 看看 containers 目录下都有什么: [root@iZuf685opgs9oyozju9i2bZ docker]# tree -L 2 containers/ containers/ └── 4ec800c3ec10654a6ea2b2317ac198514748464a217ef63bb58ef67874a79ae0 ├── 4ec800c3ec10654a6ea2b2317ac198514748464a217ef63bb58ef67874a79ae0-json.log ├── checkpoints ├── config.v2.json ├── hostconfig.json ├── hostname ├── hosts ├── mounts ├── resolv.conf └── resolv.conf.hash 3 directories, 7 files [root@iZuf685opgs9oyozju9i2bZ docker]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4ec800c3ec10 russellgao/openresty:1.17.8.2-5-alpine \"/usr/local/openrest…\" About an hour ago Up About an hour 0.0.0.0:80-\u003e80/tcp, 0.0.0.0:443-\u003e443/tcp openresty-app-1 可以看到这个下面是以容器为单位进行存放的，保存了每个容器的详细配置，在容器里看到的hostname,/etc/hosts,dns 等各种配置在这里都可以找得到，这里就不展开看每个具体文件了，有兴趣者可以自行查看。 值得一提的是，通过 docker inspect containername 得到的内容在 containers/xxx/config.v2.json 都可以找的到哦，可以查看它们的输出，会发现出奇的相似哦。 请注意好玩的事情来了 我们说到 容器=镜像+读写层 ，那是不是以为着我们在读写层做修改，容器中可以看到，反之在容器中做的修改，在读写层也应该能看到才对。 读写层时各临时的 layer 层(临时目录) ，当容器被删除时，这个 layer 也会随之被删除。 做了实验看看: 最初读写层的内容和容器的根目录如下: [root@iZuf685opgs9oyozju9i2bZ docker]# ll overlay2/b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc/diff/ 总用量 16 drwxr-xr-x 3 root root 4096 9月 19 00:25 run drwxr-xr-x 3 root root 4096 9月 19 00:25 usr [root@iZuf685opgs9oyozju9i2bZ docker]# [root@iZuf685opgs9oyozju9i2bZ docker]# docker exec openresty-app-1 ls -l / total 64 drwxr-xr-x 1 root root 4096 Sep 18 16:25 bin drwxr-xr-x 5 root root 340 Dec 12 05:29 dev drwxr-xr-x 1 root root 4096 Dec 12 05:29 etc drwxr-xr-x 2 root root 4096 May 29 2020 home drwxr-xr-x 1 root root 4096 Sep 18 16:25 lib drwxr-xr-x 5 root root 4096 May 29 2020 media drwxr-xr-x 2 root root 4096 May 29 2020 mnt drwxr-xr-x 2 root root 4096 May 29 2020 opt dr-xr-xr-x 114 root root 0 Dec 12 05:29 proc drwx------ 2 root root 4096 May 29 2020 root drwxr-xr-x 1 root root 4096 Sep 18 16:25 run drwxr-xr-x 2 root root 4096 May 29 2020 sbin drwxr-xr-x 2 root root 4096 May 29 2020 srv dr-xr-xr-x 13 root root 0 Nov 11 00:49 sys drwxrwxrwt 1 root root 4096 Sep 18 16:25 tmp drwxr-xr-x 1 root root 4096 Sep 18 16:25 usr drwxr-xr-x 1 root root 4096 May 29 2020 var 进入到容器并在根目录生成一个文件: [root@iZuf685opgs9oyozju9i2bZ docker]# docker exec -it openresty-app-1 sh / # echo \"测试容器的读写层-20201209\" \u003e /test-layer.20201209.txt 读写层看看什么情况: [root@iZuf685opgs9oyozju9i2bZ docker]# ll overlay2/b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc/diff/ 总用量 24 drwx------ 2 root root 4096 12月 12 15:22 root drwxr-xr-x 3 root root 4096 9月 19 00:25 run -rw-r--r-- 1 root root 34 12月 12 15:22 test-layer.20201209.txt drwxr-xr-x 3 root root 4096 9月 19 00:25 usr [root@iZuf685opgs9oyozju9i2bZ docker]# cat overlay2/b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc/diff/test-layer.20201209.txt 测试容器的读写层-20201209 可以看到在容器中新建的文件确实在读写层中可以看到，那么反过来再试试 在读写层新建一个文件: [root@iZuf685opgs9oyozju9i2bZ docker]# echo \"测试容器的读写层-20201209-abcdefg\" \u003e overlay2/b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc/diff/test-layer.20201209-abcdefg.txt [root@iZuf685opgs9oyozju9i2bZ docker]# ll overlay2/b04b728c94b5a269e9d102329c930e3781212717e830e1941e1008088d823cdc/diff/ 总用量 28 drwx------ 2 root root 4096 12月 12 15:22 root drwxr-xr-x 3 root root 4096 9月 19 00:25 run -rw-r--r-- 1 root root 42 12月 12 15:26 test-layer.20201209-abcdefg.txt -rw-r--r-- 1 root root 34 12月 12 15:22 test-layer.20201209.txt drwxr-xr-x 3 root root 4096 9月 19 00:25 usr 进到容器中看看: [root@iZuf685opgs9oyozju9i2bZ docker]# docker exec -it openresty-app-1 sh / # ls -l total 72 drwxr-xr-x 1 root root 4096 Sep 18 16:25 bin drwxr-xr-x 5 root root 340 Dec 12 05:29 dev drwxr-xr-x 1 root root 4096 Dec 12 05:29 etc drwxr-xr-x 2 root root 4096 May 29 2020 home drwxr-xr-x 1 root root 4096 Sep 18 16:25 lib drwxr-xr-x 5 root root 4096 May 2","date":"2020-12-09","objectID":"/docker-local-storage/:3:6","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"总结 这篇文章很长，难免表达逻辑上出现混乱，感谢能耐心看完的小伙伴，如果有不对之处欢迎批评指正。 image/overlay2 distribution: 和镜像分发相关，记录了diffid 与 digest 之间的关系。 imagedb: 记录了镜像的元信息，其中content中的内容和docker inspect image 结果基本一直。 layerdb: 记录了 layer 的元信息，如真正的 layerid， size 等信息。 repositories.json: 记录这个主机上所有的镜像。 overlay2: 镜像的具体layer 层的内容，包括镜像的只读层和容器的读写层。其中读写层是临时层，当容器删除时也会随之删除，在这一层的 diff 目录下做修改，容器内也会随之看到。 containers: 容器的配置信息，通过 docker inspect containerid 得到的结果和 containers/xxx 下的内容基本一直。 ","date":"2020-12-09","objectID":"/docker-local-storage/:4:0","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"参考 https://segmentfault.com/a/1190000017579626 ","date":"2020-12-09","objectID":"/docker-local-storage/:5:0","tags":["docker","存储"],"title":"docker 原理之本地存储","uri":"/docker-local-storage/"},{"categories":null,"content":"Python 中的迭代器与生成器","date":"2020-12-07","objectID":"/python-iter/","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"导读 这篇文章主要介绍了 python 当中的迭代器与生成器，在涉及到大数据量的场景应该考虑使用迭代器与生成器。 ","date":"2020-12-07","objectID":"/python-iter/:1:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"可迭代对象 如果一个对象实现了 __iter__ 方法，那么我们就称它是一个可迭代对象。如果没有实现 __iter__ 而实现了 __getitem__ 方法，并且其参数是从0开始索引的，这种对象也是可迭代的，比如说序列。 使用 iter 内置函数可以获取迭代器的对象，当解释器需要迭代对象时，会自动调用 item(x) ： 如果对象实现了 __iter__ 方法，获取一个迭代器 如果没有实现 __iter__ ，但是实现了 __getitem__ ，python 会创建一个迭代器，尝试从索引0开始获取元素 如果获取失败，则抛出 TypeError 标准序列都实现了 __iter__ 方法，所以标准序列都是可迭代对象。如 list,dict,set,tuple。 只有实现了 __iter__ 方法的对象能通过子类测试issubclass(Object,abc.Itertor) 检查对象能否迭代最标准的方法是调用 iter() 函数，因为 iter() 会考虑到实现 __getitem__ 方法的部分可迭代对象。 ","date":"2020-12-07","objectID":"/python-iter/:2:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"迭代器 迭代器主要用于从集合中取出元素，那么是什么迭代器呢? 实现了 next 方法的可迭代对象就是迭代器。 next 返回下一个可用的元素，当没有元素时抛出 StopIteration 异常。 __iter__ ，迭代器本身。 到这里应该可以看出「可迭代对象」与 「迭代器」的区别了，就是在于有没有实现next 方法 。 检查对象是不是一个迭代器 ：isinstance(object,abc.Iterator) 。 ","date":"2020-12-07","objectID":"/python-iter/:3:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"迭代器模式 按需一次获取一个数据项。 迭代器模式的用途有： 访问一个聚合对象而无需暴露它的内部结构 支持对聚合对象的多种遍历 为遍历不同的聚合结构提供一个统一的接口 说了这么多，那么除了标准序列之外，如何自定义一个迭代器呢，看看下面的代码： class Books: def __init__(self, books): self.books = books.split(\",\") def __iter__(self): return BookIterator(self.books) class BookIterator: def __init__(self, books): self.books = books self.index = 0 def next(self): try : book = self.books[self.index] except IndexError: raise StopIteration() self.index += 1 return book def __iter__(self): return self books = Books(\"python,golang,vue,kubernetes,istio\") print(books) for book in books : print(book) ","date":"2020-12-07","objectID":"/python-iter/:3:1","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"生成器 生成器是一种特殊的迭代器，这种迭代器更加优雅，不需要像上面一样写 __iter__ 和 next 方法了，只需要一个 yield 关键字即可。 生成器有两种方法用: yield () 生成 ","date":"2020-12-07","objectID":"/python-iter/:4:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"yield def gen(x) : for i in range(x) : yield i * 2 g1 = gen(10) ","date":"2020-12-07","objectID":"/python-iter/:4:1","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"() 生成器 l1 = [ i * 2 for i in range(10) ] g1 = (i * 2 for i in range(10)) for i in g1 : print(i) print(l1) l1 是一个列表推导式 g1 是一个生成器 两者的区别是 g1 生成器采用懒加载的方式，不会一次把数据加载到内存中 生成器的好处不用把数据全部加载到内存中，访问到的时候在计算出来。例如有 100w 的数据集，但是只需要访问前 100 个，是不是生成器就很有用了。 ","date":"2020-12-07","objectID":"/python-iter/:4:2","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"特别说明 上述写法为 python2 的写法，python3 略有不同，python3 中 next 方法对应的为 __next__ ，如上面的 Books 例子对应的写法为: class Books: def __init__(self, books): self.books = books.split(\",\") def __iter__(self): return BookIterator(self.books) class BookIterator: def __init__(self, books): self.books = books self.index = 0 def __next__(self): try : book = self.books[self.index] except IndexError: raise StopIteration() self.index += 1 return book def __iter__(self): return self books = Books(\"python,golang,vue,kubernetes,istio\") print(books) for book in books : print(book) ","date":"2020-12-07","objectID":"/python-iter/:5:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"总结 任何对象只要实现了 __iter__ 方法就是一个可迭代对象 任何对象只要实现了 __iter__ 和 next 方法就是一个迭代器 生成器是一个特殊的迭代器，可以通过 yield 和 () 的方式生成 在数据量大的时候使用会有奇效 ","date":"2020-12-07","objectID":"/python-iter/:6:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python-iter/"},{"categories":null,"content":"Python 中的迭代器与生成器","date":"2020-12-07","objectID":"/python/python-iter/","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"导读 这篇文章主要介绍了 python 当中的迭代器与生成器，在涉及到大数量的场景应该考虑使用迭代器与生成器。 ","date":"2020-12-07","objectID":"/python/python-iter/:1:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"可迭代对象 如果一个对象实现了 __iter__ 方法，那么我们就称它是一个可迭代对象。如果没有实现 __iter__ 而实现了 __getitem__ 方法，并且其参数是从0开始索引的，这种对象也是可迭代的，比如说序列。 使用 iter 内置函数可以获取迭代器的对象，当解释器需要迭代对象时，会自动调用 item(x) ： 如果对象实现了 __iter__ 方法，获取一个迭代器 如果没有实现 __iter__ ，但是实现了 __getitem__ ，python 会创建一个迭代器，尝试从索引0开始获取元素 如果获取失败，则抛出 TypeError 标准序列都实现了 __iter__ 方法，所以标准序列都是可迭代对象。如 list,dict,set,tuple。 只有实现了 __iter__ 方法的对象能通过子类测试issubclass(Object,abc.Itertor) 检查对象能否迭代最标准的方法是调用 iter() 函数，因为 iter() 会考虑到实现 __getitem__ 方法的部分可迭代对象。 ","date":"2020-12-07","objectID":"/python/python-iter/:2:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"迭代器 迭代器主要用于从集合中取出元素，那么是什么迭代器呢? 实现了 next 方法的可迭代对象就是迭代器。 next 返回下一个可用的元素，当没有元素时抛出 StopIteration 异常。 __iter__ ，迭代器本身。 到这里应该可以看出「可迭代对象」与 「迭代器」的区别了，就是在于有没有实现next 方法 。 检查对象是不是一个迭代器 ：isinstance(object,abc.Iterator) 。 ","date":"2020-12-07","objectID":"/python/python-iter/:3:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"迭代器模式 按需一次获取一个数据项。 迭代器模式的用途有： 访问一个聚合对象而无需暴露它的内部结构 支持对聚合对象的多种遍历 为遍历不同的聚合结构提供一个统一的接口 说了这么多，那么除了标准序列之外，如何自定义一个迭代器呢，看看下面的代码： class Books: def __init__(self, books): self.books = books.split(\",\") def __iter__(self): return BookIterator(self.books) class BookIterator: def __init__(self, books): self.books = books self.index = 0 def next(self): try : book = self.books[self.index] except IndexError: raise StopIteration() self.index += 1 return book def __iter__(self): return self books = Books(\"python,golang,vue,kubernetes,istio\") print(books) for book in books : print(book) ","date":"2020-12-07","objectID":"/python/python-iter/:3:1","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"生成器 生成器是一种特殊的迭代器，这种迭代器更加优雅，不需要像上面一样写 __iter__ 和 next 方法了，只需要一个 yield 关键字即可。 生成器有两种方法用: yield () 生成 ","date":"2020-12-07","objectID":"/python/python-iter/:4:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"yield def gen(x) : for i in range(x) : yield i * 2 g1 = gen(10) ","date":"2020-12-07","objectID":"/python/python-iter/:4:1","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"() 生成器 l1 = [ i * 2 for i in range(10) ] g1 = (i * 2 for i in range(10)) for i in g1 : print(i) print(l1) l1 是一个列表推导式 g1 是一个生成器 两者的区别是 g1 生成器采用懒加载的方式，不会一次把数据加载到内存中 生成器的好处不用把数据全部加载到内存中，访问到的时候在计算出来。例如有 100w 的数据集，但是只需要访问前 100 个，是不是生成器就很有用了。 ","date":"2020-12-07","objectID":"/python/python-iter/:4:2","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"特别说明 上述写法为 python2 的写法，python3 略有不同，python3 中 next 方法对应的为 __next__ ，如上面的 Books 例子对应的写法为: class Books: def __init__(self, books): self.books = books.split(\",\") def __iter__(self): return BookIterator(self.books) class BookIterator: def __init__(self, books): self.books = books self.index = 0 def __next__(self): try : book = self.books[self.index] except IndexError: raise StopIteration() self.index += 1 return book def __iter__(self): return self books = Books(\"python,golang,vue,kubernetes,istio\") print(books) for book in books : print(book) ","date":"2020-12-07","objectID":"/python/python-iter/:5:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"总结 任何对象只要实现了 __iter__ 方法就是一个可迭代对象 任何对象只要实现了 __iter__ 和 next 方法就是一个迭代器 生成器是一个特殊的迭代器，可以通过 yield 和 () 的方式生成 在数据量大的时候使用会有奇效 ","date":"2020-12-07","objectID":"/python/python-iter/:6:0","tags":["python","迭代器","生成器"],"title":"Python 中的迭代器与生成器","uri":"/python/python-iter/"},{"categories":null,"content":"docker 原理之存储驱动","date":"2020-12-05","objectID":"/docker-storage/","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"导读 提起 docker 大家应该耳熟能详，如使用 docker 所带来的持续集成、版本控制、可移植性、隔离性、安全性等诸多好处。docker 的使用也很方便，但是其内部原理是什么样的？都有哪些组件？之间是如何相互协作的呢？这是 docker 系列文章，每篇讲解一个知识点，可以更好的消化。 这篇谈谈 docker 的存储驱动。受限作者水平，如有不对之处，欢迎批评之处。 ","date":"2020-12-05","objectID":"/docker-storage/:1:0","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"什么是 docker 存储驱动 如果执行过 docker info 命令，那么肯定看到过这些信息: ... Server: Server Version: 19.03.13 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true ... 请注意 Storage Driver: overlay2 ，看到这些可能会有几个疑问: 什么是 Storage Driver ？除了 overlay2 还有其他的吗？原理是什么？ 我们知道 docker 的特别是分层的，层叠镜像是 docker 最具特色的特性之一。想象这么一个场景，docker 启动容器是依赖于镜像的，假设要一个 JDK 的镜像 要启动 10 个，这个镜像本身500M，那么 10 个这些容器是共享这一个镜像呢还是把每个镜像都复制一份呢，如果是共享模式，那么如果一个容器修改了镜像中的内容 岂不是会影响其他容器？如果是各自复制一份，那岂不是会造成存储空间的浪费? 存储驱动(Storage Driver) 就是解决这个问题，到现在也有好几种解决方案。总的解决思路就是镜像是只读的，启动容器时就是镜像上面叠加一个读写层。 在了解具体的存储驱动之前先铺垫几个知识点： ","date":"2020-12-05","objectID":"/docker-storage/:2:0","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"写时复制（CoW） 所有驱动都用到的技术——写时复制（CoW）。CoW就是copy-on-write，表示只在需要写时才去复制，这个是针对已有文件的修改场景。比如基于一个image启动多个Container，如果为每个Container都去分配一个image一样的文件系统，那么将会占用大量的磁盘空间。而CoW技术可以让所有的容器共享image的文件系统，所有数据都从image中读取，只有当要对文件进行写操作时，才从image里把要写的文件复制到自己的文件系统进行修改。所以无论有多少个容器共享同一个image，所做的写操作都是对从image中复制到自己的文件系统中的复本上进行，并不会修改image的源文件，且多个容器操作同一个文件，会在每个容器的文件系统里生成一个复本，每个容器修改的都是自己的复本，相互隔离，相互不影响。使用CoW可以有效的提高磁盘的利用率。 ","date":"2020-12-05","objectID":"/docker-storage/:2:1","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"用时分配（allocate-on-demand） 而写时分配是用在原本没有这个文件的场景，只有在要新写入一个文件时才分配空间，这样可以提高存储资源的利用率。比如启动一个容器，并不会为这个容器预分配一些磁盘空间，而是当有新文件写入时，才按需分配新空间。 ","date":"2020-12-05","objectID":"/docker-storage/:2:2","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"联合文件系统 联合文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。 ","date":"2020-12-05","objectID":"/docker-storage/:2:3","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"现有的存储驱动及其特点 ","date":"2020-12-05","objectID":"/docker-storage/:3:0","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"AUFS AUFS（AnotherUnionFS）是一种Union FS，是文件级的存储驱动。AUFS能透明覆盖一或多个现有文件系统的层状文件系统，把多层合并成文件系统的单层表示。简单来说就是支持将不同目录挂载到同一个虚拟文件系统下的文件系统。这种文件系统可以一层一层地叠加修改文件。无论底下有多少层都是只读的，只有最上层的文件系统是可写的。当需要修改一个文件时，AUFS创建该文件的一个副本，使用CoW将文件从只读层复制到可写层进行修改，结果也保存在可写层。在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示： ","date":"2020-12-05","objectID":"/docker-storage/:3:1","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"Overlay Overlay是Linux内核3.18后支持的，也是一种Union FS，和AUFS的多层不同的是Overlay只有两层：一个upper文件系统和一个lower文件系统，分别代表Docker的镜像层和容器层。当需要修改一个文件时，使用CoW将文件从只读的lower复制到可写的upper进行修改，结果也保存在upper层。在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示： OverlayFS有两种存储驱动，它们使用了相同的OverlayFS技术，但却有着不同的实现，在磁盘使用上也并不互相兼容。因为不兼容，两者之间的切换必须重新创建所有的镜像。overlay驱动是最原始的OverlayFS实现，并且，在Docker1.11之前是仅有的OverlayFS驱动选择。overlay驱动在inode消耗方面有着较明显的限制，并且会损耗一定的性能。overlay2驱动解决了这种限制，不过只能在Linux kernel 4.0以上使用它。 目前 Overlay2 是默认的存储驱动 ","date":"2020-12-05","objectID":"/docker-storage/:3:2","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"Device mapper Device mapper是Linux内核2.6.9后支持的，提供的一种从逻辑设备到物理设备的映射框架机制，在该机制下，用户可以很方便的根据自己的需要制定实现存储资源的管理策略。前面讲的AUFS和OverlayFS都是文件级存储，而Device mapper是块级存储，所有的操作都是直接对块进行操作，而不是文件。Device mapper驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并不有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。当要修改已有文件时，再使用CoW为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Device mapper 驱动默认会创建一个100G的文件包含镜像和容器。每一个容器被限制在10G大小的卷内，可以自己配置调整。结构如下图所示： ","date":"2020-12-05","objectID":"/docker-storage/:3:3","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"Btrfs Btrfs被称为下一代写时复制文件系统，并入Linux内核，也是文件级级存储，但可以像Device mapper一直接操作底层设备。Btrfs把文件系统的一部分配置为一个完整的子文件系统，称之为subvolume 。那么采用 subvolume，一个大的文件系统可以被划分为多个子文件系统，这些子文件系统共享底层的设备空间，在需要磁盘空间时便从底层设备中分配，类似应用程序调用 malloc()分配内存一样。为了灵活利用设备空间，Btrfs 将磁盘空间划分为多个chunk 。每个chunk可以使用不同的磁盘空间分配策略。比如某些chunk只存放metadata，某些chunk只存放数据。这种模型有很多优点，比如Btrfs支持动态添加设备。用户在系统中增加新的磁盘之后，可以使用Btrfs的命令将该设备添加到文件系统中。Btrfs把一个大的文件系统当成一个资源池，配置成多个完整的子文件系统，还可以往资源池里加新的子文件系统，而基础镜像则是子文件系统的快照，每个子镜像和容器都有自己的快照，这些快照则都是subvolume的快照。 当写入一个新文件时，为在容器的快照里为其分配一个新的数据块，文件写在这个空间里，这个叫用时分配。而当要修改已有文件时，使用CoW复制分配一个新的原始数据和快照，在这个新分配的空间变更数据，变结束再更新相关的数据结构指向新子文件系统和快照，原来的原始数据和快照没有指针指向，被覆盖。 ","date":"2020-12-05","objectID":"/docker-storage/:3:4","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"ZFS ZFS 文件系统是一个革命性的全新的文件系统，它从根本上改变了文件系统的管理方式，ZFS 完全抛弃了“卷管理”，不再创建虚拟的卷，而是把所有设备集中到一个存储池中来进行管理，用“存储池”的概念来管理物理存储空间。过去，文件系统都是构建在物理设备之上的。为了管理这些物理设备，并为数据提供冗余，“卷管理”的概念提供了一个单设备的映像。而ZFS创建在虚拟的，被称为“zpools”的存储池之上。每个存储池由若干虚拟设备（virtual devices，vdevs）组成。这些虚拟设备可以是原始磁盘，也可能是一个RAID1镜像设备，或是非标准RAID等级的多磁盘组。于是zpool上的文件系统可以使用这些虚拟设备的总存储容量。 下面看一下在Docker里ZFS的使用。首先从zpool里分配一个ZFS文件系统给镜像的基础层，而其他镜像层则是这个ZFS文件系统快照的克隆，快照是只读的，而克隆是可写的，当容器启动时则在镜像的最顶层生成一个可写层。如下图所示： 当要写一个新文件时，使用按需分配，一个新的数据快从zpool里生成，新的数据写入这个块，而这个新空间存于容器（ZFS的克隆）里。 当要修改一个已存在的文件时，使用写时复制，分配一个新空间并把原始数据复制到新空间完成修改。 ","date":"2020-12-05","objectID":"/docker-storage/:3:5","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"存储驱动对比 存储驱动 特点 优点 缺点 适用场景 AUFS 联合文件系统、未并入内核主线、文件级存储 作为docker的第一个存储驱动，已经有很长的历史，比较稳定，且在大量的生产中实践过，有较强的社区支持 有多层，在做写时复制操作时，如果文件比较大且存在比较低的层，可能会慢一些 大并发但少IO的场景 overlayFS 联合文件系统、并入内核主线、文件级存储 只有两层 不管修改的内容大小都会复制整个文件，对大文件进行修改显示要比小文件消耗更多的时间 大并发但少IO的场景 Devicemapper 并入内核主线、块级存储 块级无论是大文件还是小文件都只复制需要修改的块，并不是整个文件 不支持共享存储，当有多个容器读同一个文件时，需要生成多个复本，在很多容器启停的情况下可能会导致磁盘溢出 适合io密集的场景 Btrfs 并入linux内核、文件级存储 可以像devicemapper一样直接操作底层设备，支持动态添加设备 不支持共享存储，当有多个容器读同一个文件时，需要生成多个复本 不适合在高密度容器的paas平台上使用 ZFS 把所有设备集中到一个存储池中来进行管理 支持多个容器共享一个缓存块，适合内存大的环境 COW使用碎片化问题更加严重，文件在硬盘上的物理地址会变的不再连续，顺序读会变的性能比较差 适合paas和高密度的场景 ","date":"2020-12-05","objectID":"/docker-storage/:4:0","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"设置存储驱动 docker 安装时会有自己的默认存储驱动，在新版本的 docker 中，默认是 overlay2，centos，mac 是这样的，其他的没有验证过。 如果要更改存储驱动，方法为: dockerd --storage-driver=aufs 设置完成后可通过 docker info 进行验证。 ","date":"2020-12-05","objectID":"/docker-storage/:5:0","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"参考 http://dockone.io/article/1513 https://gitbook.cn/gitchat/column/5d68b823de93ed72d6eca1bc/topic/5db26784bae3b42c1fa84d5f ","date":"2020-12-05","objectID":"/docker-storage/:6:0","tags":["docker"],"title":"docker 原理之存储驱动","uri":"/docker-storage/"},{"categories":null,"content":"OSCHINA 搬迁申明","date":"2020-11-30","objectID":"/oschina-blog/","tags":[],"title":"OSCHINA 搬迁申明","uri":"/oschina-blog/"},{"categories":null,"content":"导读 我的博客即将同步至 OSCHINA 社区，这是我的 OSCHINA ID：russellgao，邀请大家一同入驻：https://www.oschina.net/sharing-plan/apply ","date":"2020-11-30","objectID":"/oschina-blog/:1:0","tags":[],"title":"OSCHINA 搬迁申明","uri":"/oschina-blog/"},{"categories":null,"content":"harbor gc 时遇到的坑","date":"2020-11-29","objectID":"/harbor-gc/","tags":["云原生","harbor"],"title":"harbor gc 时遇到的坑","uri":"/harbor-gc/"},{"categories":null,"content":"导读 Harbor 是为企业用户设计的容器镜像仓库开源项目，包括了权限管理(RBAC)、LDAP、审计、安全漏洞扫描、镜像验真、管理界面、自我注册、HA 等企业必需的功能，同时针对中国用户的特点，设计镜像复制和中文支持等功能。 在使用的过程会有 GC 的需求，可以想象下这几种场景: 在 CI 的过程，同一个版本（SNAPSHOT/latest）编译很多次，只有最后一次产生的才有 tag ，那么之前的产生 blob 去哪了，或者还有用吗 ？ 镜像的生命周期已经结束，需要从仓库中删除，应该怎么操作？要知道在 Harbor 界面上删除只是标记删除，并不会释放存储空间。 Harbor / Docker 官方已经提供比较完善的 GC 方案，可以解决 80% 的问题，但是 GC 的过程中还可能出现一些奇怪的现象，本文主要记录在 Harbor GC 过程中踩过的坑。 ","date":"2020-11-29","objectID":"/harbor-gc/:1:0","tags":["云原生","harbor"],"title":"harbor gc 时遇到的坑","uri":"/harbor-gc/"},{"categories":null,"content":"GC原理 用一个官方例子说明: A -----\u003e a \u003c----- B \\--\u003e b | c \u003c--/ 假设镜像 A 引用了层a,b ，镜像 B 引用了层 a,c ，在这个阶段，是不需要做 GC 的，接下来把 B 给删掉，如下: A -----\u003e a B \\--\u003e b c 在这个阶段层 c 是不属于任何镜像了，适合去 GC ，GC 完之后效果如下： A -----\u003e a \\--\u003e b 看着还是挺简单，很容易理解的对吧，但是当镜像数为 10,000+ 以上，存储在 TB 级别以上时，事情可能又不那么简单了。 ","date":"2020-11-29","objectID":"/harbor-gc/:2:0","tags":["云原生","harbor"],"title":"harbor gc 时遇到的坑","uri":"/harbor-gc/"},{"categories":null,"content":"Harbor 存储的目录结构 Harbor 底层还是 Docker Registry，所以它们的存储结构是一样的，可以先看看它们在磁盘上存储结构: # tree docker/registry/v2/ docker/registry/v2 │ │ ├── blogs │ │ │ └── sha256 │ │ │ └── 00 │ │ │ └── 000098c48e5c8502460fd4427fe19d9def6c3d245b46e4d3dd86a00c79ca3111 │ │ │ └── data │ │ │ └── 000098c48e5c8502460fd4427fe19d9def6c3d245b46e4d3dd86a00c79ca3112 │ │ │ └── data │ │ │ └── 01 │ │ │ └── 010098c48e5c8502460fd4427fe19d9def6c3d245b46e4d3dd86a00c79ca3111 │ │ │ └── data │ │ │ └── 010098c48e5c8502460fd4427fe19d9def6c3d245b46e4d3dd86a00c79ca3112 │ │ │ └── data │ │ ├── repositories │ │ │ └── golang │ │ │ └── golang-centos │ │ │ └── _layers │ │ │ └── sha256 │ │ │ └── _manifests │ │ │ └── revisions │ │ │ └── sha256 │ │ │ └── tags │ │ │ └── 1.14 │ │ │ └── 1.15 可以看到存储结构主要分为两个部分 blogs 和 repositories ，作用如下 : blogs 是镜像数据的真正存储。 repositories 是镜像数据的引用，换言之存储的是blogs的索引。每个镜像都会声明它引用 blogs 中的哪些层。 ","date":"2020-11-29","objectID":"/harbor-gc/:3:0","tags":["云原生","harbor"],"title":"harbor gc 时遇到的坑","uri":"/harbor-gc/"},{"categories":null,"content":"GC过程 有了上面的铺垫，GC 的过程应该很容易理解了。Harbor GC 采用的两阶段标记清除，先遍历 repositories 下的镜像，并且对引用到blogs 进行标记，遍历完成之后把没有标记的 blogs 进行删除。 看似完美的方案，在实际操作过程中却还有些坑，下面说说遇到的坑以及如何解决方案。 ","date":"2020-11-29","objectID":"/harbor-gc/:4:0","tags":["云原生","harbor"],"title":"harbor gc 时遇到的坑","uri":"/harbor-gc/"},{"categories":null,"content":"遇到的坑 ","date":"2020-11-29","objectID":"/harbor-gc/:5:0","tags":["云原生","harbor"],"title":"harbor gc 时遇到的坑","uri":"/harbor-gc/"},{"categories":null,"content":"docker pull 失败 docker pull 的时候报错如下（unknown blob）： docker pull russellgao/toolkit ... daa258f4f8c0: Already exists 0c9e9bbad61e: Already exists fa786f5d7be0: Already exists ebc05f08dcb7: Downloading f919a7128c9a: Downloading 34dfbfa16f77: Download complete 65588873bd66: Download complete fc1b74edeacc: Download complete 099607f21531: Download complete 09432885197f: Download complete 259a4564bedf: Download complete ce223372b98e: Download complete ... unknown blob 这种情况主要的原因是在 repositories 中存在对 blob 的引用，但是 blog 中却不存在，造成这种可能的原因有： GC 的时候错误的删除了 blobs （大概率如此） blob 所在的磁盘损坏 （概率较小） blob 被人为删除（概率较小） 请注意：这种情况重新推送镜像是没有用的，因为在推送的时候，harbor 认为缺失的层是存在的，因为 repositories中存在，只有在下载时才会发现。 解决的方法: 通过docker build 编译镜像时增加 --no-cache 参数，生成一个全新的镜像推送到镜像仓库，方法可能会解决问题，但也有可能解决不了，可以 想象这么一个场景，缺失的层为基础镜像，如果基础镜像缺少层，那么这种方法就失效了。 可以在部署一个镜像仓库（一般都会最少有两个仓库做互备），把编译好的镜像推送到新的仓库，然后根据缺少的 blob id 在新的仓库中找到对应的 blob 数据，然后把缺少的 copy 到之前仓库，问题即可得到解决。如： 缺少 ebc05f08dcb7 这一层，在新的仓库的中可以找到如下目录: docker/registry/v2/blobs/sha256/eb/ 通过 ebc05f08dcb7 前缀找到具体的 blob 目录，然后把找到的这个目录 copy 到对应的仓库日录，问题即可得到解决。 ","date":"2020-11-29","objectID":"/harbor-gc/:5:1","tags":["云原生","harbor"],"title":"harbor gc 时遇到的坑","uri":"/harbor-gc/"},{"categories":null,"content":"总结 这篇文章主要介绍了 harbor gc 的基本原理，然后记录在 GC 的过程中踩的坑，后续有其他坑持续补充。 ","date":"2020-11-29","objectID":"/harbor-gc/:6:0","tags":["云原生","harbor"],"title":"harbor gc 时遇到的坑","uri":"/harbor-gc/"},{"categories":null,"content":"参考 https://github.com/docker/docker.github.io/blob/master/registry/garbage-collection.md ","date":"2020-11-29","objectID":"/harbor-gc/:7:0","tags":["云原生","harbor"],"title":"harbor gc 时遇到的坑","uri":"/harbor-gc/"},{"categories":null,"content":"细谈 Golang 中那些设计优美的细节-GMP","date":"2020-11-27","objectID":"/golang-gmp/","tags":["golang","GMP","调度器"],"title":"细谈 Golang 中那些设计优美的细节-GMP","uri":"/golang-gmp/"},{"categories":null,"content":"导读 这是 Golang 系列第二篇，这篇主要谈谈 Golang 的调度模型-GMP，我们知道 Golang 在并发方面有绝对优势，现在就让我们来揭开 它神秘的面纱。 ","date":"2020-11-27","objectID":"/golang-gmp/:1:0","tags":["golang","GMP","调度器"],"title":"细谈 Golang 中那些设计优美的细节-GMP","uri":"/golang-gmp/"},{"categories":null,"content":"背景 ","date":"2020-11-27","objectID":"/golang-gmp/:2:0","tags":["golang","GMP","调度器"],"title":"细谈 Golang 中那些设计优美的细节-GMP","uri":"/golang-gmp/"},{"categories":null,"content":"如何利用 golang 操纵 oracle","date":"2020-11-25","objectID":"/golang/oracle-golang/","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/golang/oracle-golang/"},{"categories":null,"content":"导读 这篇文章主要介绍如何利用 golang 操作 oracle 数据库，包括基本的增删改查，本地 oracle 环境搭建，以及如何在 docker 中运行。 oracle client 镜像构建并不容易，花了很长时间去踩坑，文中提供了已经构建好的基础镜像，可以直接使用，这里贡献给大家。 ","date":"2020-11-25","objectID":"/golang/oracle-golang/:1:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/golang/oracle-golang/"},{"categories":null,"content":"本地环境构建 ","date":"2020-11-25","objectID":"/golang/oracle-golang/:2:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/golang/oracle-golang/"},{"categories":null,"content":"安装客户端 oracle 客户端下载页面: https://www.oracle.com/database/technologies/instant-client/downloads.html mac https://www.oracle.com/database/technologies/instant-client/macos-intel-x86-downloads.html 在上面的页面下载之后执行: # 解压 cd ~ unzip instantclient-basic-macos.x64-19.3.0.0.0dbru.zip # 创建link mkdir ~/lib ln -s ~/instantclient_19_3/libclntsh.dylib ~/lib/ 我本地环境环境是 mac ，这个亲测有用。linux 和 windows 只给出了相关参考连接，没有实际操练过。 linux https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html windows https://www.oracle.com/database/technologies/instant-client/winx64-64-downloads.html ","date":"2020-11-25","objectID":"/golang/oracle-golang/:2:1","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/golang/oracle-golang/"},{"categories":null,"content":"用法 go 操作 oracle 可以使用 github.com/godror/godror ，如 package main import ( \"database/sql\" \"fmt\" _ \"github.com/godror/godror\" \"os\" ) func main() { connString := \"oracle://hd40:xxx@121.196.127.xxx:1521/ylyx?charset=utf8\" db, err := sql.Open(\"godror\", connString) if err != nil { fmt.Println(err) os.Exit(10) } // 注意不要 sql 语句不要使用 ; 号结尾，否则会报错 // dpiStmt_execute: ORA-00911: invalid character sql := \"select * from user_tables \" rows, err1 := db.Query(sql) if err1 != nil { fmt.Println(err1) os.Exit(10) } result, err2 := GetQueryResult(rows) if err2 != nil { fmt.Println(err2) os.Exit(10) } fmt.Println(result) fmt.Println(\"end\") } func GetQueryResult(query *sql.Rows) ([]map[string]string, error) { column, _ := query.Columns() //读出查询出的列字段名 values := make([][]byte, len(column)) //values是每个列的值，这里获取到byte里 scans := make([]interface{}, len(column)) //因为每次查询出来的列是不定长的，用len(column)定住当次查询的长度 for i := range values { //让每一行数据都填充到[][]byte里面 scans[i] = \u0026values[i] } results := []map[string]string{} for query.Next() { //循环，让游标往下移动 if err := query.Scan(scans...); err != nil { //query.Scan查询出来的不定长值放到scans[i] = \u0026values[i],也就是每行都放在values里 fmt.Println(err) return nil, err } row := make(map[string]string) //每行数据 for k, v := range values { //每行数据是放在values里面，现在把它挪到row里 key := column[k] row[key] = string(v) } results = append(results, row) } query.Close() return results, nil } 用法说明 : _ \"github.com/godror/godror\" 这句是说明 sql 的 driver 是 oracle ，如果要操作 mysql ，换成 mysql 的 driver (_ \"github.com/go-sql-driver/mysql\") 即可。 crud 操作还是调用 database/sql 进行完成。 暴露出来给用户的还是 *sql.DB 。查询可以调用 *sql.DB 的 Query 方法，执行其他 sql 则调用 Exec 方法。 ","date":"2020-11-25","objectID":"/golang/oracle-golang/:3:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/golang/oracle-golang/"},{"categories":null,"content":"常见报错 如果报如下错误: (cx_Oracle.DatabaseError) DPI-1047: Cannot locate a 64-bit Oracle Client library: \"dlopen(libclntsh.dylib, 1): image not found\". See https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html for help 说明oracle的 client 没有正确安装 如果报错如下: (cx_Oracle.DatabaseError) ORA-01017: invalid username/password; logon denied 说明oracle 的用户密码不正确 如果报错如下: dpiStmt_execute: ORA-00911: invalid character 说明 sql 中包含无效字符，请注意如果 sql 中包含 ; 就会报错，需要把 sql 中的 ; trim 掉。我写了个标准化 sql 的方法，可以参考: // 去掉行首和行尾的空白字符 func TrimBlankSpace(s string) string { _s := regexp.MustCompile(\"^\\\\s+\").ReplaceAll([]byte(s), []byte(\"\")) _s = regexp.MustCompile(\"\\\\s+$\").ReplaceAll(_s, []byte(\"\")) return string(_s) } // 去掉 ; 并且全部转为大写字符 func FormatSql(sql string) string { sql = TrimBlankSpace(sql) sql = strings.TrimRight(sql, \";\") return strings.ToUpper(sql) } ","date":"2020-11-25","objectID":"/golang/oracle-golang/:4:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/golang/oracle-golang/"},{"categories":null,"content":"docker 中运行 我尝试过用 alpine 做基础镜像，然后进行安装 oracle client，把 oracle client 安装完成之后，运行编译完的 golang 程序， 出现了各种缺少动态库的问题，查了很多资料，和 apline 的官方源都彻底没有解决，然后就准备在 dockerhub 找一个现成的拿过来参考， 很遗憾没有找到一个可以直接用的，最后放弃了 alpine 这条路，选择 alpine 是因为 alpine 镜像比较小，这样做出来的镜像比较小。 看了 oracle client 官方下载和安装说明，也都是 centos 的版本，所以就基于 centos 做了一个包含 oracle client 的基础镜像。 已经上传到 dockerhub 了，参见: https://hub.docker.com/r/russellgao/oracle 如果你刚好需要，而我刚好有，那不妨试用一下（如果能帮到您可以给个 star 哟）。 dockerhub 可能访问比较慢，或者可能会出现无法访问的情况，这里贴一些关键信息。 ","date":"2020-11-25","objectID":"/golang/oracle-golang/:5:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/golang/oracle-golang/"},{"categories":null,"content":"镜像说明 镜像名称 russellgao/oracle:centos7-client12.2 操作系统 centos:centos7.9.2009 oracle client 版本 oracle-instantclient12.2-basic-12.2.0.1.0-1.x86_64 基于基础镜像优化的部分 调整时区为 CST 时区 （UTC +8） 镜像用法 这个一般用作基础镜像 或者: docker run -it --rm russellgao/oracle:centos7-client12.2 date or docker run -d russellgao/oracle:centos7-client12.2 tail -f /dev/null ","date":"2020-11-25","objectID":"/golang/oracle-golang/:5:1","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/golang/oracle-golang/"},{"categories":null,"content":"如何利用 golang 操纵 oracle","date":"2020-11-25","objectID":"/oracle-golang/","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/oracle-golang/"},{"categories":null,"content":"导读 这篇文章主要介绍如何利用 golang 操作 oracle 数据库，包括基本的增删改查，本地 oracle 环境搭建，以及如何在 docker 中运行。 oracle client 镜像构建并不容易，花了很长时间去踩坑，文中提供了已经构建好的基础镜像，可以直接使用，这里贡献给大家。 ","date":"2020-11-25","objectID":"/oracle-golang/:1:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/oracle-golang/"},{"categories":null,"content":"本地环境构建 ","date":"2020-11-25","objectID":"/oracle-golang/:2:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/oracle-golang/"},{"categories":null,"content":"安装客户端 oracle 客户端下载页面: https://www.oracle.com/database/technologies/instant-client/downloads.html mac https://www.oracle.com/database/technologies/instant-client/macos-intel-x86-downloads.html 在上面的页面下载之后执行: # 解压 cd ~ unzip instantclient-basic-macos.x64-19.3.0.0.0dbru.zip # 创建link mkdir ~/lib ln -s ~/instantclient_19_3/libclntsh.dylib ~/lib/ 我本地环境环境是 mac ，这个亲测有用。linux 和 windows 只给出了相关参考连接，没有实际操练过。 linux https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html windows https://www.oracle.com/database/technologies/instant-client/winx64-64-downloads.html ","date":"2020-11-25","objectID":"/oracle-golang/:2:1","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/oracle-golang/"},{"categories":null,"content":"用法 go 操作 oracle 可以使用 github.com/godror/godror ，如 package main import ( \"database/sql\" \"fmt\" _ \"github.com/godror/godror\" \"os\" ) func main() { connString := \"oracle://hd40:xxx@121.196.127.xxx:1521/ylyx?charset=utf8\" db, err := sql.Open(\"godror\", connString) if err != nil { fmt.Println(err) os.Exit(10) } // 注意不要 sql 语句不要使用 ; 号结尾，否则会报错 // dpiStmt_execute: ORA-00911: invalid character sql := \"select * from user_tables \" rows, err1 := db.Query(sql) if err1 != nil { fmt.Println(err1) os.Exit(10) } result, err2 := GetQueryResult(rows) if err2 != nil { fmt.Println(err2) os.Exit(10) } fmt.Println(result) fmt.Println(\"end\") } func GetQueryResult(query *sql.Rows) ([]map[string]string, error) { column, _ := query.Columns() //读出查询出的列字段名 values := make([][]byte, len(column)) //values是每个列的值，这里获取到byte里 scans := make([]interface{}, len(column)) //因为每次查询出来的列是不定长的，用len(column)定住当次查询的长度 for i := range values { //让每一行数据都填充到[][]byte里面 scans[i] = \u0026values[i] } results := []map[string]string{} for query.Next() { //循环，让游标往下移动 if err := query.Scan(scans...); err != nil { //query.Scan查询出来的不定长值放到scans[i] = \u0026values[i],也就是每行都放在values里 fmt.Println(err) return nil, err } row := make(map[string]string) //每行数据 for k, v := range values { //每行数据是放在values里面，现在把它挪到row里 key := column[k] row[key] = string(v) } results = append(results, row) } query.Close() return results, nil } 用法说明 : _ \"github.com/godror/godror\" 这句是说明 sql 的 driver 是 oracle ，如果要操作 mysql ，换成 mysql 的 driver (_ \"github.com/go-sql-driver/mysql\") 即可。 crud 操作还是调用 database/sql 进行完成。 暴露出来给用户的还是 *sql.DB 。查询可以调用 *sql.DB 的 Query 方法，执行其他 sql 则调用 Exec 方法。 ","date":"2020-11-25","objectID":"/oracle-golang/:3:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/oracle-golang/"},{"categories":null,"content":"常见报错 如果报如下错误: (cx_Oracle.DatabaseError) DPI-1047: Cannot locate a 64-bit Oracle Client library: \"dlopen(libclntsh.dylib, 1): image not found\". See https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html for help 说明oracle的 client 没有正确安装 如果报错如下: (cx_Oracle.DatabaseError) ORA-01017: invalid username/password; logon denied 说明oracle 的用户密码不正确 如果报错如下: dpiStmt_execute: ORA-00911: invalid character 说明 sql 中包含无效字符，请注意如果 sql 中包含 ; 就会报错，需要把 sql 中的 ; trim 掉。我写了个标准化 sql 的方法，可以参考: // 去掉行首和行尾的空白字符 func TrimBlankSpace(s string) string { _s := regexp.MustCompile(\"^\\\\s+\").ReplaceAll([]byte(s), []byte(\"\")) _s = regexp.MustCompile(\"\\\\s+$\").ReplaceAll(_s, []byte(\"\")) return string(_s) } // 去掉 ; 并且全部转为大写字符 func FormatSql(sql string) string { sql = TrimBlankSpace(sql) sql = strings.TrimRight(sql, \";\") return strings.ToUpper(sql) } ","date":"2020-11-25","objectID":"/oracle-golang/:4:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/oracle-golang/"},{"categories":null,"content":"docker 中运行 我尝试过用 alpine 做基础镜像，然后进行安装 oracle client，把 oracle client 安装完成之后，运行编译完的 golang 程序， 出现了各种缺少动态库的问题，查了很多资料，和 apline 的官方源都彻底没有解决，然后就准备在 dockerhub 找一个现成的拿过来参考， 很遗憾没有找到一个可以直接用的，最后放弃了 alpine 这条路，选择 alpine 是因为 alpine 镜像比较小，这样做出来的镜像比较小。 看了 oracle client 官方下载和安装说明，也都是 centos 的版本，所以就基于 centos 做了一个包含 oracle client 的基础镜像。 已经上传到 dockerhub 了，参见: https://hub.docker.com/r/russellgao/oracle 如果你刚好需要，而我刚好有，那不妨试用一下（如果能帮到您可以给个 star 哟）。 dockerhub 可能访问比较慢，或者可能会出现无法访问的情况，这里贴一些关键信息。 ","date":"2020-11-25","objectID":"/oracle-golang/:5:0","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/oracle-golang/"},{"categories":null,"content":"镜像说明 镜像名称 russellgao/oracle:centos7-client12.2 操作系统 centos:centos7.9.2009 oracle client 版本 oracle-instantclient12.2-basic-12.2.0.1.0-1.x86_64 基于基础镜像优化的部分 调整时区为 CST 时区 （UTC +8） 镜像用法 这个一般用作基础镜像 或者: docker run -it --rm russellgao/oracle:centos7-client12.2 date or docker run -d russellgao/oracle:centos7-client12.2 tail -f /dev/null ","date":"2020-11-25","objectID":"/oracle-golang/:5:1","tags":["golang","oracle","数据库"],"title":"如何利用 golang 操纵 oracle","uri":"/oracle-golang/"},{"categories":null,"content":"kubernetes 中 pod 是如何启动的呢","date":"2020-11-24","objectID":"/kubernetes/pod-create/","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/kubernetes/pod-create/"},{"categories":null,"content":"导读 Pod 应该算是 kubernetes 的基本盘，Pod 是 kubernetes 的最小调度单位，我这里有个问题，说通过 kubectl apply -f 创建一个 Pod ，从执行到 Pod 正常运行 kubernetes 做了什么事情呢？都有哪些组件参与呢？这篇文档主要讲述从提交 Pod 的创建请求到 Pod 的正常运行的这个过程追踪。 ","date":"2020-11-24","objectID":"/kubernetes/pod-create/:1:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/kubernetes/pod-create/"},{"categories":null,"content":"kubectl 阶段 kubectl apply 阶段其实分为两个过程 ，先调用 get 方法，查看资源是否存在，存在则调用 patch, 否则执行 create 方法。 这个过程可以参考源码（文章中所有源码都截取了部分，详细可以在在给出的链接中查看）: vendor/k8s.io/kubectl/pkg/cmd/apply/apply.go // NewCmdApply creates the `apply` command func NewCmdApply(baseName string, f cmdutil.Factory, ioStreams genericclioptions.IOStreams) *cobra.Command { o := NewApplyOptions(ioStreams) cmd := \u0026cobra.Command{ Use: \"apply (-f FILENAME | -k DIRECTORY)\", DisableFlagsInUseLine: true, Short: i18n.T(\"Apply a configuration to a resource by filename or stdin\"), Long: applyLong, Example: applyExample, Run: func(cmd *cobra.Command, args []string) { cmdutil.CheckErr(o.Complete(f, cmd)) cmdutil.CheckErr(validateArgs(cmd, args)) cmdutil.CheckErr(validatePruneAll(o.Prune, o.All, o.Selector)) cmdutil.CheckErr(o.Run()) }, } ... } // Run executes the `apply` command. func (o *ApplyOptions) Run() error { ... // Iterate through all objects, applying each one. for _, info := range infos { if err := o.applyOneObject(info); err != nil { errs = append(errs, err) } } ... return nil } func (o *ApplyOptions) applyOneObject(info *resource.Info) error { ... // Get the modified configuration of the object. Embed the result // as an annotation in the modified configuration, so that it will appear // in the patch sent to the server. modified, err := util.GetModifiedConfiguration(info.Object, true, unstructured.UnstructuredJSONScheme) if err != nil { return cmdutil.AddSourceToErr(fmt.Sprintf(\"retrieving modified configuration from:\\n%s\\nfor:\", info.String()), info.Source, err) } if err := info.Get(); err != nil { if !errors.IsNotFound(err) { return cmdutil.AddSourceToErr(fmt.Sprintf(\"retrieving current configuration of:\\n%s\\nfrom server for:\", info.String()), info.Source, err) } // Create the resource if it doesn't exist // First, update the annotation used by kubectl apply if err := util.CreateApplyAnnotation(info.Object, unstructured.UnstructuredJSONScheme); err != nil { return cmdutil.AddSourceToErr(\"creating\", info.Source, err) } if o.DryRunStrategy != cmdutil.DryRunClient { // Then create the resource and skip the three-way merge obj, err := helper.Create(info.Namespace, true, info.Object) if err != nil { return cmdutil.AddSourceToErr(\"creating\", info.Source, err) } info.Refresh(obj, true) } if err := o.MarkObjectVisited(info); err != nil { return err } if o.shouldPrintObject() { return nil } printer, err := o.ToPrinter(\"created\") if err != nil { return err } if err = printer.PrintObj(info.Object, o.Out); err != nil { return err } return nil } if err := o.MarkObjectVisited(info); err != nil { return err } if o.DryRunStrategy != cmdutil.DryRunClient { metadata, _ := meta.Accessor(info.Object) annotationMap := metadata.GetAnnotations() if _, ok := annotationMap[corev1.LastAppliedConfigAnnotation]; !ok { fmt.Fprintf(o.ErrOut, warningNoLastAppliedConfigAnnotation, o.cmdBaseName) } patcher, err := newPatcher(o, info, helper) if err != nil { return err } patchBytes, patchedObject, err := patcher.Patch(info.Object, modified, info.Source, info.Namespace, info.Name, o.ErrOut) if err != nil { return cmdutil.AddSourceToErr(fmt.Sprintf(\"applying patch:\\n%s\\nto:\\n%v\\nfor:\", patchBytes, info), info.Source, err) } info.Refresh(patchedObject, true) if string(patchBytes) == \"{}\" \u0026\u0026 !o.shouldPrintObject() { printer, err := o.ToPrinter(\"unchanged\") if err != nil { return err } if err = printer.PrintObj(info.Object, o.Out); err != nil { return err } return nil } } ... return nil } apply 命令调用 o.Run() 执行 创建的动作 applyOneObject 方法是真正执行创建 Pod 的动作 在 applyOneObject 方法中通过 GetModifiedConfiguration 函数执行 get 动作 如果是 IsNotFound 异常则 执行 Create 动作 否则执行 Patch 动作 上述过程除了用 kubectl ，当然也可以自己调用 api 去创建或者更新 。 ","date":"2020-11-24","objectID":"/kubernetes/pod-create/:2:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/kubernetes/pod-create/"},{"categories":null,"content":"apiserver 通过客户端把请求提交到 apiserver 之后，接下来就是 apiserver 表演的时候了。 apiserver 会把 pod 数据存储到 etcd 中。 ","date":"2020-11-24","objectID":"/kubernetes/pod-create/:3:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/kubernetes/pod-create/"},{"categories":null,"content":"Schedule Schedule通过和API Server的watch机制，查看到新的Pod（通过 pod.spec.Node == nil 判断是否为新的Pod），尝试为Pod绑定Node。Schedle 的过程为 : 过滤主机：调度器用一组规则过滤掉不符合要求的主机，比如Pod指定了所需要的资源，那么就要过滤掉资源不够的主机 主机打分：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等 选择主机：选择打分最高的主机，进行binding操作 把调度结果通过apiserver 存储到 etcd 中 ","date":"2020-11-24","objectID":"/kubernetes/pod-create/:4:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/kubernetes/pod-create/"},{"categories":null,"content":"kubelet kubelet从apiserver获取分配到其所在节点的Pod（watch 机制） kubelet调用Docker的 Api 创建容器，创建容器的过程为: 先按顺序启动 initContainers 按照 yaml 中申明的顺序 并发启动 containers kubelet将Pod状态更新到apiserver apiserver将状态信息写到etcd中 ","date":"2020-11-24","objectID":"/kubernetes/pod-create/:5:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/kubernetes/pod-create/"},{"categories":null,"content":"总结 上面每个步骤单拎出来都能有好多东西可以深入下去，这里从流程上简单的介绍了一个Pod 从发起创建请求到最终创建的一个文字版流程，仅供参考。 后续再详细分析每个组件。 Pod 的创建过程 : kubectl apply 执行过程: 先通过 get 查找资源对象，如果不存在则 create pod 否则 patch pod 通过 apply 提交请求到 apiserver ，apiserver 把 pod 数据存储到 etcd 中 Schedule通过和API Server的watch机制，查看到新的Pod（通过 pod.spec.Node == nil 判断是否为新的Pod），尝试为Pod绑定Node。Schedle 的过程为: 过滤主机：调度器用一组规则过滤掉不符合要求的主机，比如Pod指定了所需要的资源，那么就要过滤掉资源不够的主机 主机打分：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等 选择主机：选择打分最高的主机，进行binding操作 把调度结果通过apiserver 存储到 etcd 中 kubelet从apiserver获取分配到其所在节点的Pod（watch 机制） kubelet调用Docker的 Api 创建容器，创建容器的过程为: 先按顺序启动 initContainers 按照 yaml 中申明的顺序 并发启动 containers kubelet将Pod状态更新到apiserver apiserver将状态信息写到etcd中 ","date":"2020-11-24","objectID":"/kubernetes/pod-create/:6:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/kubernetes/pod-create/"},{"categories":null,"content":"kubernetes 中 pod 是如何启动的呢","date":"2020-11-24","objectID":"/pod-create/","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/pod-create/"},{"categories":null,"content":"导读 Pod 应该算是 kubernetes 的基本盘，Pod 是 kubernetes 的最小调度单位，我这里有个问题，说通过 kubectl apply -f 创建一个 Pod ，从执行到 Pod 正常运行 kubernetes 做了什么事情呢？都有哪些组件参与呢？这篇文档主要讲述从提交 Pod 的创建请求到 Pod 的正常运行的这个过程追踪。 ","date":"2020-11-24","objectID":"/pod-create/:1:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/pod-create/"},{"categories":null,"content":"kubectl 阶段 kubectl apply 阶段其实分为两个过程 ，先调用 get 方法，查看资源是否存在，存在则调用 patch, 否则执行 create 方法。 这个过程可以参考源码（文章中所有源码都截取了部分，详细可以在在给出的链接中查看）: vendor/k8s.io/kubectl/pkg/cmd/apply/apply.go // NewCmdApply creates the `apply` command func NewCmdApply(baseName string, f cmdutil.Factory, ioStreams genericclioptions.IOStreams) *cobra.Command { o := NewApplyOptions(ioStreams) cmd := \u0026cobra.Command{ Use: \"apply (-f FILENAME | -k DIRECTORY)\", DisableFlagsInUseLine: true, Short: i18n.T(\"Apply a configuration to a resource by filename or stdin\"), Long: applyLong, Example: applyExample, Run: func(cmd *cobra.Command, args []string) { cmdutil.CheckErr(o.Complete(f, cmd)) cmdutil.CheckErr(validateArgs(cmd, args)) cmdutil.CheckErr(validatePruneAll(o.Prune, o.All, o.Selector)) cmdutil.CheckErr(o.Run()) }, } ... } // Run executes the `apply` command. func (o *ApplyOptions) Run() error { ... // Iterate through all objects, applying each one. for _, info := range infos { if err := o.applyOneObject(info); err != nil { errs = append(errs, err) } } ... return nil } func (o *ApplyOptions) applyOneObject(info *resource.Info) error { ... // Get the modified configuration of the object. Embed the result // as an annotation in the modified configuration, so that it will appear // in the patch sent to the server. modified, err := util.GetModifiedConfiguration(info.Object, true, unstructured.UnstructuredJSONScheme) if err != nil { return cmdutil.AddSourceToErr(fmt.Sprintf(\"retrieving modified configuration from:\\n%s\\nfor:\", info.String()), info.Source, err) } if err := info.Get(); err != nil { if !errors.IsNotFound(err) { return cmdutil.AddSourceToErr(fmt.Sprintf(\"retrieving current configuration of:\\n%s\\nfrom server for:\", info.String()), info.Source, err) } // Create the resource if it doesn't exist // First, update the annotation used by kubectl apply if err := util.CreateApplyAnnotation(info.Object, unstructured.UnstructuredJSONScheme); err != nil { return cmdutil.AddSourceToErr(\"creating\", info.Source, err) } if o.DryRunStrategy != cmdutil.DryRunClient { // Then create the resource and skip the three-way merge obj, err := helper.Create(info.Namespace, true, info.Object) if err != nil { return cmdutil.AddSourceToErr(\"creating\", info.Source, err) } info.Refresh(obj, true) } if err := o.MarkObjectVisited(info); err != nil { return err } if o.shouldPrintObject() { return nil } printer, err := o.ToPrinter(\"created\") if err != nil { return err } if err = printer.PrintObj(info.Object, o.Out); err != nil { return err } return nil } if err := o.MarkObjectVisited(info); err != nil { return err } if o.DryRunStrategy != cmdutil.DryRunClient { metadata, _ := meta.Accessor(info.Object) annotationMap := metadata.GetAnnotations() if _, ok := annotationMap[corev1.LastAppliedConfigAnnotation]; !ok { fmt.Fprintf(o.ErrOut, warningNoLastAppliedConfigAnnotation, o.cmdBaseName) } patcher, err := newPatcher(o, info, helper) if err != nil { return err } patchBytes, patchedObject, err := patcher.Patch(info.Object, modified, info.Source, info.Namespace, info.Name, o.ErrOut) if err != nil { return cmdutil.AddSourceToErr(fmt.Sprintf(\"applying patch:\\n%s\\nto:\\n%v\\nfor:\", patchBytes, info), info.Source, err) } info.Refresh(patchedObject, true) if string(patchBytes) == \"{}\" \u0026\u0026 !o.shouldPrintObject() { printer, err := o.ToPrinter(\"unchanged\") if err != nil { return err } if err = printer.PrintObj(info.Object, o.Out); err != nil { return err } return nil } } ... return nil } apply 命令调用 o.Run() 执行 创建的动作 applyOneObject 方法是真正执行创建 Pod 的动作 在 applyOneObject 方法中通过 GetModifiedConfiguration 函数执行 get 动作 如果是 IsNotFound 异常则 执行 Create 动作 否则执行 Patch 动作 上述过程除了用 kubectl ，当然也可以自己调用 api 去创建或者更新 。 ","date":"2020-11-24","objectID":"/pod-create/:2:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/pod-create/"},{"categories":null,"content":"apiserver 通过客户端把请求提交到 apiserver 之后，接下来就是 apiserver 表演的时候了。 apiserver 会把 pod 数据存储到 etcd 中。 ","date":"2020-11-24","objectID":"/pod-create/:3:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/pod-create/"},{"categories":null,"content":"Schedule Schedule通过和API Server的watch机制，查看到新的Pod（通过 pod.spec.Node == nil 判断是否为新的Pod），尝试为Pod绑定Node。Schedle 的过程为 : 过滤主机：调度器用一组规则过滤掉不符合要求的主机，比如Pod指定了所需要的资源，那么就要过滤掉资源不够的主机 主机打分：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等 选择主机：选择打分最高的主机，进行binding操作 把调度结果通过apiserver 存储到 etcd 中 ","date":"2020-11-24","objectID":"/pod-create/:4:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/pod-create/"},{"categories":null,"content":"kubelet kubelet从apiserver获取分配到其所在节点的Pod（watch 机制） kubelet调用Docker的 Api 创建容器，创建容器的过程为: 先按顺序启动 initContainers 按照 yaml 中申明的顺序 并发启动 containers kubelet将Pod状态更新到apiserver apiserver将状态信息写到etcd中 ","date":"2020-11-24","objectID":"/pod-create/:5:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/pod-create/"},{"categories":null,"content":"总结 上面每个步骤单拎出来都能有好多东西可以深入下去，这里从流程上简单的介绍了一个Pod 从发起创建请求到最终创建的一个文字版流程，仅供参考。 后续再详细分析每个组件。 Pod 的创建过程 : kubectl apply 执行过程: 先通过 get 查找资源对象，如果不存在则 create pod 否则 patch pod 通过 apply 提交请求到 apiserver ，apiserver 把 pod 数据存储到 etcd 中 Schedule通过和API Server的watch机制，查看到新的Pod（通过 pod.spec.Node == nil 判断是否为新的Pod），尝试为Pod绑定Node。Schedle 的过程为: 过滤主机：调度器用一组规则过滤掉不符合要求的主机，比如Pod指定了所需要的资源，那么就要过滤掉资源不够的主机 主机打分：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等 选择主机：选择打分最高的主机，进行binding操作 把调度结果通过apiserver 存储到 etcd 中 kubelet从apiserver获取分配到其所在节点的Pod（watch 机制） kubelet调用Docker的 Api 创建容器，创建容器的过程为: 先按顺序启动 initContainers 按照 yaml 中申明的顺序 并发启动 containers kubelet将Pod状态更新到apiserver apiserver将状态信息写到etcd中 ","date":"2020-11-24","objectID":"/pod-create/:6:0","tags":["kubernetes","pod","kubectl","kubelet","schedule"],"title":"kubernetes 中 pod 是如何启动的呢","uri":"/pod-create/"},{"categories":null,"content":"openresty 配置文件 （二）","date":"2020-11-23","objectID":"/openresty-server/","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"导读 这篇是继上一篇 openresty 配置文件 （一） 介绍了 openresty 全局配置之后，介绍 openresty server 配置，server 的配置一般单独放在 conf.d 目录下。下面是我比较推荐的 conf.d 目录结构： [root@iZuf685opgs9oyozju9i2bZ conf.d]# tree . ├── 443.conf ├── 8080.conf ├── 80.conf └── upstream.conf 0 directories, 4 files upstream 放在单独的 配置文件，当然如果比较多，可以按照 service/product 的维度再进行拆分。不同的监听放在单独的配置文件，相对来说比较好维护一点，也更容易自动化程序处理。 这篇文章比较长，可以通过目录直接跳转到自己感兴趣的部分。 ","date":"2020-11-23","objectID":"/openresty-server/:1:0","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"server server 模块是位于 http 模块下面，进行端口监听，并把请求转发到 upstream 或者直接响应，先看它的配置是什么样子。 server { #配置监听端口 # listen 详细配置参考 listen 一节 listen 80; #配置访问域名，可以只有一个名称，也可以由多个名称并列，之间用空格隔开。每个名字就是一个域名，由两段或者三段组成，之间由点号“.”隔开 # 第一个名称作为此虚拟主机的主要名称 # server_name 更加详细的用法参考下面 server_name 一节 server_name russellgao.cn russellgao.com localhost 127.0.0.1; # log 在全局变量中已经配置，但是每个监听中也可以配置，这样做的好处，在分析日志时比较方便，通过日志就可以知道请求从哪个监听中进来的 # 也可以放在具体的 location 中。 access_log /usr/local/openresty/nginx/logs/access.log custom; error_log /usr/local/openresty/nginx/logs/error.log; # ssl 配置 ssl on; ssl_certificate /usr/local/openresty/nginx/ssl/4753767.pem; ssl_certificate_key /usr/local/openresty/nginx/ssl/4753767.key; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1 TLSv1.2 TLSv1.1; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; # location 配置，location 介绍参考下面详细介绍 location / { root /usr/local/openresty/nginx/docs; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/local/openresty/nginx/html; } error_page 404 /404.html; location = /404.html { root /usr/local/openresty/nginx/docs; } } 一个 server 只能监听一个端口。 ","date":"2020-11-23","objectID":"/openresty-server/:2:0","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"listen listen 有三种配置语法。这个指令默认的配置值是：listen *:80 | *:8000；只能在server块种配置这个指令。 //第一种 listen address[:port] [default_server] [ssl] [http2 | spdy] [proxy_protocol] [setfib=number] [fastopen=number] [backlog=number] [rcvbuf=size] [sndbuf=size] [accept_filter=filter] [deferred] [bind] [ipv6only=on|off] [reuseport] [so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]]; //第二种 listen port [default_server] [ssl] [http2 | spdy] [proxy_protocol] [setfib=number] [fastopen=number] [backlog=number] [rcvbuf=size] [sndbuf=size] [accept_filter=filter] [deferred] [bind] [ipv6only=on|off] [reuseport] [so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]]; //第三种（可以不用重点关注） listen unix:path [default_server] [ssl] [http2 | spdy] [proxy_protocol] [backlog=number] [rcvbuf=size] [sndbuf=size] [accept_filter=filter] [deferred] [bind] [so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]]; listen指令的配置非常灵活，可以单独制定ip，单独指定端口或者同时指定ip和端口。 关于上面的一些重要参数做如下说明： address：监听的IP地址（请求来源的IP地址），如果是IPv6的地址，需要使用中括号“[]”括起来，比如[fe80::1]等。 port：端口号，如果只定义了IP地址没有定义端口号，就使用80端口。这边需要做个说明：要是你压根没配置listen指令，那么那么如果nginx以超级用户权限运行，则使用*:80，否则使用*:8000。多个虚拟主机可以同时监听同一个端口,但是server_name需要设置成不一样； default_server：假如通过Host没匹配到对应的虚拟主机，则通过这台虚拟主机处理。 backlog=number：设置监听函数listen()最多允许多少网络连接同时处于挂起状态，在FreeBSD中默认为-1，其他平台默认为511。 accept_filter=filter，设置监听端口对请求的过滤，被过滤的内容不能被接收和处理。本指令只在FreeBSD和NetBSD 5.0+平台下有效。filter可以设置为dataready或httpready，感兴趣的读者可以参阅Nginx的官方文档。 bind：标识符，使用独立的bind()处理此address:port；一般情况下，对于端口相同而IP地址不同的多个连接，Nginx服务器将只使用一个监听命令，并使用bind()处理端口相同的所有连接。 ssl：标识符，设置会话连接使用SSL模式进行，此标识符和Nginx服务器提供的HTTPS服务有关。 ","date":"2020-11-23","objectID":"/openresty-server/:2:1","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"server_name 用于配置虚拟主机的名称。语法是： Syntax: server_name name ...; Default: server_name \"\"; Context: server 对于name 来说，可以只有一个名称，也可以由多个名称并列，之间用空格隔开。每个名字就是一个域名，由两段或者三段组成，之间由点号“.”隔开。比如 server_name myserver.com www.myserver.com 在该例中，此虚拟主机的名称设置为myserver.com或www. myserver.com。Nginx服务器规定，第一个名称作为此虚拟主机的主要名称。 在name 中可以使用通配符“*”，但通配符只能用在由三段字符串组成的名称的首段或尾段，或者由两段字符串组成的名称的尾段，如： server_name myserver.* *.myserver.com 另外name还支持正则表达式的形式 由于server_name指令支持使用通配符和正则表达式两种配置名称的方式，因此在包含有多个虚拟主机的配置文件中，可能会出现一个名称被多个虚拟主机的server_name匹配成功。那么，来自这个名称的请求到底要交给哪个虚拟主机处理呢？Nginx服务器做出如下规定： a. 对于匹配方式不同的，按照以下的优先级选择虚拟主机，排在前面的优先处理请求。 准确匹配server_name 通配符在开始时匹配server_name成功 通配符在结尾时匹配server_name成功 正则表达式匹配server_name成功 b. 在以上四种匹配方式中，如果server_name被处于同一优先级的匹配方式多次匹配成功，则首次匹配成功的虚拟主机处理请求。 ","date":"2020-11-23","objectID":"/openresty-server/:2:2","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"location ","date":"2020-11-23","objectID":"/openresty-server/:3:0","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"基本语法 location [=|~|~*|^~] /uri/ { ... } = : 表示精确匹配后面的url ~ : 表示正则匹配，但是区分大小写 ~* : 正则匹配，不区分大小写 ^~ : 如果把这个前缀用于一个常规字符串,那么告诉nginx 如果路径匹配那么不测试正则表达式 「=」 修饰符：要求路径完全匹配 server { server_name russellgao.cn; location = /abcd { […] } } https://russellgao.cn/abcd匹配 https://russellgao.cn/ABCD可能会匹配 ，也可以不匹配，取决于操作系统的文件系统是否大小写敏感（case-sensitive）。 https://russellgao.cn/abcd?param1\u0026param2匹配，忽略 querystring https://russellgao.cn/abcd/不匹配，带有结尾的/ https://russellgao.cn/abcde不匹配 「~」修饰符：区分大小写的正则匹配 server { server_name russellgao.cn; location ~ ^/abcd$ { […] } } ^/abcd$ 这个正则表达式表示字符串必须以/开始，以d结束，中间必须是abc，换言之只能匹配 /abcd https://russellgao.cn/abcd匹配（完全匹配） https://russellgao.cn/ABCD不匹配，大小写敏感 https://russellgao.cn/abcd?param1\u0026param2匹配 https://russellgao.cn/abcd/不匹配，不能匹配正则表达式 https://russellgao.cn/abcde不匹配，不能匹配正则表达式 「~*」不区分大小写的正则匹配 server { server_name russellgao.cn; location ~* ^/abcd$ { […] } } https://russellgao.cn/abcd匹配 (完全匹配) https://russellgao.cn/ABCD匹配 (大小写不敏感) https://russellgao.cn/abcd?param1\u0026param2匹配 https://russellgao.cn/abcd/ 不匹配，不能匹配正则表达式 https://russellgao.cn/abcde 不匹配，不能匹配正则表达式 「^~」修饰符 前缀匹配 如果该 location 是最佳的匹配，那么对于匹配这个 location 的字符串， 该修饰符不再进行正则表达式检测。注意，这不是一个正则表达式匹配，它的目的是优先于正则表达式的匹配。 ","date":"2020-11-23","objectID":"/openresty-server/:3:1","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"查找的顺序及优先级 当有多条 location 规则时，nginx 有一套比较复杂的规则，优先级如下： 精确匹配 = 前缀匹配 ^~（立刻停止后续的正则搜索） 按文件中顺序的正则匹配 ~或~* 匹配不带任何修饰的前缀匹配。 这个规则大体的思路是: 先精确匹配，没有则查找带有 ^~的前缀匹配，没有则进行正则匹配，最后才返回前缀匹配的结果（如果有的话） ","date":"2020-11-23","objectID":"/openresty-server/:3:2","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"alias 与 root 区别 root 实际访问文件路径会拼接URL中的路径 alias 实际访问文件路径不会拼接URL中的路径 看一个例子 location ^~ /sta/ { alias /usr/local/nginx/html/static/; } 请求：https://russellgao.cn/sta/index.html 实际访问：/usr/local/nginx/html/static/index.html 文件 location ^~ /static/ { root /usr/local/nginx/html/; } 请求：https://russellgao.cn/static/index.html 实际访问：/usr/local/nginx/html/static/index.html 文件 ","date":"2020-11-23","objectID":"/openresty-server/:3:3","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"rewrite rewrite 模块主要用于重定向。 指令语法：rewrite regex replacement[flag]; ，默认值为 none 。 看个简单例子 : location / { rewrite ^/(.*) https://russellgao.cn/$1 permanent; } 这是我 http 强转 https 的例子。 ","date":"2020-11-23","objectID":"/openresty-server/:3:4","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"常用正则表达式 字符 描述 \\ 将后面接着的字符标记为一个特殊字符或者一个原义字符或一个向后引用 ^ 匹配输入字符串的起始位置 $ 匹配输入字符串的结束位置 * 匹配前面的字符零次或者多次 + 匹配前面字符串一次或者多次 ? 匹配前面字符串的零次或者一次 . 匹配除“\\n”之外的所有单个字符 (pattern) 匹配括号内的pattern ","date":"2020-11-23","objectID":"/openresty-server/:3:5","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"flag参数 标记符号 说明 last 本条规则匹配完成后继续向下匹配新的location URI规则 break 本条规则匹配完成后终止，不在匹配任何规则 redirect 返回302临时重定向 permanent 返回301永久重定向 last 和 break关键字的区别 last 匹配到了还会继续向下匹配 break 匹配到了不会继续向下匹配，会终止掉 permanent 和 redirect关键字的区别 last 和 break 当出现在location 之外时，两者的作用是一致的没有任何差异 last 和 break 当出现在location 内部时： rewrite … permanent 永久性重定向，请求日志中的状态码为301 rewrite … redirect 临时重定向，请求日志中的状态码为302 ","date":"2020-11-23","objectID":"/openresty-server/:3:6","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"proxy_pass 在nginx中配置proxy_pass代理转发时，如果在proxy_pass后面的url加/，表示绝对根路径；如果没有/，表示相对路径，把匹配的路径部分也给代理走。 假设我们访问地址为 : https://russellgao.cn/proxypass/index.html 当配置为 location /proxypass/ { proxy_pass https://russellgao.cn/; } 代理到: https://russellgao.cn/index.html 当配置为 location /proxypass/ { proxy_pass https://russellgao.cn; } 代理到: https://russellgao.cn/proxypass/index.html 请注意：proxy_pass 最后没有 / 当配置为 location /proxypass/ { proxy_pass https://russellgao.cn/test/; } 代理到: https://russellgao.cn/test/index.html 当配置为 location /proxypass/ { proxy_pass https://russellgao.cn/test; } 代理到: https://russellgao.cn/testindex.html nginx 的 ngx_http_proxy_module 和 ngx_stream_proxy_module 模块都有 proxy_pass ，下面看看两者之间的关系与区别。 ngx_http_proxy_module 语法: proxy_pass URL 场景: location if in location limit_except 设置后端代理服务器的协议(protocol)和地址(address),以及location中可以匹配的一个可选的URI。协议可以是\"http\"或\"https”。地址可以是一个域名或ip地址和端口，或者一个 unix-domain socket 路径。 例: location ~* (/api/v1/blog-server) { proxy_pass_header Server; proxy_pass http://blog_server; } ngx_stream_proxy_module 语法: proxy_pass address; 场景: server 设置后端代理服务器的地址。这个地址(address)可以是一个域名或ip地址和端口，或者一个 unix-domain socket路径。 例: server { listen 127.0.0.1:12345; proxy_pass 127.0.0.1:8080; } 在两个模块中，两个proxy_pass都是用来做后端代理的指令。 ngx_stream_proxy_module模块的proxy_pass指令只能在server段使用使用, 只需要提供域名或ip地址和端口。可以理解为端口转发，可以是tcp端口，也可以是udp端口。 ngx_http_proxy_module模块的proxy_pass指令需要在location段，location中的if段，limit_except段中使用，处理需要提供域名或ip地址和端口外，还需要提供协议，如\"http\"或\"https”，还有一个可选的uri可以配置。 ","date":"2020-11-23","objectID":"/openresty-server/:3:7","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"常见 location 配置样例 静态网站 server { listen 80; server_name russellgao.cn; access_log /usr/local/openresty/nginx/logs/access.log custom; error_log /usr/local/openresty/nginx/logs/error.log; location / { rewrite ^/(.*) https://russellgao.cn/$1 permanent; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/local/openresty/nginx/html; } error_page 404 /404.html; location = /404.html { root /usr/local/openresty/nginx/blog; } } 反向代理 location ~* (/api/v1/blog-server) { access_log /var/nginx/logs/blog_access.log custom; error_log /var/nginx/logs/blog_error.log error; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; # rewrite 只是举个例子，根据实际情况配置 # rewrite /api/v1/blog-server/(.*)$ /api/$1 break; proxy_pass http://blog_server; } 可以在 location 级别设置日志格式以及目录，方便精细化管理 通过proxy_pass 跳转到 upstream ","date":"2020-11-23","objectID":"/openresty-server/:3:8","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"upstream upstream 是后端服务器组，也称为虚拟服务器组，作用是负载均衡。配置样例参考 upstream blog_server { #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 172.19.208.76:80 weight=10; server 172.19.208.77:80 weight=50; server 172.19.208.78:80 weight=40; } nginx的upstream目前支持 5 种方式的分配: 轮询（默认）：每个请求按时间顺序逐一分配到不同的后端服务。如: upstream server { server 172.19.208.76:80; server 172.19.208.77:80; server 172.19.208.78:80; } 权重 ：指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 upstream server { server 172.19.208.76:80 weight=10; server 172.19.208.77:80 weight=50; server 172.19.208.78:80 weight=40; } ip_hash：每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 upstream server { ip_hash; server 172.19.208.76:80; server 172.19.208.77:80; server 172.19.208.78:80; } fair：按后端服务器的响应时间来分配请求，响应时间短的优先分配。 upstream server { fair; server 172.19.208.76:80; server 172.19.208.77:80; server 172.19.208.78:80; } url_hash：按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。如: upstream server { hash $request_uri; hash_method crc32; server 172.19.208.76:80; server 172.19.208.77:80; server 172.19.208.78:80; } 在 upstream 中可以给 server 设置状态，如: upstream server { server 172.19.208.76:80 down; server 172.19.208.77:80 weight=10; server 172.19.208.78:80 backup; } 支持的状态有: down表示单前的server暂时不参与负载 weight为weight越大，负载的权重就越大。 max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 fail_timeout:max_fails次失败后，暂停的时间。 backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 ","date":"2020-11-23","objectID":"/openresty-server/:4:0","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"参考 https://www.cnblogs.com/54chensongxia/p/12938929.html https://juejin.cn/post/6844903849166110733 ","date":"2020-11-23","objectID":"/openresty-server/:5:0","tags":["nginx","openresty","server","location"],"title":"openresty 配置文件 （二）","uri":"/openresty-server/"},{"categories":null,"content":"openresty 配置文件 （一）","date":"2020-11-22","objectID":"/openresty-nginx.conf/","tags":["nginx","openresty","nginx.conf"],"title":"openresty 配置文件 （一）","uri":"/openresty-nginx.conf/"},{"categories":null,"content":"导读 openresty（nginx plus） 在日常工作中用的应该比较多，要想真正了解清楚其原理并不容易。我尝试着从配置的角度去分析 nginx 的基本原理。这篇主要介绍 nginx.conf 这个配置文件，后续再介绍其他的配置文件。 nginx.conf 中主要配置全局配置，配置好之后一般很少改动。 ","date":"2020-11-22","objectID":"/openresty-nginx.conf/:1:0","tags":["nginx","openresty","nginx.conf"],"title":"openresty 配置文件 （一）","uri":"/openresty-nginx.conf/"},{"categories":null,"content":"nginx.conf 配置项说明 #定义Nginx运行的用户和用户组 #user nobody; #nginx进程数，建议设置为等于CPU总核心数。 worker_processes 1; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ] #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info; #进程文件 #pid logs/nginx.pid; #指定进程可以打开的最大描述符 #工作模式与连接数上限 ##这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。 #这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。 worker_rlimit_nofile 65535; events { #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 #补充说明： #与apache相类，nginx针对不同的操作系统，有不同的事件模型 #A）标准事件模型 #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll #B）高效事件模型 #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 #Epoll：使用于Linux内核2.6版本及以后的系统。 #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 use epoll #单个进程最大连接数（最大连接数=连接数+进程数） worker_connections 65535; #keepalive 超时时间 keepalive_timeout 60; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 #分页大小可以用命令getconf PAGESIZE 取得。 #[root@web001 ~]# getconf PAGESIZE #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 client_header_buffer_size 4k; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #这个是指多长时间检查一次缓存的有效信息。 #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid 80s; #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses 1; #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件是记录cache错误. open_file_cache_errors on; #默认是on。设置为on后，多个worker按串行方式来处理连接，也就是一个连接只有一个worker被唤醒，其他的处于休眠状态。 #设置为off后，多个worker按并行方式来处理连接，也就是一个连接会唤醒所有的worker，知道连接分配完毕，没有取得连接的继续休眠。 #当你的服务器连接数不多时，开启这个参数会让负载有一定程度的降低。但是当服务器的吞吐量很大时，为了效率，请关闭这个参数。 multi_accept off; } #设定http服务器 http { #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; # 增加 header add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Methods \"GET, POST, OPTIONS\"; #默认编码 charset utf-8; #服务器名字的hash表大小 #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 32k; #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 large_client_header_buffers 4 64k; #设定通过nginx上传文件的大小 client_max_body_size 8m; #开启目录列表访问，合适下载服务器，默认关闭。 autoindex on; # 限流 limit_req_zone $server_name zone=xiaozhi_log:50m rate=40r/s; limit_req_zone $server_name zone=tico_log:50m rate=40r/s; #开启限制IP连接数的时候需要使用 #limit_zone crawler $binary_remote_addr 10m; # 日志格式定义，这里可以根据自己的日志规范进行自定义 #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; # 这里推荐一个日志格式 log_format custom '[$time_local] $remote_addr $remote_user $request ' '$status $upstream_response_time $request_time $body_bytes_sent $http_referer ' '$http_user_agent $http_x_forwarded_for ' ; #access_log logs/access.log main; #开启高效文件传输模式，sendfil","date":"2020-11-22","objectID":"/openresty-nginx.conf/:2:0","tags":["nginx","openresty","nginx.conf"],"title":"openresty 配置文件 （一）","uri":"/openresty-nginx.conf/"},{"categories":null,"content":"nginx.conf 样例 worker_processes 2; events { worker_connections 65535; } http { include mime.types; default_type application/octet-stream; log_format custom '[$time_local] $remote_addr $remote_user $request ' '$status $upstream_response_time $request_time $body_bytes_sent $http_referer ' '$http_user_agent $http_x_forwarded_for ' ; client_body_temp_path /var/run/openresty/nginx-client-body; proxy_temp_path /var/run/openresty/nginx-proxy; fastcgi_temp_path /var/run/openresty/nginx-fastcgi; uwsgi_temp_path /var/run/openresty/nginx-uwsgi; scgi_temp_path /var/run/openresty/nginx-scgi; sendfile on; keepalive_timeout 90; include /etc/nginx/conf.d/*.conf; } ","date":"2020-11-22","objectID":"/openresty-nginx.conf/:3:0","tags":["nginx","openresty","nginx.conf"],"title":"openresty 配置文件 （一）","uri":"/openresty-nginx.conf/"},{"categories":null,"content":"接下来 这是 openresty 配置文件系列篇，这篇介绍了 nginx.conf (全局配置) ，接下来还有 server 、 upstream、location 等介绍。 每天掌握一个知识点，积少成多，一周玩转 openresty 。 ","date":"2020-11-22","objectID":"/openresty-nginx.conf/:4:0","tags":["nginx","openresty","nginx.conf"],"title":"openresty 配置文件 （一）","uri":"/openresty-nginx.conf/"},{"categories":null,"content":"参考 https://juejin.cn/post/6844903741678698510 ","date":"2020-11-22","objectID":"/openresty-nginx.conf/:5:0","tags":["nginx","openresty","nginx.conf"],"title":"openresty 配置文件 （一）","uri":"/openresty-nginx.conf/"},{"categories":null,"content":"kubernetes 中patch与update比较","date":"2020-11-21","objectID":"/kubernetes/patch-update/","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"导读 不知道你有没有想过一个问题：对于一个 K8s 资源对象比如 Deployment，我们尝试在修改其中 image 镜像时，如果有其他人同时也在对这个 Deployment 做修改，会发生什么？ 当然，这里还可以引申出两个问题： 如果双方修改的是同一个字段，比如 image 字段，结果会怎样？ 如果双方修改的是不同字段，比如一个修改 image，另一个修改 replicas，又会怎么样？ 其实，对一个 Kubernetes 资源对象做“更新”操作，简单来说就是通知 kube-apiserver 组件我们希望如何修改这个对象。而 K8s 为这类需求定义了两种“通知”方式，分别是 update 和 patch。在 update 请求中，我们需要将整个修改后的对象提交给 K8s；而对于 patch 请求，我们只需要将对象中某些字段的修改提交给 K8s。 ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:1:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"Update 机制 Kubernetes 中的所有资源对象，都有一个全局唯一的版本号（metadata.resourceVersion）。每个资源对象从创建开始就会有一个版本号，而后每次被修改（不管是 update 还是 patch 修改），版本号都会发生变化。 官方文档 告诉我们，这个版本号是一个 K8s 的内部机制，用户不应该假设它是一个数字或者通过比较两个版本号大小来确定资源对象的新旧，唯一能做的就是通过比较版本号相等来确定对象是否是同一个版本（即是否发生了变化）。而 resourceVersion 一个重要的用处，就是来做 update 请求的版本控制。 K8s 要求用户 update 请求中提交的对象必须带有 resourceVersion，也就是说我们提交 update 的数据必须先来源于 K8s 中已经存在的对象。因此，一次完整的 update 操作流程是： 首先，从 K8s 中拿到一个已经存在的对象（可以选择直接从 K8s 中查询；如果在客户端做了 list watch，推荐从本地 informer 中获取）； 然后，基于这个取出来的对象做一些修改，比如将 Deployment 中的 replicas 做增减，或是将 image 字段修改为一个新版本的镜像； 最后，将修改后的对象通过 update 请求提交给 K8s； 此时，kube-apiserver 会校验用户 update 请求提交对象中的 resourceVersion 一定要和当前 K8s 中这个对象最新的 resourceVersion 一致，才能接受本次 update。否则，K8s 会拒绝请求，并告诉用户发生了版本冲突（Conflict）。 上图展示了多个用户同时 update 某一个资源对象时会发生的事情。而如果如果发生了 Conflict 冲突，对于 User A 而言应该做的就是做一次重试，再次获取到最新版本的对象，修改后重新提交 update。 因此，我们上面的两个问题也都得到了解答： 用户修改 YAML 后提交 update 失败，是因为 YAML 文件中没有包含 resourceVersion 字段。对于 update 请求而言，应该取出当前 K8s 中的对象做修改后提交； 如果两个用户同时对一个资源对象做 update，不管操作的是对象中同一个字段还是不同字段，都存在版本控制的机制确保两个用户的 update 请求不会发生覆盖。 ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:2:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"Patch 机制 相比于 update 的版本控制，K8s 的 patch 机制则显得更加简单。 当用户对某个资源对象提交一个 patch 请求时，kube-apiserver 不会考虑版本问题，而是“无脑”地接受用户的请求（只要请求发送的 patch 内容合法），也就是将 patch 打到对象上、同时更新版本号。 不过，patch 的复杂点在于，目前 K8s 提供了 4 种 patch 策略：json patch、merge patch、strategic merge patch、apply patch（从 K8s 1.14 支持 server-side apply 开始）。通过 kubectl patch -h 命令我们也可以看到这个策略选项（默认采用 strategic）： $ kubectl patch -h # ... --type='strategic': The type of patch being provided; one of [json merge strategic] 篇幅限制这里暂不对每个策略做详细的介绍了，我们就以一个简单的例子来看一下它们的差异性。如果针对一个已有的 Deployment 对象，假设 template 中已经有了一个名为 app 的容器： 如果要在其中新增一个 nginx 容器，如何 patch 更新？ 如果要修改 app 容器的镜像，如何 patch 更新？ ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:3:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"json patch 新增容器： kubectl patch deployment/foo --type='json' -p \\ '[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/1\",\"value\":{\"name\":\"nginx\",\"image\":\"nginx:alpine\"}}]' 修改已有容器 image： kubectl patch deployment/foo --type='json' -p \\ '[{\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/image\",\"value\":\"app-image:v2\"}]' 这样一来，如果我们 patch 之前这个对象已经被其他人修改了，那么我们的 patch 有可能产生非预期的后果。比如在执行 app 容器镜像更新时，我们指定的序号是 0，但此时 containers 列表中第一个位置被插入了另一个容器，则更新的镜像就被错误地插入到这个非预期的容器中。 ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:3:1","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"merge patch merge patch 无法单独更新一个列表中的某个元素，因此不管我们是要在 containers 里新增容器、还是修改已有容器的 image、env 等字段，都要用整个 containers 列表来提交 patch： kubectl patch deployment/foo --type='merge' -p \\ '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"app\",\"image\":\"app-image:v2\"},{\"name\":\"nginx\",\"image\":\"nginx:alpline\"}]}}}}' 显然，这个策略并不适合我们对一些列表深层的字段做更新，更适用于大片段的覆盖更新。 不过对于 labels/annotations 这些 map 类型的元素更新，merge patch 是可以单独指定 key-value 操作的，相比于 json patch 方便一些，写起来也更加直观： kubectl patch deployment/foo –type='merge’ -p ‘{“metadata”:{“labels”:{“test-key”:“foo”}}}’ ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:3:2","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"strategic merge patch 这种 patch 策略并没有一个通用的 RFC 标准，而是 K8s 独有的，不过相比前两种而言却更为强大的。 我们先从 K8s 源码看起，在 K8s 原生资源的数据结构定义中额外定义了一些的策略注解。比如以下这个截取了 podSpec 中针对 containers 列表的定义 // ... // +patchMergeKey=name // +patchStrategy=merge Containers []Container `json:\"containers\" patchStrategy:\"merge\" patchMergeKey:\"name\" protobuf:\"bytes,2,rep,name=containers\"` 可以看到其中有两个关键信息：patchStrategy:“merge” patchMergeKey:“name” 。这就代表了，containers 列表使用 strategic merge patch 策略更新时，会把下面每个元素中的 name 字段看作 key。 简单来说，在我们 patch 更新 containers 不再需要指定下标序号了，而是指定 name 来修改，K8s 会把 name 作为 key 来计算 merge。比如针对以下的 patch 操作： kubectl patch deployment/foo -p \\ '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"image\":\"nginx:mainline\"}]}}}}' 如果 K8s 发现当前 containers 中已经有名字为 nginx 的容器，则只会把 image 更新上去；而如果当前 containers 中没有 nginx 容器，K8s 会把这个容器插入 containers 列表。 此外还要说明的是，目前 strategic 策略只能用于原生 K8s 资源以及 Aggregated API 方式的自定义资源，对于 CRD 定义的资源对象，是无法使用的。这很好理解，因为 kube-apiserver 无法得知 CRD 资源的结构和 merge 策略。如果用 kubectl patch 命令更新一个 CR，则默认会采用 merge patch 的策略来操作。 ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:3:3","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"kubectl 封装 了解完了 K8s 的基础更新机制，我们再次回到最初的问题上。为什么用户修改 YAML 文件后无法直接调用 update 接口更新，却可以通过 kubectl apply 命令更新呢？ 其实 kubectl 为了给命令行用户提供良好的交互体感，设计了较为复杂的内部执行逻辑，诸如 apply、edit 这些常用操作其实背后并非对应一次简单的 update 请求。毕竟 update 是有版本控制的，如果发生了更新冲突对于普通用户并不友好。以下简略介绍下 kubectl 几种更新操作的逻辑，有兴趣可以看一下 link:https://github.com/kubernetes/kubectl[kubectl] 封装的源码。 ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:3:4","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"apply 在使用默认参数执行 apply 时，触发的是 client-side apply。kubectl 逻辑如下： 首先解析用户提交的数据（YAML/JSON）为一个对象 A；然后调用 Get 接口从 K8s 中查询这个资源对象： 如果查询结果不存在，kubectl 将本次用户提交的数据记录到对象 A 的 annotation 中（key 为 kubectl.kubernetes.io/last-applied-configuration），最后将对象 A提交给 K8s 创建； 如果查询到 K8s 中已有这个资源，假设为对象 B：1. kubectl 尝试从对象 B 的 annotation 中取出 kubectl.kubernetes.io/last-applied-configuration 的值（对应了上一次 apply 提交的内容）；2. kubectl 根据前一次 apply 的内容和本次 apply 的内容计算出 diff（默认为 strategic merge patch 格式，如果非原生资源则采用 merge patch）；3. 将 diff 中添加本次的 kubectl.kubernetes.io/last-applied-configuration annotation，最后用 patch 请求提交给 K8s 做更新。 ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:3:5","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"edit kubectl edit 逻辑上更简单一些。在用户执行命令之后，kubectl 从 K8s 中查到当前的资源对象，并打开一个命令行编辑器（默认用 vi）为用户提供编辑界面。 当用户修改完成、保存退出时，kubectl 并非直接把修改后的对象提交 update（避免 Conflict，如果用户修改的过程中资源对象又被更新），而是会把修改后的对象和初始拿到的对象计算 diff，最后将 diff 内容用 patch 请求提交给 K8s。 ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:3:6","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"总结 看了上述的介绍，大家应该对 K8s 更新机制有了一个初步的了解了。接下来想一想，既然 K8s 提供了两种更新方式，我们在不同的场景下怎么选择 update 或 patch 来使用呢？这里我们的建议是： 如果要更新的字段只有我们自己会修改（比如我们有一些自定义标签，并写了 operator 来管理），则使用 patch 是最简单的方式； 如果要更新的字段可能会被其他方修改（比如我们修改的 replicas 字段，可能有一些其他组件比如 HPA 也会做修改），则建议使用 update 来更新，避免出现互相覆盖。 ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:4:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"参考 https://aijishu.com/a/1060000000118183 ","date":"2020-11-21","objectID":"/kubernetes/patch-update/:5:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/kubernetes/patch-update/"},{"categories":null,"content":"kubernetes 中patch与update比较","date":"2020-11-21","objectID":"/patch-update/","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"导读 不知道你有没有想过一个问题：对于一个 K8s 资源对象比如 Deployment，我们尝试在修改其中 image 镜像时，如果有其他人同时也在对这个 Deployment 做修改，会发生什么？ 当然，这里还可以引申出两个问题： 如果双方修改的是同一个字段，比如 image 字段，结果会怎样？ 如果双方修改的是不同字段，比如一个修改 image，另一个修改 replicas，又会怎么样？ 其实，对一个 Kubernetes 资源对象做“更新”操作，简单来说就是通知 kube-apiserver 组件我们希望如何修改这个对象。而 K8s 为这类需求定义了两种“通知”方式，分别是 update 和 patch。在 update 请求中，我们需要将整个修改后的对象提交给 K8s；而对于 patch 请求，我们只需要将对象中某些字段的修改提交给 K8s。 ","date":"2020-11-21","objectID":"/patch-update/:1:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"Update 机制 Kubernetes 中的所有资源对象，都有一个全局唯一的版本号（metadata.resourceVersion）。每个资源对象从创建开始就会有一个版本号，而后每次被修改（不管是 update 还是 patch 修改），版本号都会发生变化。 官方文档 告诉我们，这个版本号是一个 K8s 的内部机制，用户不应该假设它是一个数字或者通过比较两个版本号大小来确定资源对象的新旧，唯一能做的就是通过比较版本号相等来确定对象是否是同一个版本（即是否发生了变化）。而 resourceVersion 一个重要的用处，就是来做 update 请求的版本控制。 K8s 要求用户 update 请求中提交的对象必须带有 resourceVersion，也就是说我们提交 update 的数据必须先来源于 K8s 中已经存在的对象。因此，一次完整的 update 操作流程是： 首先，从 K8s 中拿到一个已经存在的对象（可以选择直接从 K8s 中查询；如果在客户端做了 list watch，推荐从本地 informer 中获取）； 然后，基于这个取出来的对象做一些修改，比如将 Deployment 中的 replicas 做增减，或是将 image 字段修改为一个新版本的镜像； 最后，将修改后的对象通过 update 请求提交给 K8s； 此时，kube-apiserver 会校验用户 update 请求提交对象中的 resourceVersion 一定要和当前 K8s 中这个对象最新的 resourceVersion 一致，才能接受本次 update。否则，K8s 会拒绝请求，并告诉用户发生了版本冲突（Conflict）。 上图展示了多个用户同时 update 某一个资源对象时会发生的事情。而如果如果发生了 Conflict 冲突，对于 User A 而言应该做的就是做一次重试，再次获取到最新版本的对象，修改后重新提交 update。 因此，我们上面的两个问题也都得到了解答： 用户修改 YAML 后提交 update 失败，是因为 YAML 文件中没有包含 resourceVersion 字段。对于 update 请求而言，应该取出当前 K8s 中的对象做修改后提交； 如果两个用户同时对一个资源对象做 update，不管操作的是对象中同一个字段还是不同字段，都存在版本控制的机制确保两个用户的 update 请求不会发生覆盖。 ","date":"2020-11-21","objectID":"/patch-update/:2:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"Patch 机制 相比于 update 的版本控制，K8s 的 patch 机制则显得更加简单。 当用户对某个资源对象提交一个 patch 请求时，kube-apiserver 不会考虑版本问题，而是“无脑”地接受用户的请求（只要请求发送的 patch 内容合法），也就是将 patch 打到对象上、同时更新版本号。 不过，patch 的复杂点在于，目前 K8s 提供了 4 种 patch 策略：json patch、merge patch、strategic merge patch、apply patch（从 K8s 1.14 支持 server-side apply 开始）。通过 kubectl patch -h 命令我们也可以看到这个策略选项（默认采用 strategic）： $ kubectl patch -h # ... --type='strategic': The type of patch being provided; one of [json merge strategic] 篇幅限制这里暂不对每个策略做详细的介绍了，我们就以一个简单的例子来看一下它们的差异性。如果针对一个已有的 Deployment 对象，假设 template 中已经有了一个名为 app 的容器： 如果要在其中新增一个 nginx 容器，如何 patch 更新？ 如果要修改 app 容器的镜像，如何 patch 更新？ ","date":"2020-11-21","objectID":"/patch-update/:3:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"json patch 新增容器： kubectl patch deployment/foo --type='json' -p \\ '[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/1\",\"value\":{\"name\":\"nginx\",\"image\":\"nginx:alpine\"}}]' 修改已有容器 image： kubectl patch deployment/foo --type='json' -p \\ '[{\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/image\",\"value\":\"app-image:v2\"}]' 这样一来，如果我们 patch 之前这个对象已经被其他人修改了，那么我们的 patch 有可能产生非预期的后果。比如在执行 app 容器镜像更新时，我们指定的序号是 0，但此时 containers 列表中第一个位置被插入了另一个容器，则更新的镜像就被错误地插入到这个非预期的容器中。 ","date":"2020-11-21","objectID":"/patch-update/:3:1","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"merge patch merge patch 无法单独更新一个列表中的某个元素，因此不管我们是要在 containers 里新增容器、还是修改已有容器的 image、env 等字段，都要用整个 containers 列表来提交 patch： kubectl patch deployment/foo --type='merge' -p \\ '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"app\",\"image\":\"app-image:v2\"},{\"name\":\"nginx\",\"image\":\"nginx:alpline\"}]}}}}' 显然，这个策略并不适合我们对一些列表深层的字段做更新，更适用于大片段的覆盖更新。 不过对于 labels/annotations 这些 map 类型的元素更新，merge patch 是可以单独指定 key-value 操作的，相比于 json patch 方便一些，写起来也更加直观： kubectl patch deployment/foo –type='merge’ -p ‘{“metadata”:{“labels”:{“test-key”:“foo”}}}’ ","date":"2020-11-21","objectID":"/patch-update/:3:2","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"strategic merge patch 这种 patch 策略并没有一个通用的 RFC 标准，而是 K8s 独有的，不过相比前两种而言却更为强大的。 我们先从 K8s 源码看起，在 K8s 原生资源的数据结构定义中额外定义了一些的策略注解。比如以下这个截取了 podSpec 中针对 containers 列表的定义 // ... // +patchMergeKey=name // +patchStrategy=merge Containers []Container `json:\"containers\" patchStrategy:\"merge\" patchMergeKey:\"name\" protobuf:\"bytes,2,rep,name=containers\"` 可以看到其中有两个关键信息：patchStrategy:“merge” patchMergeKey:“name” 。这就代表了，containers 列表使用 strategic merge patch 策略更新时，会把下面每个元素中的 name 字段看作 key。 简单来说，在我们 patch 更新 containers 不再需要指定下标序号了，而是指定 name 来修改，K8s 会把 name 作为 key 来计算 merge。比如针对以下的 patch 操作： kubectl patch deployment/foo -p \\ '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"image\":\"nginx:mainline\"}]}}}}' 如果 K8s 发现当前 containers 中已经有名字为 nginx 的容器，则只会把 image 更新上去；而如果当前 containers 中没有 nginx 容器，K8s 会把这个容器插入 containers 列表。 此外还要说明的是，目前 strategic 策略只能用于原生 K8s 资源以及 Aggregated API 方式的自定义资源，对于 CRD 定义的资源对象，是无法使用的。这很好理解，因为 kube-apiserver 无法得知 CRD 资源的结构和 merge 策略。如果用 kubectl patch 命令更新一个 CR，则默认会采用 merge patch 的策略来操作。 ","date":"2020-11-21","objectID":"/patch-update/:3:3","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"kubectl 封装 了解完了 K8s 的基础更新机制，我们再次回到最初的问题上。为什么用户修改 YAML 文件后无法直接调用 update 接口更新，却可以通过 kubectl apply 命令更新呢？ 其实 kubectl 为了给命令行用户提供良好的交互体感，设计了较为复杂的内部执行逻辑，诸如 apply、edit 这些常用操作其实背后并非对应一次简单的 update 请求。毕竟 update 是有版本控制的，如果发生了更新冲突对于普通用户并不友好。以下简略介绍下 kubectl 几种更新操作的逻辑，有兴趣可以看一下 link:https://github.com/kubernetes/kubectl[kubectl] 封装的源码。 ","date":"2020-11-21","objectID":"/patch-update/:3:4","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"apply 在使用默认参数执行 apply 时，触发的是 client-side apply。kubectl 逻辑如下： 首先解析用户提交的数据（YAML/JSON）为一个对象 A；然后调用 Get 接口从 K8s 中查询这个资源对象： 如果查询结果不存在，kubectl 将本次用户提交的数据记录到对象 A 的 annotation 中（key 为 kubectl.kubernetes.io/last-applied-configuration），最后将对象 A提交给 K8s 创建； 如果查询到 K8s 中已有这个资源，假设为对象 B：1. kubectl 尝试从对象 B 的 annotation 中取出 kubectl.kubernetes.io/last-applied-configuration 的值（对应了上一次 apply 提交的内容）；2. kubectl 根据前一次 apply 的内容和本次 apply 的内容计算出 diff（默认为 strategic merge patch 格式，如果非原生资源则采用 merge patch）；3. 将 diff 中添加本次的 kubectl.kubernetes.io/last-applied-configuration annotation，最后用 patch 请求提交给 K8s 做更新。 ","date":"2020-11-21","objectID":"/patch-update/:3:5","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"edit kubectl edit 逻辑上更简单一些。在用户执行命令之后，kubectl 从 K8s 中查到当前的资源对象，并打开一个命令行编辑器（默认用 vi）为用户提供编辑界面。 当用户修改完成、保存退出时，kubectl 并非直接把修改后的对象提交 update（避免 Conflict，如果用户修改的过程中资源对象又被更新），而是会把修改后的对象和初始拿到的对象计算 diff，最后将 diff 内容用 patch 请求提交给 K8s。 ","date":"2020-11-21","objectID":"/patch-update/:3:6","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"总结 看了上述的介绍，大家应该对 K8s 更新机制有了一个初步的了解了。接下来想一想，既然 K8s 提供了两种更新方式，我们在不同的场景下怎么选择 update 或 patch 来使用呢？这里我们的建议是： 如果要更新的字段只有我们自己会修改（比如我们有一些自定义标签，并写了 operator 来管理），则使用 patch 是最简单的方式； 如果要更新的字段可能会被其他方修改（比如我们修改的 replicas 字段，可能有一些其他组件比如 HPA 也会做修改），则建议使用 update 来更新，避免出现互相覆盖。 ","date":"2020-11-21","objectID":"/patch-update/:4:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"参考 https://aijishu.com/a/1060000000118183 ","date":"2020-11-21","objectID":"/patch-update/:5:0","tags":["kubernetes","api","patch","update"],"title":"kubernetes 中patch与update比较","uri":"/patch-update/"},{"categories":null,"content":"kubectl 常用命令指南","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"导读 kubectl 应该是每个接触 kubernetes 的人都会接触的一个组件，它带给我们强大的命令行体验，本篇文章就是介绍 kubectl 中的一些常用命令，在结合一些具体的使用场景说说如何利用 kubectl 实现。好记性不如烂笔头，在这里尽可能全的罗列，方便后续 用的时候查找。如果能帮到您就收藏起来吧(😄)。 本次实验环境是 kubernetes-1.16.9，本篇文档的写作思路是按照平时 的使用场景进行写作，不会详细介绍 kubectl 的命令，kubectl 详细的帮助文档参考 kubectl --help or kubectl command --help 。 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:1:0","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"kubectl 支持的命令 kubectl --help kubectl controls the Kubernetes cluster manager. Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/ Basic Commands (Beginner): create Create a resource from a file or from stdin. expose 使用 replication controller, service, deployment 或者 pod 并暴露它作为一个 新的 Kubernetes Service run 在集群中运行一个指定的镜像 set 为 objects 设置一个指定的特征 Basic Commands (Intermediate): explain 查看资源的文档 get 显示一个或更多 resources edit 在服务器上编辑一个资源 delete Delete resources by filenames, stdin, resources and names, or by resources and label selector Deploy Commands: rollout Manage the rollout of a resource scale 为 Deployment, ReplicaSet, Replication Controller 或者 Job 设置一个新的副本数量 autoscale 自动调整一个 Deployment, ReplicaSet, 或者 ReplicationController 的副本数量 Cluster Management Commands: certificate 修改 certificate 资源. cluster-info 显示集群信息 top Display Resource (CPU/Memory/Storage) usage. cordon 标记 node 为 unschedulable uncordon 标记 node 为 schedulable drain Drain node in preparation for maintenance taint 更新一个或者多个 node 上的 taints Troubleshooting and Debugging Commands: describe 显示一个指定 resource 或者 group 的 resources 详情 logs 输出容器在 pod 中的日志 attach Attach 到一个运行中的 container exec 在一个 container 中执行一个命令 port-forward Forward one or more local ports to a pod proxy 运行一个 proxy 到 Kubernetes API server cp 复制 files 和 directories 到 containers 和从容器中复制 files 和 directories. auth Inspect authorization Advanced Commands: diff Diff live version against would-be applied version apply 通过文件名或标准输入流(stdin)对资源进行配置 patch 使用 strategic merge patch 更新一个资源的 field(s) replace 通过 filename 或者 stdin替换一个资源 wait Experimental: Wait for a specific condition on one or many resources. convert 在不同的 API versions 转换配置文件 kustomize Build a kustomization target from a directory or a remote url. Settings Commands: label 更新在这个资源上的 labels annotate 更新一个资源的注解 completion Output shell completion code for the specified shell (bash or zsh) Other Commands: api-resources Print the supported API resources on the server api-versions Print the supported API versions on the server, in the form of \"group/version\" config 修改 kubeconfig 文件 plugin Provides utilities for interacting with plugins. version 输出 client 和 server 的版本信息 Usage: kubectl [flags] [options] Use \"kubectl \u003ccommand\u003e --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). 从上面的帮助文档可以看出， kubectl 基本格式为 kubectl verb resource options , kubectl 后跟谓语动词， 再跟要操作的资源，可以加 options ，如： 要看 monitoring namespace 下面有哪些pod : kubectl -n monitoring get po ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:2:0","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"pod pod 场景下，可能会有如下需求: ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:0","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看某个 namespace 下，所有的pod # 先查看有哪些namespace kubectl get namespace # 查看 pod kubectl -n $namespace get po # 或者 kubectl get po -n $namespace 上面两种写法在达到的效果上是一样的，但是有一个细节可以注意一下，如果 kubectl 环境有命令自动补全的话，资源对象又比较多 的情况下，第一种写法将会有极大的优势，可以思考这么个场景，如：要查看 monitoring namespace 下的某个pod 详情, 就可以通过: kubectl -n monitoring get po 加 tab 键，列出这个namespace 下的所有 pod 供筛选。 centos 下命令自动补全需要安装 bash-completion ，方法为 yum install -y bash-completion 如果不加 -n $namespace ，则默认是 default namespace ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:1","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看所有namespace 的pod kubectl get po --all-namespaces # or kubectl get po -A ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:2","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看某个具体的 pod 信息 ，以 wide、json、yaml 的格式输出 kubectl -n $namespace get po xxx -o wide/json/yaml # 如 查看 monitoring 下的 prometheus-0 pod 信息，并以yaml 形式输出。 kubectl -n monitoring get po prometheus-0 -o yaml ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:3","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看某个 pod 的某个字段信息 如果我们只想知道 pod 的 hostIP 或者其他的 一些字段， 可以通过 -o jsonpath or -o template or -o go-template 其中template 语法遵循 golang template 需要对 pod 的对象模型有一定的了解，如果不了解，可以 -o yaml or -o json 直接查看。 查看 hostIP 的方法如下: # -o jsonpath kubectl -n monitoring get po prometheus-k8s-0 -o jsonpath=\"{.status.hostIP}\" # -o template kubectl -n monitoring get po prometheus-k8s-0 -o template --template=\"{{.status.hostIP}}\" # -o go-template kubectl -n monitoring get po prometheus-k8s-0 -o go-template=\"{{.status.hostIP}}\" 如果需要查看其他的字段照猫画虎即可。 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:4","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"通过标签选择查看 pod 通过 -l key1=value1,key2=value2 进行选择，如 kubectl -n monitoring get po -l app=prometheus kubectl -n monitoring get po -l app=prometheus,prometheus=k8s ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:5","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看某个node 上部署的所有 pod #先获取集群内所有的node kubectl get node -o wide # 假设其中一个 node 的名称为 node-0001 kubectl get po -A -o wide | grep node-0001 通过 kubectl get po -A -o wide | grep 可以做很多事情，具体可以根据情况而定，比如查看所有状态异常的 pod （非 Running） kubectl get po -A -o wide | grep -v Running ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:6","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看 pod 的详细信息 kubectl -n monitoring describe po prometheus-k8s-0 这个命令在查看 pod 的基本信息和问题定位时特别有用，当 pod 异常，可以查看 Events 或许就能发现问题所在。 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:7","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看 pod log kubectl -n $namespace logs -f $podName $containerName # 其中 $namespace，$podName，$containerName 替换成真实值即可，当 pod 中只有一个 容器时可省略 $containerName，如： kubectl -n monitoring logs -f prometheus-k8s-0 prometheus ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:8","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"进入容器 kubectl -n $namespace exec -it $podName -c $containerName sh # 其中 $namespace，$podName，$containerName 替换成真实值即可，当 pod 中只有一个 容器时可省略 -c $containerName，如： kubectl -n monitoring exec -it prometheus-k8s-0 -c prometheus sh ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:9","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看 pod 的资源使用情况 kubectl -n $namespace top pod # 其中 $namespace 替换成真实值即可，如： kubectl -n monitoring top pod ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:10","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"删除 pod kubectl -n $namespace delete po $podName kubectl -n monitoring delete po prometheus-k8s-0 # 在某些异常情况下删除 pod 会卡住，删不掉，需要强制才能删除 ，强制删除需要增加 --grace-period=0 --force ， kubectl -n monitoring delete po prometheus-k8s-0 --grace-period=0 --force 原理如下， 默认执行 delete po 时，kubectl 会增加–grace-period=30 参数，表示预留30秒的时间给 pod 处理当前的请求， 但同时也不接收新的请求了，以一种相对优雅的方式停止容器，注意这个参数在创建 pod 时可以指定，默认是30秒。强制删除时需要把–grace-period 设置为0，表示不等待马上删除，否则强制删除就会失效。 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:11","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"pod 标签管理 pod 的大多数的情况都会由 deployment or statefulset 来管理，所以标签也会通过它们管理，实际情况下很少会通过 kubectl 对 pod label 做增删改，如有需要可参考 下面 node 的用法，只需要把资源对象换成 pod 即可。 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:12","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"文件 copy 从 pod 中 copy 文件或者 copy 到 pod 中去。 容器中需要有 tar 命令，否则会失败 # 从本地 copy 到 pod kubectl cp /tmp/foo_dir \u003csome-pod\u003e:/tmp/bar_dir kubectl -n monitoring cp abc.txt prometheus-k8s-0:/tmp/abc.txt # 如果 pod 中有多个 container 可以用 -c 指定 container kubectl cp /tmp/foo \u003csome-pod\u003e:/tmp/bar -c \u003cspecific-container\u003e kubectl -n monitoring cp abc.txt prometheus-k8s-0:/tmp/abc.txt -c prometheus # 从 pod copy 到 本地 kubectl cp \u003csome-pod\u003e:/tmp/foo /tmp/bar kubectl -n monitoring cp prometheus-k8s-0:/tmp/abc.txt /tmp/abd.txt ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:3:13","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"node 在 pod 一节 已经了解了 kubectl get ,kubectl describe , 等相关的用法，node 的操作和 pod 类似，只是后面接的资源对象不同。 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:4:0","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看有哪些node以及其基本信息 kubectl get node -o wide ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:4:1","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看 node 上的详细情况 # 查看所有 node 的详细信息 kubectl describe node # 也可以查看某个 node 的信息 kubectl describe node node-0001 ... 这个命令在定位 node 的问题很有用，会输出如下信息: Labels Annotations Non-terminated Pods (正在运行的 pod) Allocated resources (已经分配的资源) … ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:4:2","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"查看 node 的资源使用情况 kubectl top node ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:4:3","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"node 的标签管理 增加标签 kubectl label node $nodename key1=value1 key2=value2 # 如 kubectl label node node-0001 a1=bbb a2=ccc 更新标签 # 在 增加标签的基础 加 --overwrite 参数 kubectl label node node-0001 a1=bbb --overwrite # 当标签不存在也可以 加 --overwrite 参数 kubectl label node node-0001 a10=bbb --overwrite 删除标签 kubectl label node $nodename key1- key2- kubectl label node node-0001 a10- a3- ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:4:4","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"将一个 node 标记为不可调度/可调度 在调试过程中或者当其中的某些 node 出现问题时，需要将 node 标记为不可调度，等恢复回来再标记回来。 # 将一个 node 可以 标记为不可调度(unschedulable) ，如果只是看看效果，而不是真正标记可加 --dry-run 参数 kubectl cordon $nodeName kubectl cordon node-0001 kubectl cordon node-0001 --dry-run # 将一个 node 可以 标记为可调度(schedulable) ，如果只是看看效果，而不是真正标记可加 --dry-run 参数 kubectl uncordon $nodeName kubectl uncordon node-0001 kubectl uncordon node-0001 --dry-run ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:4:5","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"排空 node 上的 pod # 排空node 上的所有 pod ，即使没有被 rc 管理，但是不会排空 被 daemonset 管理的 pod， 因为排空之后又会马上创建出来 kubectl drain foo --force ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:4:6","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"node 上的污点（taint）管理 污点需要配合 pod 的亲和性使用，否则污点没有什么意义 # 增加/更新 taint kubectl taint nodes node-0001 dedicated=special-user:NoSchedule --overwrite # 删除 taint kubectl taint nodes foo dedicated:NoSchedule- kubectl taint nodes foo dedicated- 整体用法和 label 类似 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:4:7","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"node 的 annotate 管理 和 label 是类似的，只是把 verb 换成 annotate 即可 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:4:8","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"其他场景 上面通过 pod 和 node 的例子，穿插的介绍了大部分的 verb（如 get 、describe、top … ），这个小节再介绍其他的一些常用场景 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:5:0","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"apply 在准备好一个资源对象的 yaml 文件时可以用 kubectl apply -f xxx.ymal 使之生效，kubernetes 的api 中并没有 apply，api 中有的是 create 、update、patch 等，apply 是kubectl 自己封装实现的，先执行 get ，再判断是 create 还是 patch，所以用kubectl 创建或者更新资源时 都可以用 apply 命令。 # 创建资源 kubectl apply -f xxx.ymal kubectl create -f xxx.ymal # 更新资源 kubectl apply -f xxx.ymal kubectl update -f xxx.ymal kubectl patch -f xxx.ymal ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:5:1","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"滚动更新 想象这么一个场景，如果使用 configmap 或者 secret 当作 pod 的环境变量，那么当 configmap 或者 secret 更新了应该如何更新 对应的pod 呢？ pod 应该都会通过 deployment 或者 statefulset 来环境， 换言之该如何更新 deployment 或者 statefulset 呢？默认情况下 configmap 或者 secret 的更新是不会触发 deployment 或者 statefulset 的更新，一种可行的方法为: 更新 annotations 中一个无关的字段: kubectl -n $namespace patch deployment $deploymentName -p \\ \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"test_date\\\":\\\"`date +'%s'`\\\"}}}}}\" kubectl -n monitor patch deployment prometheus -p \\ \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"test_date\\\":\\\"`date +'%s'`\\\"}}}}}\" ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:5:2","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"总结 这篇文章介绍了 kubectl 的基本用法，常见场景中的一些操作，如果有其他场景可以通过 kubectl --help 和 kubectl command --help 查看帮助文档。 如有不正确之处欢迎指正。 ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:6:0","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"参考 https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/ https://mp.weixin.qq.com/s/OxYbLmTKXn5jrgStIQ6ohQ ","date":"2020-11-20","objectID":"/kubernetes/kubectl-command/:7:0","tags":["kubernetes","kubectl"],"title":"kubectl 常用命令","uri":"/kubernetes/kubectl-command/"},{"categories":null,"content":"kubectl 常用命令指南","date":"2020-11-20","objectID":"/kubectl-command/","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"导读 kubectl 应该是每个接触 kubernetes 的人都会接触的一个组件，它带给我们强大的命令行体验，本篇文章就是介绍 kubectl 中的一些常用命令，在结合一些具体的使用场景说说如何利用 kubectl 实现。好记性不如烂笔头，在这里尽可能全的罗列，方便后续 用的时候查找。如果能帮到您就收藏起来吧(😄)。 本次实验环境是 kubernetes-1.16.9，本篇文档的写作思路是按照平时 的使用场景进行写作，不会详细介绍 kubectl 的命令，kubectl 详细的帮助文档参考 kubectl --help or kubectl command --help 。 ","date":"2020-11-20","objectID":"/kubectl-command/:1:0","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"kubectl 支持的命令 kubectl --help kubectl controls the Kubernetes cluster manager. Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/ Basic Commands (Beginner): create Create a resource from a file or from stdin. expose 使用 replication controller, service, deployment 或者 pod 并暴露它作为一个 新的 Kubernetes Service run 在集群中运行一个指定的镜像 set 为 objects 设置一个指定的特征 Basic Commands (Intermediate): explain 查看资源的文档 get 显示一个或更多 resources edit 在服务器上编辑一个资源 delete Delete resources by filenames, stdin, resources and names, or by resources and label selector Deploy Commands: rollout Manage the rollout of a resource scale 为 Deployment, ReplicaSet, Replication Controller 或者 Job 设置一个新的副本数量 autoscale 自动调整一个 Deployment, ReplicaSet, 或者 ReplicationController 的副本数量 Cluster Management Commands: certificate 修改 certificate 资源. cluster-info 显示集群信息 top Display Resource (CPU/Memory/Storage) usage. cordon 标记 node 为 unschedulable uncordon 标记 node 为 schedulable drain Drain node in preparation for maintenance taint 更新一个或者多个 node 上的 taints Troubleshooting and Debugging Commands: describe 显示一个指定 resource 或者 group 的 resources 详情 logs 输出容器在 pod 中的日志 attach Attach 到一个运行中的 container exec 在一个 container 中执行一个命令 port-forward Forward one or more local ports to a pod proxy 运行一个 proxy 到 Kubernetes API server cp 复制 files 和 directories 到 containers 和从容器中复制 files 和 directories. auth Inspect authorization Advanced Commands: diff Diff live version against would-be applied version apply 通过文件名或标准输入流(stdin)对资源进行配置 patch 使用 strategic merge patch 更新一个资源的 field(s) replace 通过 filename 或者 stdin替换一个资源 wait Experimental: Wait for a specific condition on one or many resources. convert 在不同的 API versions 转换配置文件 kustomize Build a kustomization target from a directory or a remote url. Settings Commands: label 更新在这个资源上的 labels annotate 更新一个资源的注解 completion Output shell completion code for the specified shell (bash or zsh) Other Commands: api-resources Print the supported API resources on the server api-versions Print the supported API versions on the server, in the form of \"group/version\" config 修改 kubeconfig 文件 plugin Provides utilities for interacting with plugins. version 输出 client 和 server 的版本信息 Usage: kubectl [flags] [options] Use \"kubectl \u003ccommand\u003e --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). 从上面的帮助文档可以看出， kubectl 基本格式为 kubectl verb resource options , kubectl 后跟谓语动词， 再跟要操作的资源，可以加 options ，如： 要看 monitoring namespace 下面有哪些pod : kubectl -n monitoring get po ","date":"2020-11-20","objectID":"/kubectl-command/:2:0","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"pod pod 场景下，可能会有如下需求: ","date":"2020-11-20","objectID":"/kubectl-command/:3:0","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看某个 namespace 下，所有的pod # 先查看有哪些namespace kubectl get namespace # 查看 pod kubectl -n $namespace get po # 或者 kubectl get po -n $namespace 上面两种写法在达到的效果上是一样的，但是有一个细节可以注意一下，如果 kubectl 环境有命令自动补全的话，资源对象又比较多 的情况下，第一种写法将会有极大的优势，可以思考这么个场景，如：要查看 monitoring namespace 下的某个pod 详情, 就可以通过: kubectl -n monitoring get po 加 tab 键，列出这个namespace 下的所有 pod 供筛选。 centos 下命令自动补全需要安装 bash-completion ，方法为 yum install -y bash-completion 如果不加 -n $namespace ，则默认是 default namespace ","date":"2020-11-20","objectID":"/kubectl-command/:3:1","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看所有namespace 的pod kubectl get po --all-namespaces # or kubectl get po -A ","date":"2020-11-20","objectID":"/kubectl-command/:3:2","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看某个具体的 pod 信息 ，以 wide、json、yaml 的格式输出 kubectl -n $namespace get po xxx -o wide/json/yaml # 如 查看 monitoring 下的 prometheus-0 pod 信息，并以yaml 形式输出。 kubectl -n monitoring get po prometheus-0 -o yaml ","date":"2020-11-20","objectID":"/kubectl-command/:3:3","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"通过 go-template 进行查看 kubectl get pods -l app=hostnames \\ -o go-template='{{range .items}}{{.status.podIP}}{{\"\\n\"}}{{end}}' ","date":"2020-11-20","objectID":"/kubectl-command/:3:4","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看某个 pod 的某个字段信息 如果我们只想知道 pod 的 hostIP 或者其他的 一些字段， 可以通过 -o jsonpath or -o template or -o go-template 其中template 语法遵循 golang template 需要对 pod 的对象模型有一定的了解，如果不了解，可以 -o yaml or -o json 直接查看。 查看 hostIP 的方法如下: # -o jsonpath kubectl -n monitoring get po prometheus-k8s-0 -o jsonpath=\"{.status.hostIP}\" # -o template kubectl -n monitoring get po prometheus-k8s-0 -o template --template=\"{{.status.hostIP}}\" # -o go-template kubectl -n monitoring get po prometheus-k8s-0 -o go-template=\"{{.status.hostIP}}\" 如果需要查看其他的字段照猫画虎即可。 ","date":"2020-11-20","objectID":"/kubectl-command/:3:5","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"通过标签选择查看 pod 通过 -l key1=value1,key2=value2 进行选择，如 kubectl -n monitoring get po -l app=prometheus kubectl -n monitoring get po -l app=prometheus,prometheus=k8s ","date":"2020-11-20","objectID":"/kubectl-command/:3:6","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看某个node 上部署的所有 pod #先获取集群内所有的node kubectl get node -o wide # 假设其中一个 node 的名称为 node-0001 kubectl get po -A -o wide | grep node-0001 通过 kubectl get po -A -o wide | grep 可以做很多事情，具体可以根据情况而定，比如查看所有状态异常的 pod （非 Running） kubectl get po -A -o wide | grep -v Running ","date":"2020-11-20","objectID":"/kubectl-command/:3:7","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看 pod 的详细信息 kubectl -n monitoring describe po prometheus-k8s-0 这个命令在查看 pod 的基本信息和问题定位时特别有用，当 pod 异常，可以查看 Events 或许就能发现问题所在。 ","date":"2020-11-20","objectID":"/kubectl-command/:3:8","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看 pod log kubectl -n $namespace logs -f $podName $containerName # 其中 $namespace，$podName，$containerName 替换成真实值即可，当 pod 中只有一个 容器时可省略 $containerName，如： kubectl -n monitoring logs -f prometheus-k8s-0 prometheus ","date":"2020-11-20","objectID":"/kubectl-command/:3:9","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"进入容器 kubectl -n $namespace exec -it $podName -c $containerName sh # 其中 $namespace，$podName，$containerName 替换成真实值即可，当 pod 中只有一个 容器时可省略 -c $containerName，如： kubectl -n monitoring exec -it prometheus-k8s-0 -c prometheus sh ","date":"2020-11-20","objectID":"/kubectl-command/:3:10","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看 pod 的资源使用情况 kubectl -n $namespace top pod # 其中 $namespace 替换成真实值即可，如： kubectl -n monitoring top pod ","date":"2020-11-20","objectID":"/kubectl-command/:3:11","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"删除 pod kubectl -n $namespace delete po $podName kubectl -n monitoring delete po prometheus-k8s-0 # 在某些异常情况下删除 pod 会卡住，删不掉，需要强制才能删除 ，强制删除需要增加 --grace-period=0 --force ， kubectl -n monitoring delete po prometheus-k8s-0 --grace-period=0 --force 原理如下， 默认执行 delete po 时，kubectl 会增加–grace-period=30 参数，表示预留30秒的时间给 pod 处理当前的请求， 但同时也不接收新的请求了，以一种相对优雅的方式停止容器，注意这个参数在创建 pod 时可以指定，默认是30秒。强制删除时需要把–grace-period 设置为0，表示不等待马上删除，否则强制删除就会失效。 ","date":"2020-11-20","objectID":"/kubectl-command/:3:12","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"pod 标签管理 pod 的大多数的情况都会由 deployment or statefulset 来管理，所以标签也会通过它们管理，实际情况下很少会通过 kubectl 对 pod label 做增删改，如有需要可参考 下面 node 的用法，只需要把资源对象换成 pod 即可。 ","date":"2020-11-20","objectID":"/kubectl-command/:3:13","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"文件 copy 从 pod 中 copy 文件或者 copy 到 pod 中去。 容器中需要有 tar 命令，否则会失败 # 从本地 copy 到 pod kubectl cp /tmp/foo_dir \u003csome-pod\u003e:/tmp/bar_dir kubectl -n monitoring cp abc.txt prometheus-k8s-0:/tmp/abc.txt # 如果 pod 中有多个 container 可以用 -c 指定 container kubectl cp /tmp/foo \u003csome-pod\u003e:/tmp/bar -c \u003cspecific-container\u003e kubectl -n monitoring cp abc.txt prometheus-k8s-0:/tmp/abc.txt -c prometheus # 从 pod copy 到 本地 kubectl cp \u003csome-pod\u003e:/tmp/foo /tmp/bar kubectl -n monitoring cp prometheus-k8s-0:/tmp/abc.txt /tmp/abd.txt ","date":"2020-11-20","objectID":"/kubectl-command/:3:14","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"node 在 pod 一节 已经了解了 kubectl get ,kubectl describe , 等相关的用法，node 的操作和 pod 类似，只是后面接的资源对象不同。 ","date":"2020-11-20","objectID":"/kubectl-command/:4:0","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看有哪些node以及其基本信息 kubectl get node -o wide ","date":"2020-11-20","objectID":"/kubectl-command/:4:1","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看 node 上的详细情况 # 查看所有 node 的详细信息 kubectl describe node # 也可以查看某个 node 的信息 kubectl describe node node-0001 ... 这个命令在定位 node 的问题很有用，会输出如下信息: Labels Annotations Non-terminated Pods (正在运行的 pod) Allocated resources (已经分配的资源) … ","date":"2020-11-20","objectID":"/kubectl-command/:4:2","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"查看 node 的资源使用情况 kubectl top node ","date":"2020-11-20","objectID":"/kubectl-command/:4:3","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"node 的标签管理 增加标签 kubectl label node $nodename key1=value1 key2=value2 # 如 kubectl label node node-0001 a1=bbb a2=ccc 更新标签 # 在 增加标签的基础 加 --overwrite 参数 kubectl label node node-0001 a1=bbb --overwrite # 当标签不存在也可以 加 --overwrite 参数 kubectl label node node-0001 a10=bbb --overwrite 删除标签 kubectl label node $nodename key1- key2- kubectl label node node-0001 a10- a3- ","date":"2020-11-20","objectID":"/kubectl-command/:4:4","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"将一个 node 标记为不可调度/可调度 在调试过程中或者当其中的某些 node 出现问题时，需要将 node 标记为不可调度，等恢复回来再标记回来。 # 将一个 node 可以 标记为不可调度(unschedulable) ，如果只是看看效果，而不是真正标记可加 --dry-run 参数 kubectl cordon $nodeName kubectl cordon node-0001 kubectl cordon node-0001 --dry-run # 将一个 node 可以 标记为可调度(schedulable) ，如果只是看看效果，而不是真正标记可加 --dry-run 参数 kubectl uncordon $nodeName kubectl uncordon node-0001 kubectl uncordon node-0001 --dry-run ","date":"2020-11-20","objectID":"/kubectl-command/:4:5","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"排空 node 上的 pod # 排空node 上的所有 pod ，即使没有被 rc 管理，但是不会排空 被 daemonset 管理的 pod， 因为排空之后又会马上创建出来 kubectl drain foo --force ","date":"2020-11-20","objectID":"/kubectl-command/:4:6","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"node 上的污点（taint）管理 污点需要配合 pod 的亲和性使用，否则污点没有什么意义 # 增加/更新 taint kubectl taint nodes node-0001 dedicated=special-user:NoSchedule --overwrite # 删除 taint kubectl taint nodes foo dedicated:NoSchedule- kubectl taint nodes foo dedicated- 整体用法和 label 类似 ","date":"2020-11-20","objectID":"/kubectl-command/:4:7","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"node 的 annotate 管理 和 label 是类似的，只是把 verb 换成 annotate 即可 ","date":"2020-11-20","objectID":"/kubectl-command/:4:8","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"其他场景 上面通过 pod 和 node 的例子，穿插的介绍了大部分的 verb（如 get 、describe、top … ），这个小节再介绍其他的一些常用场景 ","date":"2020-11-20","objectID":"/kubectl-command/:5:0","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"apply 在准备好一个资源对象的 yaml 文件时可以用 kubectl apply -f xxx.ymal 使之生效，kubernetes 的api 中并没有 apply，api 中有的是 create 、update、patch 等，apply 是kubectl 自己封装实现的，先执行 get ，再判断是 create 还是 patch，所以用kubectl 创建或者更新资源时 都可以用 apply 命令。 # 创建资源 kubectl apply -f xxx.ymal kubectl create -f xxx.ymal # 更新资源 kubectl apply -f xxx.ymal kubectl update -f xxx.ymal kubectl patch -f xxx.ymal ","date":"2020-11-20","objectID":"/kubectl-command/:5:1","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"滚动更新 想象这么一个场景，如果使用 configmap 或者 secret 当作 pod 的环境变量，那么当 configmap 或者 secret 更新了应该如何更新 对应的pod 呢？ pod 应该都会通过 deployment 或者 statefulset 来环境， 换言之该如何更新 deployment 或者 statefulset 呢？默认情况下 configmap 或者 secret 的更新是不会触发 deployment 或者 statefulset 的更新，一种可行的方法为: 更新 annotations 中一个无关的字段: kubectl -n $namespace patch deployment $deploymentName -p \\ \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"test_date\\\":\\\"`date +'%s'`\\\"}}}}}\" kubectl -n monitor patch deployment prometheus -p \\ \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"test_date\\\":\\\"`date +'%s'`\\\"}}}}}\" ","date":"2020-11-20","objectID":"/kubectl-command/:5:2","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"总结 这篇文章介绍了 kubectl 的基本用法，常见场景中的一些操作，如果有其他场景可以通过 kubectl --help 和 kubectl command --help 查看帮助文档。 如有不正确之处欢迎指正。 ","date":"2020-11-20","objectID":"/kubectl-command/:6:0","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"参考 https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/ https://mp.weixin.qq.com/s/OxYbLmTKXn5jrgStIQ6ohQ ","date":"2020-11-20","objectID":"/kubectl-command/:7:0","tags":["kubernetes","kubectl","cli"],"title":"kubectl 常用命令","uri":"/kubectl-command/"},{"categories":null,"content":"markdown中插入表情的方法","date":"2020-11-19","objectID":"/markdownemoji/","tags":["markdown","emoji"],"title":"markdown中插入表情的方法","uri":"/markdownemoji/"},{"categories":null,"content":"导读 markdown 中也是可以插入表情，但并不是所有的markdown 解析器都支持，我本地用的 Goland、MacDown 不支持，但是 hugo 是支持的， 写作的过程可以适当加入一些表情，可以表达当时写作的一个心情。 ","date":"2020-11-19","objectID":"/markdownemoji/:1:0","tags":["markdown","emoji"],"title":"markdown中插入表情的方法","uri":"/markdownemoji/"},{"categories":null,"content":"使用场景 markdown 中支持 emoji 的地方有 : Campfire GitHub Basecamp Redbooth Trac Flowdock Sprint.ly Kandan Textbox.io Kippt Redmine JabbR Trello Hall Qiita Zendesk Ruby China Grove Idobata NodeBB Forums Slack Streamup OrganisedMinds Hackpad Cryptbin Kato Reportedly Cheerful Ghost IRCCloud Dashcube MyVideoGameList Subrosa Sococo Quip And Bang Bonusly Discourse Ello Twemoji Awesome Got Chosen Flow ReadMe.io esa DBook Groups.io TeamworkChat Damn Bugs Let’s Chat Buildkite ChatGrape Dokuwiki Usersnap Discord Status Hero Morfy Bitbucket Gitter Yellow YouTube Habitica and Mattermost ","date":"2020-11-19","objectID":"/markdownemoji/:2:0","tags":["markdown","emoji"],"title":"markdown中插入表情的方法","uri":"/markdownemoji/"},{"categories":null,"content":"用法 emoji 的表情大全参考 : https://www.webfx.com/tools/emoji-cheat-sheet/ 如果想要在 markdown 中使用，只要在相应的地方插入 :xxx: 即可， 其中 xxx 就是表情的名字 ，例: ## 插入表情示例 - 笑： 😄 - 傻笑 😏 - 害羞 😊 ... :/play secret: ","date":"2020-11-19","objectID":"/markdownemoji/:3:0","tags":["markdown","emoji"],"title":"markdown中插入表情的方法","uri":"/markdownemoji/"},{"categories":null,"content":"参考 https://www.webfx.com/tools/emoji-cheat-sheet/ ","date":"2020-11-19","objectID":"/markdownemoji/:4:0","tags":["markdown","emoji"],"title":"markdown中插入表情的方法","uri":"/markdownemoji/"},{"categories":null,"content":"python中的多线程与多进程（二）","date":"2020-11-18","objectID":"/concurrent/","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/concurrent/"},{"categories":null,"content":"导读 在上一篇“python中的多线程与多进程(一)中介绍了进程、线程的概念、基本用法和在 python 中使用遇到的一些坑， 这在一篇中会介绍一些高级的用法，当然更多的是遇到的坑，换言之这是一片避坑指南。 ","date":"2020-11-18","objectID":"/concurrent/:1:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/concurrent/"},{"categories":null,"content":"concurrent.futures 我们都知道在 python 中，多线程的标准库是使用 threading , 如 ： # -*- coding: UTF-8 -*- import threading import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) for index,value in enumerate([\"python\", \"java\", \"golang\", \"php\"]) : thread = threading.Thread(target=runner,args=(index, value, )) thread.start() 多进程的库是 multiprocessing ,如： # -*- coding: UTF-8 -*- from multiprocessing import Process import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) for index,value in enumerate([\"python\", \"java\", \"golang\", \"php\"]) : process = Process(target=runner,args=(index, value, )) process.start() 以上两个库已经 python2 已经支持，可以很好的实现我们多进程与多线程的需求。 python3.2 提供了 concurrent.futures 库，并且已经回溯到python2，这个库在 threading 与 multiprocessing 的基础上提供了一层封装，使得多线程和多进程在使用行为上保持了一致，为什么这么说呢，且看下面分析，请先看两段代码： 多线程 # -*- coding: UTF-8 -*- from concurrent.futures._base import TimeoutError from concurrent.futures import ThreadPoolExecutor import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) max_workers = 4 print(\"执行升级任务的并发数为为： {}\".format(max_workers)) runners = [\"python\", \"java\", \"golang\", \"php\", \"rust\", \"shell\", \"c\"] with ThreadPoolExecutor(max_workers=max_workers) as executor: for index, value in enumerate(runners): result = executor.submit(runner, index, value) try: result.result(timeout=3 * 60 ) except TimeoutError as err: print(\"任务超时,\", err) 多进程 # -*- coding: UTF-8 -*- from concurrent.futures._base import TimeoutError from concurrent.futures import ProcessPoolExecutor import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) max_workers = 4 print(\"执行升级任务的并发数为为： {}\".format(max_workers)) runners = [\"python\", \"java\", \"golang\", \"php\", \"rust\", \"shell\", \"c\"] with ProcessPoolExecutor(max_workers=max_workers) as executor: for index, value in enumerate(runners): result = executor.submit(runner, index, value) try: result.result(timeout=3 * 60 ) except TimeoutError as err: print(\"任务超时,\", err) 可以看到多进程和多线程写法超级类似，一个使用的是 ProcessPoolExecutor ，一个使用的是 ThreadPoolExecutor，其他代码基本一直，查看源码可以发现 concurrent.futures 定义了一个 Executor 抽象基类，提供了 submit 、map 、shutdown 等方法 class Executor(object): \"\"\"This is an abstract base class for concrete asynchronous executors.\"\"\" def submit(self, fn, *args, **kwargs): \"\"\"Submits a callable to be executed with the given arguments. Schedules the callable to be executed as fn(*args, **kwargs) and returns a Future instance representing the execution of the callable. Returns: A Future representing the given call. \"\"\" raise NotImplementedError() def map(self, fn, *iterables, **kwargs): \"\"\"Returns a iterator equivalent to map(fn, iter). Args: fn: A callable that will take as many arguments as there are passed iterables. timeout: The maximum number of seconds to wait. If None, then there is no limit on the wait time. Returns: An iterator equivalent to: map(func, *iterables) but the calls may be evaluated out-of-order. Raises: TimeoutError: If the entire result iterator could not be generated before the given timeout. Exception: If fn(*args) raises for any values. \"\"\" timeout = kwargs.get('timeout') if timeout is not None: end_time = timeout + time.time() fs = [self.submit(fn, *args) for args in itertools.izip(*iterables)] # Yield must be hidden in closure so that the futures are submitted # before the first iterator value is required. def result_iterator(): try: for future in fs: if timeout is None: yield future.result() else: yield future.result(end_time - time.time()) finally: for future in fs: future.cancel() return result_iterator() def shutdown(self, wait=True): \"\"","date":"2020-11-18","objectID":"/concurrent/:2:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/concurrent/"},{"categories":null,"content":"concurrent 使用过程中遇到的坑 执行环境为 python-2.7.15 假设有这么一个脚本 multipy.py # -*- coding: UTF-8 -*- from concurrent.futures._base import TimeoutError from concurrent.futures import ProcessPoolExecutor import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) def main(max_workers=1) : print(\"执行升级任务的并发数为为： {}\".format(max_workers)) runners = [\"python\", \"java\", \"golang\", \"php\", \"rust\", \"shell\", \"c\"] with ProcessPoolExecutor(max_workers=max_workers) as executor: for index, value in enumerate(runners): result = executor.submit(runner, index, value) try: result.result(timeout=3 * 60) except TimeoutError as err: print(\"任务超时,\", err) if __name__ == \"__main__\" : main(3) 通过在命令行执行 python multipy.py ，大家可以在心里想象一下会输出什么。 第二个场景是：同样的脚本， 通过 setuptools 安装后执行，部分代码（setup.py）如下: #!/usr/bin/env python from setuptools import setup, find_packages setup( name=\"pyctl\", entry_points=''' [console_scripts] pyctl=pyctl.commands.shell:cli ''', classifiers=[ ... ], install_requires=[ ... 'click==7.0' ], ) 安装完成之后可以在命令行通过 pyctl xxx ... 执行，和执行系统命令是一样的，如果不熟悉 setuptools 可以先了解一下，文档参考https://pypi.org/project/setuptools/ 言归正传，通过 setuptools 打包之后再执行这个脚本，我们可以假设打包之后的执行方式为 pyctl multipy ，执行后会发生什么呢？大家也可以在心里先想象一下。 实际的结果就是直接通过 python multipy.py 的方式可以得到正确的结果，确实按照多进程的方式并发执行，但是到第二个场景时却无法运行，通过 ps -ef 查看进程，确实创建了多个进程，但这些进程都被阻塞，没有执行 runner 函数里面的内容，程序会被卡死。当时百思不解其中的原因，尝试过很多方法，包括使用原生的 multiprocessing 自己实现进程管理也是同样的效果，最后是同样的代码，换到python3.8，两种方法都可以得到正确结果。python2.7 为啥会卡死，多个进程创建出来没有执行 runner 任务至今还没有找到原因，后续有进展再更新， 欢迎知道原因的小伙伴留言告知！！！ ","date":"2020-11-18","objectID":"/concurrent/:3:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/concurrent/"},{"categories":null,"content":"总结 在python2.7的环境下面，如果通过 setuptools 打包安装，安装后多进程使用会有问题，现象是会创建多个子进程出来，但是主进程和子进程都会被阻塞而无法真正执行runner任务，一个行之有效的方法是切换到python3（python3.8亲测没有问题，其他的没测过）。 ","date":"2020-11-18","objectID":"/concurrent/:4:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/concurrent/"},{"categories":null,"content":"python中的多线程与多进程（二）","date":"2020-11-18","objectID":"/python/concurrent/","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/python/concurrent/"},{"categories":null,"content":"导读 在上一篇“python中的多线程与多进程(一)中介绍了进程、线程的概念、基本用法和在 python 中使用遇到的一些坑， 这在一篇中会介绍一些高级的用法，当然更多的是遇到的坑，换言之这是一片避坑指南。 ","date":"2020-11-18","objectID":"/python/concurrent/:1:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/python/concurrent/"},{"categories":null,"content":"concurrent.futures 我们都知道在 python 中，多线程的标准库是使用 threading , 如 ： # -*- coding: UTF-8 -*- import threading import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) for index,value in enumerate([\"python\", \"java\", \"golang\", \"php\"]) : thread = threading.Thread(target=runner,args=(index, value, )) thread.start() 多进程的库是 multiprocessing ,如： # -*- coding: UTF-8 -*- from multiprocessing import Process import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) for index,value in enumerate([\"python\", \"java\", \"golang\", \"php\"]) : process = Process(target=runner,args=(index, value, )) process.start() 以上两个库已经 python2 已经支持，可以很好的实现我们多进程与多线程的需求。 python3.2 提供了 concurrent.futures 库，并且已经回溯到python2，这个库在 threading 与 multiprocessing 的基础上提供了一层封装，使得多线程和多进程在使用行为上保持了一致，为什么这么说呢，且看下面分析，请先看两段代码： 多线程 # -*- coding: UTF-8 -*- from concurrent.futures._base import TimeoutError from concurrent.futures import ThreadPoolExecutor import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) max_workers = 4 print(\"执行升级任务的并发数为为： {}\".format(max_workers)) runners = [\"python\", \"java\", \"golang\", \"php\", \"rust\", \"shell\", \"c\"] with ThreadPoolExecutor(max_workers=max_workers) as executor: for index, value in enumerate(runners): result = executor.submit(runner, index, value) try: result.result(timeout=3 * 60 ) except TimeoutError as err: print(\"任务超时,\", err) 多进程 # -*- coding: UTF-8 -*- from concurrent.futures._base import TimeoutError from concurrent.futures import ProcessPoolExecutor import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) max_workers = 4 print(\"执行升级任务的并发数为为： {}\".format(max_workers)) runners = [\"python\", \"java\", \"golang\", \"php\", \"rust\", \"shell\", \"c\"] with ProcessPoolExecutor(max_workers=max_workers) as executor: for index, value in enumerate(runners): result = executor.submit(runner, index, value) try: result.result(timeout=3 * 60 ) except TimeoutError as err: print(\"任务超时,\", err) 可以看到多进程和多线程写法超级类似，一个使用的是 ProcessPoolExecutor ，一个使用的是 ThreadPoolExecutor，其他代码基本一直，查看源码可以发现 concurrent.futures 定义了一个 Executor 抽象基类，提供了 submit 、map 、shutdown 等方法 class Executor(object): \"\"\"This is an abstract base class for concrete asynchronous executors.\"\"\" def submit(self, fn, *args, **kwargs): \"\"\"Submits a callable to be executed with the given arguments. Schedules the callable to be executed as fn(*args, **kwargs) and returns a Future instance representing the execution of the callable. Returns: A Future representing the given call. \"\"\" raise NotImplementedError() def map(self, fn, *iterables, **kwargs): \"\"\"Returns a iterator equivalent to map(fn, iter). Args: fn: A callable that will take as many arguments as there are passed iterables. timeout: The maximum number of seconds to wait. If None, then there is no limit on the wait time. Returns: An iterator equivalent to: map(func, *iterables) but the calls may be evaluated out-of-order. Raises: TimeoutError: If the entire result iterator could not be generated before the given timeout. Exception: If fn(*args) raises for any values. \"\"\" timeout = kwargs.get('timeout') if timeout is not None: end_time = timeout + time.time() fs = [self.submit(fn, *args) for args in itertools.izip(*iterables)] # Yield must be hidden in closure so that the futures are submitted # before the first iterator value is required. def result_iterator(): try: for future in fs: if timeout is None: yield future.result() else: yield future.result(end_time - time.time()) finally: for future in fs: future.cancel() return result_iterator() def shutdown(self, wait=True): \"\"","date":"2020-11-18","objectID":"/python/concurrent/:2:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/python/concurrent/"},{"categories":null,"content":"concurrent 使用过程中遇到的坑 执行环境为 python-2.7.15 假设有这么一个脚本 multipy.py # -*- coding: UTF-8 -*- from concurrent.futures._base import TimeoutError from concurrent.futures import ProcessPoolExecutor import time def runner(index, param) : print(\"线程{} 开始运行: ------------\".format(index)) print(\"线程{} : {}\".format(index,param)) time.sleep(3) print(\"线程{} 运行结束: ------------\".format(index)) def main(max_workers=1) : print(\"执行升级任务的并发数为为： {}\".format(max_workers)) runners = [\"python\", \"java\", \"golang\", \"php\", \"rust\", \"shell\", \"c\"] with ProcessPoolExecutor(max_workers=max_workers) as executor: for index, value in enumerate(runners): result = executor.submit(runner, index, value) try: result.result(timeout=3 * 60) except TimeoutError as err: print(\"任务超时,\", err) if __name__ == \"__main__\" : main(3) 通过在命令行执行 python multipy.py ，大家可以在心里想象一下会输出什么。 第二个场景是：同样的脚本， 通过 setuptools 安装后执行，部分代码（setup.py）如下: #!/usr/bin/env python from setuptools import setup, find_packages setup( name=\"pyctl\", entry_points=''' [console_scripts] pyctl=pyctl.commands.shell:cli ''', classifiers=[ ... ], install_requires=[ ... 'click==7.0' ], ) 安装完成之后可以在命令行通过 pyctl xxx ... 执行，和执行系统命令是一样的，如果不熟悉 setuptools 可以先了解一下，文档参考https://pypi.org/project/setuptools/ 言归正传，通过 setuptools 打包之后再执行这个脚本，我们可以假设打包之后的执行方式为 pyctl multipy ，执行后会发生什么呢？大家也可以在心里先想象一下。 实际的结果就是直接通过 python multipy.py 的方式可以得到正确的结果，确实按照多进程的方式并发执行，但是到第二个场景时却无法运行，通过 ps -ef 查看进程，确实创建了多个进程，但这些进程都被阻塞，没有执行 runner 函数里面的内容，程序会被卡死。当时百思不解其中的原因，尝试过很多方法，包括使用原生的 multiprocessing 自己实现进程管理也是同样的效果，最后是同样的代码，换到python3.8，两种方法都可以得到正确结果。python2.7 为啥会卡死，多个进程创建出来没有执行 runner 任务至今还没有找到原因，后续有进展再更新， 欢迎知道原因的小伙伴留言告知！！！ ","date":"2020-11-18","objectID":"/python/concurrent/:3:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/python/concurrent/"},{"categories":null,"content":"总结 在python2.7的环境下面，如果通过 setuptools 打包安装，安装后多进程使用会有问题，现象是会创建多个子进程出来，但是主进程和子进程都会被阻塞而无法真正执行runner任务，一个行之有效的方法是切换到python3（python3.8亲测没有问题，其他的没测过）。 ","date":"2020-11-18","objectID":"/python/concurrent/:4:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（二）","uri":"/python/concurrent/"},{"categories":null,"content":"python中的多线程与多进程（一）","date":"2020-11-16","objectID":"/multithread/","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/multithread/"},{"categories":null,"content":"导读 在编码的过程，多线程、多进程、并发、并行这些概念肯定不止一次的出现在我们面前。概念理解是一回事，但是能真正用好又是另一回事。不同的编程语言，并发编程难易程度相差还是很大的，正好这几天梳理了他们之间的关系与区别，分享给大家。（基于自己的理解谈谈，如果不对欢迎指出） 灵魂拷问：什么是线程？什么是进程？ ","date":"2020-11-16","objectID":"/multithread/:1:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/multithread/"},{"categories":null,"content":"进程 进程是资源分配的最小单位。 ","date":"2020-11-16","objectID":"/multithread/:2:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/multithread/"},{"categories":null,"content":"线程 线程是 cpu 调度的最小调度。线程又分为内核线程，用户线程。 内核线程只运行在内核态，不受用户态的拖累。 用户线程是完全建立在用户空间的线程库，用户线程的创建、调度、同步和销毁在用户空间完成，不需要内核的帮助。用户线程又称为协程。 一个线程只能属于一个进程，但是一个进程可以有多个线程，多线程处理就是一个进程可以有多个线程在同一个时刻执行多个任务。 这些是比较官方的定义，简单理解就是运行一段程序，需要一定的资源，如cpu，系统内核会分配给进程，至于怎么分配这些资源可由线程去抢，如果某个线程占用资源(cpu)时间太长，内核为了平衡，会强行中断，切换给其他的线程执行，但是每次切换都是有代价的，需要把执行现场保留以确保后续恢复的时候可以正常执行，这就有了内核和用户态的切换（进程和线程都是受内核控制的）。那么问题来了，如果只在用户态切换，岂不是很好？还真是这样，go 语言就是这样实现。 ","date":"2020-11-16","objectID":"/multithread/:3:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/multithread/"},{"categories":null,"content":"python 多线程时遇到的坑 python 中如果要用多线程或者多进程时需要自己创建线程或者进程，这和go语言不一样，go语言只需要通过 go 关键字创建出协程，然后由runtime 进行调度（不需要自己处理进程与线程）。今天先看看python 中如何使用多线程与多进程。python 环境为 2.7.15 。 先看一段简单的代码 import threading import time ​ lock = threading.Lock() ​ def runner(i, p1, p2, p3=\"\", p4=\"\", **kwargs): \"\"\" :return: \"\"\" count = 0 print(\"线程{} param1:====\".format(i), p1) print(\"线程{} param2:====\".format(i), p2) print(\"线程{} param3:====\".format(i), p3) print(\"线程{} param3:====\".format(i), p4) while True: with lock: count += 1 print(\"线程{} 第 {} 秒 后: ......\".format(i, count)) time.sleep(1) if count == 5: break def main_thread(): \"\"\" 主线程的运行代码 :return: \"\"\" print(\"主线程开始执行\") time.sleep(1) print(\"主线程执行结束\") ​ def main(): \"\"\" :return: \"\"\" for i in range(5): thread = threading.Thread(target=runner, args=(i, \"a1\", \"a2\"), kwargs={\"p3\": \"p2\", \"p4\": \"p4\"}) thread.setName(\"线程{}\".format(i)) thread.start() ​ main_thread() ​ if __name__ == \"__main__\": main() ​ 这个应该能想的出来不同的线程的输出是交叉打印出来的，说明这是多个线程并发执行的。 现在假设有这么一个场景，有10台机器，每个机器上有10个容器需要重启，如果要并发的执行应该怎么做呢? 看看下面的代码有没有问题? import os ​ import threading ​ def runnner(hostip) : appIds = [ \"app-{}\".format(i) for i in range(10)] cmd = \"docker restart {}\".format(\" \".join(appIds)) print(cmd) os.system(\"ssh {hostip} {cmd}\".format(hostip=hostip,cmd=cmd)) ​ hosts = [\"10.0.0.1\", \"10.0.0.2\", \"10.0.0.3\" , \"10.0.0.4\", \"10.0.0.5\",\"10.0.0.6\", \"10.0.0.7\", \"10.0.0.8\" , \"10.0.0.9\", \"10.0.0.10\"] ​ for hostip in hosts : thread = threading.Thread(target=runnner, args=(hostip,)) thread.start() 看起来应该是没有问题的，然而真正执行的还是串行，没有到达并发执行的目的。问题出在哪里呢？ 在python 中线程执行需要先获取GIL锁（全局解释器锁），看起来是创建了多个线程，但是同一个时间点每个进程只能有一个线程获取这个锁并且执行，是一种伪多线程，如果在密集计算的场景，就会频繁的发生线程切换，这个是很耗时间的，还没有单线程效果好。那么问题又来了，什么会发生线程切换呢? 如果遇到io等待或者sleep 时肯定会发生切换，还有就是每100条指令切换一次线程。可以通过如下指令设置: sys.setcheckinterval 上面这里例子就没有触发线程的切换，当前面的线程执行完退出之后释放GIL锁，后续的线程才能执行，所以才有看起来是多线程的写法，但是确实单线程的效果。 改进的方法当然使用多进程，每个进程都有自己的GIL锁，可以真正的实现并发。进程并不是越多越好，创建进程开销比线程要大很多，如果进程之间有数据交换也比线程复杂，并且真正执行还是要落到cpu上去执行，进程多了也会造成排队，理论上和创建和cpu相同个数的进程性能最好。下面看看多进程的写法。 import os from multiprocessing import Process ​ def runnner(hostip) : appIds = [ \"app-{}\".format(i) for i in range(10)] cmd = \"docker restart {}\".format(\" \".join(appIds)) print(cmd) os.system(\"ssh {hostip} {cmd}\".format(hostip=hostip,cmd=cmd)) ​ ​ hosts = [\"10.0.0.{}\".format(i) for i in range(10)] ​ ​ for hostip in hosts : process = Process(target=runnner, args=(hostip,)) process.start() 当然可以用进程池控制进程的个数，如: import os ​ from multiprocessing import Pool ​ def runnner(hostip) : appIds = [ \"app-{}\".format(i) for i in range(10)] cmd = \"docker restart {}\".format(\" \".join(appIds)) print(cmd) os.system(\"ssh {hostip} {cmd}\".format(hostip=hostip,cmd=cmd)) ​ ​ hosts = [\"10.0.0.{}\".format(i) for i in range(10)] ​ p = Pool(4) ​ for hostip in hosts : p.apply_async(runnner, args=(hostip,)) # 异步执行 p.close() p.join() ","date":"2020-11-16","objectID":"/multithread/:4:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/multithread/"},{"categories":null,"content":"参考 https://docs.python.org/2/library/sys.html#sys.setcheckinterval ","date":"2020-11-16","objectID":"/multithread/:5:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/multithread/"},{"categories":null,"content":"python中的多线程与多进程（一）","date":"2020-11-16","objectID":"/python/multithread/","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/python/multithread/"},{"categories":null,"content":"导读 在编码的过程，多线程、多进程、并发、并行这些概念肯定不止一次的出现在我们面前。概念理解是一回事，但是能真正用好又是另一回事。不同的编程语言，并发编程难易程度相差还是很大的，正好这几天梳理了他们之间的关系与区别，分享给大家。（基于自己的理解谈谈，如果不对欢迎指出） 灵魂拷问：什么是线程？什么是进程？ ","date":"2020-11-16","objectID":"/python/multithread/:1:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/python/multithread/"},{"categories":null,"content":"进程 进程是资源分配的最小单位。 ","date":"2020-11-16","objectID":"/python/multithread/:2:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/python/multithread/"},{"categories":null,"content":"线程 线程是 cpu 调度的最小调度。线程又分为内核线程，用户线程。 内核线程只运行在内核态，不受用户态的拖累。 用户线程是完全建立在用户空间的线程库，用户线程的创建、调度、同步和销毁在用户空间完成，不需要内核的帮助。用户线程又称为协程。 一个线程只能属于一个进程，但是一个进程可以有多个线程，多线程处理就是一个进程可以有多个线程在同一个时刻执行多个任务。 这些是比较官方的定义，简单理解就是运行一段程序，需要一定的资源，如cpu，系统内核会分配给进程，至于怎么分配这些资源可由线程去抢，如果某个线程占用资源(cpu)时间太长，内核为了平衡，会强行中断，切换给其他的线程执行，但是每次切换都是有代价的，需要把执行现场保留以确保后续恢复的时候可以正常执行，这就有了内核和用户态的切换（进程和线程都是受内核控制的）。那么问题来了，如果只在用户态切换，岂不是很好？还真是这样，go 语言就是这样实现。 ","date":"2020-11-16","objectID":"/python/multithread/:3:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/python/multithread/"},{"categories":null,"content":"python 多线程时遇到的坑 python 中如果要用多线程或者多进程时需要自己创建线程或者进程，这和go语言不一样，go语言只需要通过 go 关键字创建出协程，然后由runtime 进行调度（不需要自己处理进程与线程）。今天先看看python 中如何使用多线程与多进程。python 环境为 2.7.15 。 先看一段简单的代码 import threading import time ​ lock = threading.Lock() ​ def runner(i, p1, p2, p3=\"\", p4=\"\", **kwargs): \"\"\" :return: \"\"\" count = 0 print(\"线程{} param1:====\".format(i), p1) print(\"线程{} param2:====\".format(i), p2) print(\"线程{} param3:====\".format(i), p3) print(\"线程{} param3:====\".format(i), p4) while True: with lock: count += 1 print(\"线程{} 第 {} 秒 后: ......\".format(i, count)) time.sleep(1) if count == 5: break def main_thread(): \"\"\" 主线程的运行代码 :return: \"\"\" print(\"主线程开始执行\") time.sleep(1) print(\"主线程执行结束\") ​ def main(): \"\"\" :return: \"\"\" for i in range(5): thread = threading.Thread(target=runner, args=(i, \"a1\", \"a2\"), kwargs={\"p3\": \"p2\", \"p4\": \"p4\"}) thread.setName(\"线程{}\".format(i)) thread.start() ​ main_thread() ​ if __name__ == \"__main__\": main() ​ 这个应该能想的出来不同的线程的输出是交叉打印出来的，说明这是多个线程并发执行的。 现在假设有这么一个场景，有10台机器，每个机器上有10个容器需要重启，如果要并发的执行应该怎么做呢? 看看下面的代码有没有问题? import os ​ import threading ​ def runnner(hostip) : appIds = [ \"app-{}\".format(i) for i in range(10)] cmd = \"docker restart {}\".format(\" \".join(appIds)) print(cmd) os.system(\"ssh {hostip} {cmd}\".format(hostip=hostip,cmd=cmd)) ​ hosts = [\"10.0.0.1\", \"10.0.0.2\", \"10.0.0.3\" , \"10.0.0.4\", \"10.0.0.5\",\"10.0.0.6\", \"10.0.0.7\", \"10.0.0.8\" , \"10.0.0.9\", \"10.0.0.10\"] ​ for hostip in hosts : thread = threading.Thread(target=runnner, args=(hostip,)) thread.start() 看起来应该是没有问题的，然而真正执行的还是串行，没有到达并发执行的目的。问题出在哪里呢？ 在python 中线程执行需要先获取GIL锁（全局解释器锁），看起来是创建了多个线程，但是同一个时间点每个进程只能有一个线程获取这个锁并且执行，是一种伪多线程，如果在密集计算的场景，就会频繁的发生线程切换，这个是很耗时间的，还没有单线程效果好。那么问题又来了，什么会发生线程切换呢? 如果遇到io等待或者sleep 时肯定会发生切换，还有就是每100条指令切换一次线程。可以通过如下指令设置: sys.setcheckinterval 上面这里例子就没有触发线程的切换，当前面的线程执行完退出之后释放GIL锁，后续的线程才能执行，所以才有看起来是多线程的写法，但是确实单线程的效果。 改进的方法当然使用多进程，每个进程都有自己的GIL锁，可以真正的实现并发。进程并不是越多越好，创建进程开销比线程要大很多，如果进程之间有数据交换也比线程复杂，并且真正执行还是要落到cpu上去执行，进程多了也会造成排队，理论上和创建和cpu相同个数的进程性能最好。下面看看多进程的写法。 import os from multiprocessing import Process ​ def runnner(hostip) : appIds = [ \"app-{}\".format(i) for i in range(10)] cmd = \"docker restart {}\".format(\" \".join(appIds)) print(cmd) os.system(\"ssh {hostip} {cmd}\".format(hostip=hostip,cmd=cmd)) ​ ​ hosts = [\"10.0.0.{}\".format(i) for i in range(10)] ​ ​ for hostip in hosts : process = Process(target=runnner, args=(hostip,)) process.start() 当然可以用进程池控制进程的个数，如: import os ​ from multiprocessing import Pool ​ def runnner(hostip) : appIds = [ \"app-{}\".format(i) for i in range(10)] cmd = \"docker restart {}\".format(\" \".join(appIds)) print(cmd) os.system(\"ssh {hostip} {cmd}\".format(hostip=hostip,cmd=cmd)) ​ ​ hosts = [\"10.0.0.{}\".format(i) for i in range(10)] ​ p = Pool(4) ​ for hostip in hosts : p.apply_async(runnner, args=(hostip,)) # 异步执行 p.close() p.join() ","date":"2020-11-16","objectID":"/python/multithread/:4:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/python/multithread/"},{"categories":null,"content":"参考 https://docs.python.org/2/library/sys.html#sys.setcheckinterval https://mp.weixin.qq.com/s/nrT8iIe73POFTBpCdfXupA ","date":"2020-11-16","objectID":"/python/multithread/:5:0","tags":["python","多线程","多进程"],"title":"python中的多线程与多进程（一）","uri":"/python/multithread/"},{"categories":null,"content":"投稿到 servicemesh 社区的文章","date":"2020-11-10","objectID":"/elk/","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"ELK 这篇文档是由我投稿的云原生社区的文章，节选自 istio-handbook，如果有兴趣可以参考这本书。 ELK 指的是由 Elasticsearch + Logstash + Kibana 组成的日志采集、存储、展示为一体的日志解决方案，简称 “ELK Stack”。ELK Stack 还包含 Beats（如Filebeat、Metricbeat、Heartbeat等）、Kafka等成员，是目前主流的一种日志解决方案。 Elasticsearch 是个开源分布式搜索引擎，提供搜集、分析、存储数据三大功能。 Logstash 是免费且开放的服务器端数据处理管道，能够从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。Logstash 比较耗资源，在实践中我们一般用作实时解析和转换数据。Logstash 采用可插拔框架，拥有 200 多个插件。您可以将不同的输入选择、过滤器和输出选择混合搭配、精心安排，让它们在管道中和谐地运行。 Kibana 是一个开源和免费的工具，Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助汇总、分析和搜索重要数据日志。 Kafka 是由 Apache 软件基金会开发的一个开源流处理平台，由 Scala 和 Java 编写。用来做缓冲，当日志量比较大的时候可以缓解后端 Elasticsearch 的压力。 Beats 是数据采集的得力工具。Beats家族成员包括如下： Filebeat：用于日志文件采集，内置了多种模块（Apache、Cisco ASA、Microsoft Azure、NGINX、MySQL 等等）。 Metricbeat： 用于指标采集。 Packetbeat：用于网络数据采集。 Winlogbeat：用于Windows 事件采集。 Auditbeat：用于审计日志采集。 Heartbeat：用于运行时间采集。 其中 Filebeat 被经常用来收集 Node 或者 Pod 中的日志。 Beats 用于收集客户端的日志，发送给缓存队列如Kafka，目的是为了解耦数据收集与解析入库的过程，同时提高了可扩展性，使日志系统有峰值处理能力，不会因为突发的访问压力造成日志系统奔溃。缓存队列可选的还有 Redis，由于 Redis 是内存型，很容易写满，生产环境建议用 kafka。Logstash 从 缓存队列中消费日志解析处理之后写到 Elasticsearch，通过 Kibana 展示给最终用户。 ","date":"2020-11-10","objectID":"/elk/:0:0","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"采集方案 Filebeat 有两种部署模式，一是通过 DaemonSet 方式部署，二是通过 Sidecar 方式部署，Filebeat 采集后发送到 Kafka ，再由 Logstash 从 Kafka 中消费写到 Elasticsearch。 ","date":"2020-11-10","objectID":"/elk/:1:0","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"DaemonSet 方式部署 开启 Envoy 的访问日志输出到 stdout ，以 DaemonSet 的方式在每一台集群节点部署 Filebeat ，并将日志目录挂载至 Filebeat Pod，实现对 Envoy 访问日志的采集。 ","date":"2020-11-10","objectID":"/elk/:1:1","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"Sidecar 方式部署 Filebeat 和 Envoy 部署在同一个 Pod 内，共享日志数据卷， Envoy 写，Filebeat 读，实现对 Envoy 访问日志的采集。 ","date":"2020-11-10","objectID":"/elk/:1:2","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"部署 ELK 有了以上的基础，我们开始部署 ELK Stack ","date":"2020-11-10","objectID":"/elk/:2:0","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"部署 Kafka 首先，创建一个新的 namespace 用于部署 ELK Stack： # Logging Namespace. All below are a part of this namespace. apiVersion: v1 kind: Namespace metadata: name: logging 接下来，部署 Kafka 服务。 Kafka 通过 Zookeeper 管理集群配置，所以在部署 Kafka 需要先部署 Zookeeper。 Zookeeper 是一个分布式的，开放源码的分布式应用程序协调服务。 Kafka 与 Zookeeper 都是有状态服务，部署时需要选择 StatefulSet 。 部署 Zookeeper Service apiVersion:v1kind:Servicemetadata:name:zookeeper-clusternamespace:loggingspec:selector:app:zookeeper-clusterports:- name:httpport:2181targetPort:2181type:ClusterIP Zookeeper 在集群内使用，供 Kafka 使用，创建类型为 ClusterIP 的 Service 。 Zookeeper 的默认端口是2181。 部署 Zookeeper ConfigMap apiVersion:v1kind:ConfigMapmetadata:name:zookeeper-confignamespace:loggingdata:ZOO_CONF_DIR:/confZOO_PORT:\"2181\" Zookeeper 配置文件中的 key 都可以 以 ZOO_ 加大写的方式设置到环境变量中，使之生效。 这里仅列举部分配置。 部署 Zookeeper StatefulSet apiVersion:apps/v1kind:StatefulSetmetadata:name:zookeepernamespace:loggingspec:serviceName:zookeeper-clusterreplicas:1updateStrategy:type:RollingUpdateselector:matchLabels:app:zookeeper-clustertemplate:metadata:labels:app:zookeeper-clusterannotations:sidecar.istio.io/inject:\"false\"spec:containers:- name:zookeeperresources:requests:cpu:10mmemory:100Milimits:memory:200Miimage:zookeeperimagePullPolicy:IfNotPresentenvFrom:- configMapRef:name:zookeeper-configreadinessProbe:tcpSocket:port:2181initialDelaySeconds:5periodSeconds:10livenessProbe:tcpSocket:port:2181initialDelaySeconds:15periodSeconds:20ports:- containerPort:2181name:zk-client sidecar.istio.io/inject=false 标识此服务无需 sidecar 注入。 部署 Kafka Service apiVersion:v1kind:Servicemetadata:name:bootstrap-kafkanamespace:loggingspec:clusterIP:Noneports:- port:9092selector:app:kafka---apiVersion:v1kind:Servicemetadata:name:kafka-clusternamespace:loggingspec:ports:- name:httptargetPort:9092port:9092selector:app:kafkatype:ClusterIP 部署两个 Service 。 bootstrap-kafka 为后续部署 Kafka Statefulset 使用。 kafka-cluster 为 Kafka 的访问入口，在生产中使用可以用其他的 Service 类型。 kafka 的默认端口是9092 部署 Kafka ConfigMap apiVersion:v1kind:ConfigMapmetadata:name:kafka-confignamespace:loggingdata:KAFKA_ADVERTISED_LISTENERS:\"PLAINTEXT://kafka-cluster:9092\"KAFKA_LISTENERS:\"PLAINTEXT://0.0.0.0:9092\"KAFKA_ZOOKEEPER_CONNECT:\"zookeeper-cluster:2181\"KAFKA_LOG_RETENTION_HOURS:\"48\"KAFKA_NUM_PARTITIONS:\"30\" Kafka 配置文件（server.properties）中的 key 都可以 以 KAFKA_ 加大写的方式设置到环境变量中，使之生效。 KAFKA_ADVERTISED_LISTENERS 为 Kafka 监听的服务地址。 KAFKA_ZOOKEEPER_CONNECT 为前面部署的 Zookeeper 的服务地址。 KAFKA_LOG_RETENTION_HOURS 为 Kafka 数据保留的时间，超过这个时间将会被清理，可以根据实际情况进行调整。 KAFKA_NUM_PARTITIONS 为创建 Kafka topic 时的默认分片数，设置大一些可以增加 Kafka 的吞吐量。 这里仅列举部分配置。 部署 Kafka StatefulSet apiVersion:apps/v1kind:StatefulSetmetadata:name:kafkanamespace:loggingspec:selector:matchLabels:app:kafkaserviceName:bootstrap-kafkareplicas:1template:metadata:labels:app:kafkaannotations:sidecar.istio.io/inject:\"false\"spec:containers:- name:kafka-brokerimage:russellgao/kafka:2.12-2.0.1ports:- name:insidecontainerPort:9092resources:requests:cpu:0.1memory:1024Milimits:memory:3069MireadinessProbe:tcpSocket:port:9092timeoutSeconds:1initialDelaySeconds:5periodSeconds:10livenessProbe:tcpSocket:port:9092timeoutSeconds:1initialDelaySeconds:15periodSeconds:20envFrom:- configMapRef:name:kafka-config kafka 对磁盘的 IO 要求较高，可以选择固态硬盘或者经过IO优化的磁盘，否则可能会成为日志系统的瓶颈。 请注意，本次实践没有把数据卷映射出来，在生产实践中使用 volumeClaimTemplates 来为 Pod 提供持久化存储。resources 可以根据实际情况调整。 ","date":"2020-11-10","objectID":"/elk/:2:1","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"部署 Logstash Logstash 是一个无状态服务，通过 Deployment 进行部署。 部署 Logstash ConfigMap apiVersion:v1kind:ConfigMapmetadata:name:logstash-confnamespace:loggingdata:logstash.conf:| input {http{host=\u003e\"0.0.0.0\"# default: 0.0.0.0port =\u003e 8080 # default:8080user=\u003e\"logstash\"password=\u003e\"aoDJ0JVgkfNPjarn\"response_headers=\u003e{\"Content-Type\"=\u003e\"text/plain\"\"Access-Control-Allow-Origin\"=\u003e\"*\"\"Access-Control-Allow-Methods\"=\u003e\"GET, POST, DELETE, PUT\"\"Access-Control-Allow-Headers\"=\u003e\"authorization, content-type\"\"Access-Control-Allow-Credentials\"=\u003etrue}}kafka{topics=\u003e\"istio\"bootstrap_servers=\u003e\"kafka-cluster:9092\"auto_offset_reset=\u003e\"earliest\"group_id=\u003e\"istio_kafka_gr\"consumer_threads=\u003e3codec=\u003e\"json\"}}filter{grok{match=\u003e{\"message\"=\u003e\"(?m)\\[%{TIMESTAMP_ISO8601:timestamp}\\] \"%{NOTSPACE:method}%{NOTSPACE:path}%{NOTSPACE:protocol}\" %{NUMBER:response_code:int} %{NOTSPACE:response_flags} \"%{NOTSPACE:istio_policy_status}\" \"%{NOTSPACE:upstream_transport_failure_reason}\" %{NUMBER:bytes_received:int} %{NUMBER:bytes_sent:int} %{NUMBER:duration:int} %{NUMBER:upstream_service_time:int} \"%{NOTSPACE:x_forwarded_for}\" \"%{NOTSPACE:user_agent}\" \"%{NOTSPACE:request_id}\" \"%{NOTSPACE:authority}\" \"%{NOTSPACE:upstream_host}\" %{NOTSPACE:upstream_cluster} %{NOTSPACE:upstream_local_address} %{NOTSPACE:downstream_local_address} %{NOTSPACE:downstream_remote_address} %{NOTSPACE:requested_server_name} %{NOTSPACE:route_name}\"}remove_field=\u003e[\"message\"]}date{match=\u003e[\"timestamp\",\"yyyy-MM-ddTHH:mm:ss.SSSZ\"]timezone=\u003e\"Asia/Shanghai\"}ruby{code=\u003e\"event.set('[@metadata][index_day]',(event.get('@timestamp').time.localtime + 8*60*60 ).strftime('%Y.%m.%d'))\"}}output{if\"_grokparsefailure\"notin[tags]{elasticsearch{user=\u003e\"elastic\"password=\u003e\"elastic\"hosts=\u003e[\"elasticsearch.com:9200\"]index=\u003e\"istio-%{[@metadata][index_day]}\"}}} Logstash 配置由3部分组成： input Logstash input 支持非常多的数据源，如 File、Elasticsearch、Beats、Redis、Kafka、Http等。 Http input 用于Logstash 的健康检查，也可通过 http 接口将日志直接发送到 Logstash，主要用于移动端的场景。 Kafka input 用于收集日志，一个input只能从一个 Topic 中读取数据，需要和后续的 Filebeat output 对应。 filter Logstash filter 支持非常多的插件，可以对数据进行解析、加工、转换，如 grok、date、ruby、json、drop等。 grok 用于对日志进行解析。 date 用于把 timestamp 转化成 elasticsearch 中的 @timestamp 字段，可以指定时区。 ruby 插件支持执行 ruby 代码，可以进行复杂逻辑的处理，此处的用法是 @timestamp 字段的时间加8小时，解决自动生成的索引时差问题。 output Logstash output 支持非常多的数据源，如 elasticsearch、cvs、jdbc 等。 此处是把 grok 解析成功的日志写到 elasticsearch 。 部署 Logstash Deployment apiVersion:apps/v1beta2kind:Deploymentmetadata:name:logstashnamespace:loggingspec:replicas:2selector:matchLabels:app:logstashtemplate:metadata:labels:app:logstashannotations:sidecar.istio.io/inject:\"false\"spec:volumes:- name:configconfigMap:name:logstash-confhostname:logstashcontainers:- name:logstashimage:logstash:7.2.0args:[\"-f\",\"/usr/share/logstash/pipeline/logstash.conf\",]imagePullPolicy:IfNotPresentvolumeMounts:- name:configmountPath:\"/usr/share/logstash/pipeline/logstash.conf\"readOnly:truesubPath:logstash.confresources:requests:cpu:0.5memory:1024Milimits:cpu:1.5memory:3072MireadinessProbe:tcpSocket:port:8080initialDelaySeconds:5periodSeconds:10livenessProbe:tcpSocket:port:8080initialDelaySeconds:15periodSeconds:20 Logstash 不需要对外发布服务，即不需要创建 Service，从 Kafka 中消费日志，处理完成之后写到 Elasticsearch 。 Logstash 只需要把配置文件挂载进去，无需挂载其他目录，排查错误时可通过 Logstash Console Log 进行查看。 部署 Logstash HorizontalPodAutoscaler apiVersion:autoscaling/v2beta1kind:HorizontalPodAutoscalermetadata:name:logstashnamespace:loggingspec:scaleTargetRef:apiVersion:apps/v1beta2kind:Deploymentname:logstashminReplicas:2maxReplicas:10metrics:- type:Resourceresource:name:cputargetAverageUtilization:80 Logstash 比较消费 CPU ，可以部署 HPA，可以根据日志量动态的扩所容。 Logstash 的压力对 CPU 比较敏感，可以只根据 CPU 这一个指标进行 HPA。 Logstash 的配置文件支持if/else条件判断，通过这种方式，一个 Logstash 集群可以支持比较多的日志格式。另外 Logstash 的 grok 语法相对复杂，可以使用 Kibana Dev Tools 工具进行调试，如下图： ","date":"2020-11-10","objectID":"/elk/:2:2","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"部署 Filebeat 这里仅给出 Filebeat DaemonSet 的部署过程。 部署 Filebeat ConfigMap apiVersion:v1kind:ConfigMapmetadata:name:filebeat-confnamespace:loggingdata:filebeat.yml:| filebeat:inputs:- paths:- /var/log- /var/lib/docker/containersignore_older:1hforce_close_files:true#强制filebeat在文件名改变时，关闭文件，会有丢失日志的风险close_older:1mfields_under_root:trueoutput:kafka:enabled:truehosts:[\"kafka-cluster:9092\"]topic:\"istio\"version:\"2.0.0\"partition.round_robin:reachable_only:falseworker:2max_retries:3bulk_max_size:2048timeout:30sbroker_timeout:10schannel_buffer_size:256keep_alive:60compression:gzipmax_message_bytes:1000000required_acks:1 input.paths 代表 Filebeat 监听的日志路径。 input.ignore_older 代表日志文件的修改时间超过这个之间，将会忽略，这个在 Filebeat 重启时很有效果，解决重复读取日志的问题。 out.kafka.hosts 和之前部署的 Kafka Service 对应。 out.kafka.topic 和之前部署的 Logstash ConfigMap 中的 input 对应。 部署 Filebeat DaemonSet apiVersion:apps/v1kind:DaemonSetmetadata:name:filebeatnamespace:logginglabels:app:filebeatspec:selector:matchLabels:app:filebeattemplate:metadata:labels:app:filebeatannotations:sidecar.istio.io/inject:\"false\"spec:containers:- name:filebeatimage:elastic/filebeat:7.2.0imagePullPolicy:IfNotPresentvolumeMounts:- name:configmountPath:\"/usr/share/filebeat/filebeat.yml\"readOnly:truesubPath:filebeat.yml- name:varlogmountPath:/var/log- name:varlibdockercontainersmountPath:/var/lib/docker/containersresources:requests:cpu:0.1memory:200Milimits:cpu:0.3memory:600Mivolumes:- name:varloghostPath:path:/var/log- name:varlibdockercontainershostPath:path:/var/lib/docker/containers- name:configconfigMap:name:filebeat-conf 这里声明了两个 hostPath 类型的数据卷，路径为日志存储的路径。 将宿主机的 /var/log 和 /var/lib/docker/containers 挂载到了 Filebeat Pod 内便于 Filebeat 收集日志。 Filebeat 不需要部署 Service 。 Filebeat 对资源消耗比较少，可忽略对 Node 的资源消耗。 ","date":"2020-11-10","objectID":"/elk/:2:3","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"小结 本节为大家介绍了 ELK 的原理和安装部署，以及如何收集日志。 ","date":"2020-11-10","objectID":"/elk/:3:0","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"参考 Beats Logstash Zookeeper ","date":"2020-11-10","objectID":"/elk/:4:0","tags":["kubernetes","istio"],"title":"istio中的ELK实践","uri":"/elk/"},{"categories":null,"content":"Hugo, the world's fastest framework for building websites","date":"2020-11-08","objectID":"/about/","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"简介 高维宗（russellgao），现就职于上海海鼎信息工程股份有限公司，担任运维开发经理。 ","date":"2020-11-08","objectID":"/about/:1:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"关注领域 专注于devops，aiops，golong，python，kubernetes，servicemesh，云原生，算法等领域，热衷于参与开源软件和开源社区。 ","date":"2020-11-08","objectID":"/about/:2:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"个人公众号 ","date":"2020-11-08","objectID":"/about/:3:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"投稿 如果有好的文章需要分享也可以投稿给作者哟！ ","date":"2020-11-08","objectID":"/about/:4:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"投稿指南 文章须为原创的技术文章 须包含作者的姓名，公司头衔和简要介绍 须通过在 github 提交 PR 的方式提供 ","date":"2020-11-08","objectID":"/about/:4:1","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"原创申明 本博客上的所有文档均为原创，在写入过程中不免参考其他人优秀的文章，一般都会在文末申明参考的文章，如有侵犯到您的权益请联系作者第一时间修改。 ","date":"2020-11-08","objectID":"/about/:5:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"转载说明 如需转载，请加注原文出处。 ","date":"2020-11-08","objectID":"/about/:6:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"随笔记录 https://github.com/russellgao/blog https://github.com/russellgao/blogs https://github.com/russellgao/algorithm ","date":"2020-11-08","objectID":"/about/:7:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"Hugo, the world's fastest framework for building websites","date":"2020-11-08","objectID":"/golang/defer/","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/golang/defer/"},{"categories":null,"content":"背景 在学习和使用 Go 的过程中发现，Go 在语言层面的设计有很多有趣的地方，所以准备用一个系列来细数这些有趣的地方。写这个系列一是为了加深自己的理解，二是愿意分享，分享 Go 中有趣的设计细节。每篇都会通过一个例子讲述一个细节，感兴趣的话可以关注一下哟！ ","date":"2020-11-08","objectID":"/golang/defer/:1:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/golang/defer/"},{"categories":null,"content":"Go 介绍 Go（又称 Golang）是 Google 的 Robert Griesemer，Rob Pike 及 Ken Thompson 开发的一种静态强类型、编译型语言。Go 语言语法与 C 相近，但功能上有：内存安全，GC（垃圾回收），结构形态及 CSP-style 并发计算。 Go 是由这3位大佬从2007年9月开始设计Go，2009年正式推出，到目前为止已经发了15个大版本，最新版为1.15.4。Go 现在广泛应用于云原生、中间件、还有各个业务平台，如 docker、kubernetes、etcd等都是Go语言编写。所以还是很有必要了解一下哟！ 下面简单说说Go的优缺点，俗话说：一万个人眼中有一万个哈姆雷特，所以优缺点都是相对而言，就谈谈自己使用过程中的感受，具体的优缺点会在后面的系列文章中一一提到，这里是抛砖引玉。 ","date":"2020-11-08","objectID":"/golang/defer/:2:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/golang/defer/"},{"categories":null,"content":"Go 优点 语言层面支持并发：一个 go 关键字即可实现并发，其他编程语言依赖于库实现并发，这是有本质的区别 高性能 编译完之后生成二进制文件，可免去环境依赖 defer 机制 内置runtime 内嵌C支持，Go里面也可以直接包含C代码，利用现有的丰富的C库 跨平台编译 。。。 ","date":"2020-11-08","objectID":"/golang/defer/:3:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/golang/defer/"},{"categories":null,"content":"Go 缺点 包管理 。。。 ","date":"2020-11-08","objectID":"/golang/defer/:4:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/golang/defer/"},{"categories":null,"content":"defer 说起 Go 语言的最强大的地方，不得不说 Go 的并发机制和调度原理，但是今天不讲这些高深的理论，先从简单的开始。先思考这么几个问题（可以用自己熟悉的语言思考如何解决）: 对于文件的打开关闭，网络连接的建立断开场景，当打开时候应该何时关闭? 当调用一个函数，希望在函数返回时修改它的值，该如何解决? 先看看defer 的官方定义 ： A “defer” statement invokes a function whose execution is deferred to the moment the surrounding function returns, either because the surrounding function executed a return statement, reached the end of its function body, or because the corresponding goroutine is panicking. 意思是说，当包裹defer 的函数返回时或者包裹defer的函数执行到末尾时或者所在的goroutine发生panic时才会执行。 换句话说就是当函数执行完之后或者发生异常时再执行defer语句，就是说在被调函数返回之后，赋值给调用函数之前，还有机会执行其他指令，是不是很神奇。先看一段python 代码 : def f(x,y) : z = x / y z += 1 return z ​ if __name__ == \"__main__\" : result = f(4 /2) 当调用函数f，f返回给z并且赋值给result，在这时间，是没有任何机会执行其他的函数代码的。再看一段go代码: package main func main() { result := f(4, 2) fmt.Println(result) } ​ func f(x, y int) (r int) { r = x / y r += 1 defer func() { r += 2 }() return } 当调用函数f，f返回之后，在赋值之前执行了r +=2 。现在回想一下之前的两个问题，如果有defer 机制，是不是可以很好的解决。如对于第一个问题，在defer 语句中处理文件的关闭，连接的释放等，而不用考虑一些异常情况。 那defer的实现原理是怎样的呢? defer 其实是调用runtime.deferproc 进行实现，在defer 出现的地方，插入了call runtime.deferproc，然后在函数返回之前的地方，插入指令call runtime.deferreturn。 普通函数返回时，汇编代码类似于: add xx SP return 如果包含了defer 语句，汇编代码类似于: call runtime.deferreturn， add xx SP return goroutine的控制结构中，有一张表记录defer，调用runtime.deferproc时会将需要defer的表达式记录在表中，而在调用runtime.deferreturn的时候，则会依次从defer表中出栈并执行。 defer 在使用过程中也存在一些坑，看几个例子: 例1: func f() (result int) { defer func() { result++ }() return 10 } 例2: func f() (result int) { t := 10 defer func() { t = t + 1 }() return t } 例3: func f() (result int) { defer func(result int) { result = result + 1 }(result) return 10 } 大家可以先心里默默算一下他们的结果 第一个是11，第二个是10，第三个是10。 defer表达式可能会在设置函数返回值之后，在返回到调用函数之前，修改返回值，使最终的函数返回值与你想象的不一致。其实使用defer时，用一个简单的转换规则改写一下，就不会迷糊了。改写规则是将return语句拆成两句写，return xxx会被改写成: 返回值 = xxx 调用defer函数 空的return 例1 会被改写成: func f() (result int) { result = 10 // return语句不是一条原子调用，return xxx其实是赋值＋ret指令 defer func() { result++ }() return // 空的return指令 } 所以返回值是11 例2 会被改写成: func f() (result int) { t := 10 result = t // 赋值指令 defer func() { t = t + 1 //defer被插入到赋值与返回之间执行，这个例子中返回值 result没被修改过 }() return // 空的return指令 } 所以返回值是10 例3 就留给大家自己改写一下啦，有兴趣可以私我沟通哟！ ","date":"2020-11-08","objectID":"/golang/defer/:5:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/golang/defer/"},{"categories":null,"content":"总结 这篇主要做了对Go语言的介绍和优缺点，分析了defer 的用法以及实现原理，最后用例子展示了使用过程中可能会存在的坑。下篇预告: Go 的调度模型，欢迎关注!!! 如果有理解不正确的地方，欢迎指出。 ","date":"2020-11-08","objectID":"/golang/defer/:6:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/golang/defer/"},{"categories":null,"content":"Hugo, the world's fastest framework for building websites","date":"2020-11-08","objectID":"/defer/","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/defer/"},{"categories":null,"content":"背景 在学习和使用 Go 的过程中发现，Go 在语言层面的设计有很多有趣的地方，所以准备用一个系列来细数这些有趣的地方。写这个系列一是为了加深自己的理解，二是愿意分享，分享 Go 中有趣的设计细节。每篇都会通过一个例子讲述一个细节，感兴趣的话可以关注一下哟！ ","date":"2020-11-08","objectID":"/defer/:1:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/defer/"},{"categories":null,"content":"Go 介绍 Go（又称 Golang）是 Google 的 Robert Griesemer，Rob Pike 及 Ken Thompson 开发的一种静态强类型、编译型语言。Go 语言语法与 C 相近，但功能上有：内存安全，GC（垃圾回收），结构形态及 CSP-style 并发计算。 Go 是由这3位大佬从2007年9月开始设计Go，2009年正式推出，到目前为止已经发了15个大版本，最新版为1.15.4。Go 现在广泛应用于云原生、中间件、还有各个业务平台，如 docker、kubernetes、etcd等都是Go语言编写。所以还是很有必要了解一下哟！ 下面简单说说Go的优缺点，俗话说：一万个人眼中有一万个哈姆雷特，所以优缺点都是相对而言，就谈谈自己使用过程中的感受，具体的优缺点会在后面的系列文章中一一提到，这里是抛砖引玉。 ","date":"2020-11-08","objectID":"/defer/:2:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/defer/"},{"categories":null,"content":"Go 优点 语言层面支持并发：一个 go 关键字即可实现并发，其他编程语言依赖于库实现并发，这是有本质的区别 高性能 编译完之后生成二进制文件，可免去环境依赖 defer 机制 内置runtime 内嵌C支持，Go里面也可以直接包含C代码，利用现有的丰富的C库 跨平台编译 。。。 ","date":"2020-11-08","objectID":"/defer/:3:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/defer/"},{"categories":null,"content":"Go 缺点 包管理 。。。 ","date":"2020-11-08","objectID":"/defer/:4:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/defer/"},{"categories":null,"content":"defer 说起 Go 语言的最强大的地方，不得不说 Go 的并发机制和调度原理，但是今天不讲这些高深的理论，先从简单的开始。先思考这么几个问题（可以用自己熟悉的语言思考如何解决）: 对于文件的打开关闭，网络连接的建立断开场景，当打开时候应该何时关闭? 当调用一个函数，希望在函数返回时修改它的值，该如何解决? 先看看defer 的官方定义 ： A “defer” statement invokes a function whose execution is deferred to the moment the surrounding function returns, either because the surrounding function executed a return statement, reached the end of its function body, or because the corresponding goroutine is panicking. 意思是说，当包裹defer 的函数返回时或者包裹defer的函数执行到末尾时或者所在的goroutine发生panic时才会执行。 换句话说就是当函数执行完之后或者发生异常时再执行defer语句，就是说在被调函数返回之后，赋值给调用函数之前，还有机会执行其他指令，是不是很神奇。先看一段python 代码 : def f(x,y) : z = x / y z += 1 return z ​ if __name__ == \"__main__\" : result = f(4 /2) 当调用函数f，f返回给z并且赋值给result，在这时间，是没有任何机会执行其他的函数代码的。再看一段go代码: package main func main() { result := f(4, 2) fmt.Println(result) } ​ func f(x, y int) (r int) { r = x / y r += 1 defer func() { r += 2 }() return } 当调用函数f，f返回之后，在赋值之前执行了r +=2 。现在回想一下之前的两个问题，如果有defer 机制，是不是可以很好的解决。如对于第一个问题，在defer 语句中处理文件的关闭，连接的释放等，而不用考虑一些异常情况。 那defer的实现原理是怎样的呢? defer 其实是调用runtime.deferproc 进行实现，在defer 出现的地方，插入了call runtime.deferproc，然后在函数返回之前的地方，插入指令call runtime.deferreturn。 普通函数返回时，汇编代码类似于: add xx SP return 如果包含了defer 语句，汇编代码类似于: call runtime.deferreturn， add xx SP return goroutine的控制结构中，有一张表记录defer，调用runtime.deferproc时会将需要defer的表达式记录在表中，而在调用runtime.deferreturn的时候，则会依次从defer表中出栈并执行。 defer 在使用过程中也存在一些坑，看几个例子: 例1: func f() (result int) { defer func() { result++ }() return 10 } 例2: func f() (result int) { t := 10 defer func() { t = t + 1 }() return t } 例3: func f() (result int) { defer func(result int) { result = result + 1 }(result) return 10 } 大家可以先心里默默算一下他们的结果 第一个是11，第二个是10，第三个是10。 defer表达式可能会在设置函数返回值之后，在返回到调用函数之前，修改返回值，使最终的函数返回值与你想象的不一致。其实使用defer时，用一个简单的转换规则改写一下，就不会迷糊了。改写规则是将return语句拆成两句写，return xxx会被改写成: 返回值 = xxx 调用defer函数 空的return 例1 会被改写成: func f() (result int) { result = 10 // return语句不是一条原子调用，return xxx其实是赋值＋ret指令 defer func() { result++ }() return // 空的return指令 } 所以返回值是11 例2 会被改写成: func f() (result int) { t := 10 result = t // 赋值指令 defer func() { t = t + 1 //defer被插入到赋值与返回之间执行，这个例子中返回值 result没被修改过 }() return // 空的return指令 } 所以返回值是10 例3 就留给大家自己改写一下啦，有兴趣可以私我沟通哟！ ","date":"2020-11-08","objectID":"/defer/:5:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/defer/"},{"categories":null,"content":"总结 这篇主要做了对Go语言的介绍和优缺点，分析了defer 的用法以及实现原理，最后用例子展示了使用过程中可能会存在的坑。下篇预告: Go 的调度模型，欢迎关注!!! 如果有理解不正确的地方，欢迎指出。 ","date":"2020-11-08","objectID":"/defer/:6:0","tags":["golang","defer"],"title":"细谈 Golang 中那些设计优美的细节-defer","uri":"/defer/"},{"categories":null,"content":"自己开源的项目","date":"2020-11-08","objectID":"/opensrouce/toolkit/","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"toolkit ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:0:0","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"作用 用于提供工作效率的工具箱，里面有各种工具，就比如真实工具箱中里面有扳手，各种大小的起子，钳子等 某些场景下确实可以达到事半功倍的效果 ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:1:0","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"安装 ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:2:0","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"源码安装 有 go 语言环境的可以直接用源码进行编译运行 git clone https://github.com/russellgao/toolkit.git cd toolkit make ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:2:1","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"二进制 可以直接在release 页面进行下载对应的操作系统的二进制文件 https://github.com/russellgao/toolkit/releases/ ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:2:2","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"用法 ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:3:0","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"本机运行 可以通过如下命令进行 gwz:toolkit gaoweizong$ tkctl --help tkctl is a toolkit entrypoint,run `tkctl --help` get more information. Usage: tkctl [flags] tkctl [command] Available Commands: help Help about any command replace 文本替换，支持正则替换和非正则替换，类似与linux下的sed，但比sed更好用，而且可以跨平台使用 secret 生成随机密码，支持1～100位长度，可以指定是否包含特殊字符 version tkctl version Flags: -h, --help help for tkctl -v, --version show the version and exit Use \"tkctl [command] --help\" for more information about a command. tkctl 中的子命令会不断更新，某个具体的功能请查看Available Commands:下的帮助文档，如文本替换 tkctl replace --help 文本替换，支持正则替换和非正则替换，类似与linux下的sed，但比sed更好用，而且可以跨平台使用 Usage: tkctl replace [flags] Flags: -d, --dirs string 需要替换的目录, 默认为当前路径 (default \".\") -h, --help help for replace -m, --mode string 替换的模式，支持正则（regexp）和非正则（text）两种模式，默认非正则， (default \"text\") -p, --pattern string 需要替换的pattern [required] -r, --repl string 目标字符串 [required] ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:3:1","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"docker 如果本地有docker环境，也可以不用下载二进制的制品，可以通过docker 环境直接运行 docker run -it --rm russellgao/toolkit:latest tkctl --help # 如果有需要可以把目录挂载进去 docker run -it -v /data:/data --rm russellgao/toolkit:latest tkctl --help ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:3:2","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"适用范围 可以跨平台使用 mac windows linux ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:4:0","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"开发环境 go 1.14.2 ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:5:0","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"支持的功能 ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:6:0","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"1.0.0 文本正则替换 生成随机密码 ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:6:1","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"未来展望 期望可以成为一个完整的工具箱，可以解决日常工作中的繁杂事情。 ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:7:0","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"项目地址 https://github.com/russellgao/toolkit ","date":"2020-11-08","objectID":"/opensrouce/toolkit/:8:0","tags":["golang","toolkit"],"title":"自己开源的项目 - toolkit","uri":"/opensrouce/toolkit/"},{"categories":null,"content":"如何利用 python 操纵 oracle","date":"2020-07-09","objectID":"/oracle/","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/oracle/"},{"categories":null,"content":"安装库 pip3 install sqlalchemy pip3 install cx_Oracle ","date":"2020-07-09","objectID":"/oracle/:1:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/oracle/"},{"categories":null,"content":"安装客户端 oracle 客户端下载页面: https://www.oracle.com/database/technologies/instant-client/downloads.html ","date":"2020-07-09","objectID":"/oracle/:2:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/oracle/"},{"categories":null,"content":"mac https://www.oracle.com/database/technologies/instant-client/macos-intel-x86-downloads.html 在上面的页面下载之后执行: # 解压 cd ~ unzip instantclient-basic-macos.x64-19.3.0.0.0dbru.zip # 创建link mkdir ~/lib ln -s ~/instantclient_19_3/libclntsh.dylib ~/lib/ ","date":"2020-07-09","objectID":"/oracle/:2:1","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/oracle/"},{"categories":null,"content":"linux https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html ","date":"2020-07-09","objectID":"/oracle/:2:2","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/oracle/"},{"categories":null,"content":"windows https://www.oracle.com/database/technologies/instant-client/winx64-64-downloads.html ","date":"2020-07-09","objectID":"/oracle/:2:3","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/oracle/"},{"categories":null,"content":"使用 在上面装好库和oracle client 就可以用python 操作 oracle 了 简单用法参见 : from sqlalchemy import * # 连接oracle engine = create_engine('oracle://username:passwoed@xxxxx', encoding=\"utf8\",echo=True) connection = engine.connect() # table, 会根据表名自动生成Table 对象 meta = MetaData() t = Table(\"abcd\",meta,autoload=True,autoload_with=engine) # 获取列 columns = t.c print(columns) # 查询 # s = select([t]) # s = select([t]).where(t.c.name == \"xxxx\") s = select([t]).where(t.c.code == \"xxxx\") result = connection.execute(s) for row in result : print(row[t.c.gid],row[t.c.code],row[t.c.name],row[t.c.note]) result.close() print(\"end\") ","date":"2020-07-09","objectID":"/oracle/:3:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/oracle/"},{"categories":null,"content":"报错 如果报如下错误: sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) DPI-1047: Cannot locate a 64-bit Oracle Client library: \"dlopen(libclntsh.dylib, 1): image not found\". See https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html for help (Background on this error at: http://sqlalche.me/e/13/4xp6) 说明oracle的 client 没有正确安装 如果报错如下: sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) ORA-01017: invalid username/password; logon denied (Background on this error at: http://sqlalche.me/e/13/4xp6) 说明oracle 的用户密码不正确 ","date":"2020-07-09","objectID":"/oracle/:4:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/oracle/"},{"categories":null,"content":"参考 https://docs.sqlalchemy.org/en/13/dialects/oracle.html https://www.cnblogs.com/iupoint/p/10932069.html ","date":"2020-07-09","objectID":"/oracle/:5:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/oracle/"},{"categories":null,"content":"如何利用 python 操纵 oracle","date":"2020-07-09","objectID":"/python/oracle/","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/python/oracle/"},{"categories":null,"content":"安装库 pip3 install sqlalchemy pip3 install cx_Oracle ","date":"2020-07-09","objectID":"/python/oracle/:1:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/python/oracle/"},{"categories":null,"content":"安装客户端 oracle 客户端下载页面: https://www.oracle.com/database/technologies/instant-client/downloads.html ","date":"2020-07-09","objectID":"/python/oracle/:2:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/python/oracle/"},{"categories":null,"content":"mac https://www.oracle.com/database/technologies/instant-client/macos-intel-x86-downloads.html 在上面的页面下载之后执行: # 解压 cd ~ unzip instantclient-basic-macos.x64-19.3.0.0.0dbru.zip # 创建link mkdir ~/lib ln -s ~/instantclient_19_3/libclntsh.dylib ~/lib/ ","date":"2020-07-09","objectID":"/python/oracle/:2:1","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/python/oracle/"},{"categories":null,"content":"linux https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html ","date":"2020-07-09","objectID":"/python/oracle/:2:2","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/python/oracle/"},{"categories":null,"content":"windows https://www.oracle.com/database/technologies/instant-client/winx64-64-downloads.html ","date":"2020-07-09","objectID":"/python/oracle/:2:3","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/python/oracle/"},{"categories":null,"content":"使用 在上面装好库和oracle client 就可以用python 操作 oracle 了 简单用法参见 : from sqlalchemy import * # 连接oracle engine = create_engine('oracle://username:passwoed@xxxxx', encoding=\"utf8\",echo=True) connection = engine.connect() # table, 会根据表名自动生成Table 对象 meta = MetaData() t = Table(\"abcd\",meta,autoload=True,autoload_with=engine) # 获取列 columns = t.c print(columns) # 查询 # s = select([t]) # s = select([t]).where(t.c.name == \"xxxx\") s = select([t]).where(t.c.code == \"xxxx\") result = connection.execute(s) for row in result : print(row[t.c.gid],row[t.c.code],row[t.c.name],row[t.c.note]) result.close() print(\"end\") ","date":"2020-07-09","objectID":"/python/oracle/:3:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/python/oracle/"},{"categories":null,"content":"报错 如果报如下错误: sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) DPI-1047: Cannot locate a 64-bit Oracle Client library: \"dlopen(libclntsh.dylib, 1): image not found\". See https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html for help (Background on this error at: http://sqlalche.me/e/13/4xp6) 说明oracle的 client 没有正确安装 如果报错如下: sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) ORA-01017: invalid username/password; logon denied (Background on this error at: http://sqlalche.me/e/13/4xp6) 说明oracle 的用户密码不正确 ","date":"2020-07-09","objectID":"/python/oracle/:4:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/python/oracle/"},{"categories":null,"content":"参考 https://docs.sqlalchemy.org/en/13/dialects/oracle.html https://www.cnblogs.com/iupoint/p/10932069.html ","date":"2020-07-09","objectID":"/python/oracle/:5:0","tags":["python","oracle","数据库"],"title":"如何利用 python 操纵 oracle","uri":"/python/oracle/"},{"categories":null,"content":"pod 配置文件说明","date":"2020-06-18","objectID":"/kubernetes/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/","tags":["kubernetes","pod"],"title":"pod 配置文件说明","uri":"/kubernetes/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/"},{"categories":null,"content":"Pod的定义文件 apiVersion:v1kind:Podmetadata:name:stringnamaspace:stringlabels:- name:stringannotations:- name:stringspec:containers:- name:string# 使用的镜像image:stringimagePullPolicy:[Always|Never|IfNotPresent]command:[string]args:[string]# 工作目录workingDir:stringvolumeMounts:- name:stringmountPath:stringreadOnly:booleanports:- name:stringcontainerPort:inthostPort:intprotocol:stringenv:- name:stringvalue:stringresources:limits:cpu:stringmemory:stringrequests:cpu:stringmemory:stringlivenessProbe:exec:command:[string]httpGet:path:stringport:inthost:stringscheme:stringhttpHeaders:- name:stringvalue:stringtcpSocket:port:int# 多久之后去检查initialDelaySeconds:number# 健康检查超时时间timeoutSeconds:number# 多长时间检查一次periodSeconds:number# 成功的阀值，检查几次成功才算成功successThreshold:0# 失败的阀值，检查几次失败才算失败failureThreshold:0securityContext:# 详细参见 pod_SecurityContext 章节# securityContext 可以配置pod 或者container 级别runAsUser:1000# 运行的用户runAsGroup:3000# 运行的用户组fsGroup:2000privileged:bool# 是否以privileged 权限运行，即这是这个进程拥有特权allowPrivilegeEscalation:bool# 控制一个进程是否能比其父进程获取更多的权限，如果一个容器以privileged权限运行或具有CAP_SYS_ADMIN权限，则AllowPrivilegeEscalation的值将总是truecapabilities:add:[\"NET_ADMIN\",\"SYS_TIME\",\"...\"]# 给某个特定的进程privileged权限，而不用给root用户所有的privileged权限terminationMessagePath:/dev/termination-log# 容器终止的日志文件terminationMessagePolicy:[File|FallbackToLogsOnError]# 默认为File, 容器终止消息输出到文件restartPolicy:[Always|Never|OnFailure]# 重启策略，默认为 AlwaysnodeSelector:object# 通过label 选取nodednsPolicy:ClusterFirst# pod 的 dns 策略 ,可以配置如下值# Default : 和宿主机的DNS完全一致# ClusterFirst: 把集群的DNS写入到Pod的DNS配置，但是如果设置了HostNetwork=true，就会强制设置为Default# ClusterFirstWithHostNet: 把集群的DNS写入到Pod的DNS配置，不管是否设置HostNetwork# None: 忽略所有的DNS配置，一般来说，设置了None之后会自己手动再设置dnsConfigenableServiceLinks:true# Kubernetes支持两种查找服务的主要模式: 环境变量和DNS, 如果不需要服务环境变量, 将 `enableServiceLinks` 标志设置为 `false` 来禁用此模式terminationGracePeriodSeconds:10# 发出删除pod指令后多久之后真正的删除podserviceAccountName:jenkins# pod 绑定的serviceAccountpriorityClassName:# 给pod 设置优先级，参考 : https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/schedulerName:default-scheduler# 如果不配置则使用kubernetes 默认的default-scheduler，如果这个不满足要求则可以自定义一个scheduler# https://kubernetes.io/zh/docs/tasks/administer-cluster/configure-multiple-schedulers/affinity:# 亲和性设置tolerations:- effect:NoExecutekey:node.kubernetes.io/not-readyoperator:ExiststolerationSeconds:300- effect:NoExecutekey:node.kubernetes.io/unreachableoperator:ExiststolerationSeconds:300# 容忍设置imagePullSecrets:- name:string# 镜像拉取策略hostNetwork:false# 是否使用主机网络，默认为false，如果为true，pod直接用主机网络，在pod中可以看到主机的网络接口volumes:- name:stringemptyDir:{}hostPath:path:stringsecret:secretName:stringitems:- key:stringpath:stringconfigMap:name:stringitems:- key:stringpath:string# 目录挂载 ","date":"2020-06-18","objectID":"/kubernetes/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/:1:0","tags":["kubernetes","pod"],"title":"pod 配置文件说明","uri":"/kubernetes/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/"},{"categories":null,"content":"pod 具体的样例 apiVersion:v1kind:Podmetadata:labels:app:elastic-clustername:enode-0spec:containers:- env:- name:ES_JAVA_OPTSvalueFrom:configMapKeyRef:key:ES_JAVA_OPTSname:es-configimage:elasticsearch:6.7.2imagePullPolicy:IfNotPresentlivenessProbe:failureThreshold:3httpGet:path:/_cluster/health?local=trueport:9200scheme:HTTPperiodSeconds:600successThreshold:1timeoutSeconds:1name:elasticsearchports:- containerPort:9200name:es-httpprotocol:TCP- containerPort:9300name:es-transportprotocol:TCPreadinessProbe:failureThreshold:3httpGet:path:/_cluster/health?local=trueport:9200scheme:HTTPinitialDelaySeconds:30periodSeconds:20successThreshold:1timeoutSeconds:1resources:limits:cpu:\"2\"memory:10Girequests:cpu:\"1\"memory:8GisecurityContext:capabilities:add:- IPC_LOCK- SYS_RESOURCEprivileged:truerunAsUser:1000terminationMessagePath:/dev/termination-logterminationMessagePolicy:FilevolumeMounts:- mountPath:/usr/share/elasticsearch/dataname:es-data- mountPath:/usr/share/elasticsearch/logsname:es-logs- mountPath:/usr/share/elasticsearch/config/elasticsearch.ymlname:elasticsearch-configsubPath:elasticsearch.yml- mountPath:/var/run/secrets/kubernetes.io/serviceaccountname:default-token-k4r6freadOnly:truednsPolicy:ClusterFirstenableServiceLinks:truehostname:enode-0initContainers:- command:- sysctl- -w- vm.max_map_count=262144image:busyboximagePullPolicy:IfNotPresentname:init-sysctlresources:{}securityContext:privileged:trueterminationMessagePath:/dev/termination-logterminationMessagePolicy:FilevolumeMounts:- mountPath:/var/run/secrets/kubernetes.io/serviceaccountname:default-token-k4r6freadOnly:truepriority:0restartPolicy:AlwaysschedulerName:default-schedulersecurityContext:fsGroup:1000serviceAccount:defaultserviceAccountName:defaultsubdomain:elasticsearch-clusterterminationGracePeriodSeconds:30tolerations:- effect:NoExecutekey:node.kubernetes.io/not-readyoperator:ExiststolerationSeconds:300- effect:NoExecutekey:node.kubernetes.io/unreachableoperator:ExiststolerationSeconds:300volumes:- name:es-datapersistentVolumeClaim:claimName:es-data-enode-0- name:es-logspersistentVolumeClaim:claimName:es-logs-enode-0- configMap:defaultMode:420items:- key:elasticsearch.ymlpath:elasticsearch.ymlname:es-configname:elasticsearch-config- name:default-token-k4r6fsecret:defaultMode:420secretName:default-token-k4r6f ","date":"2020-06-18","objectID":"/kubernetes/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/:2:0","tags":["kubernetes","pod"],"title":"pod 配置文件说明","uri":"/kubernetes/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/"},{"categories":null,"content":"pod 配置文件说明","date":"2020-06-18","objectID":"/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/","tags":["kubernetes","pod"],"title":"pod 配置文件说明","uri":"/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/"},{"categories":null,"content":"Pod的定义文件 apiVersion:v1kind:Podmetadata:name:stringnamaspace:stringlabels:- name:stringannotations:- name:stringspec:containers:- name:string# 使用的镜像image:stringimagePullPolicy:[Always|Never|IfNotPresent]command:[string]args:[string]# 工作目录workingDir:stringvolumeMounts:- name:stringmountPath:stringreadOnly:booleanports:- name:stringcontainerPort:inthostPort:intprotocol:stringenv:- name:stringvalue:stringresources:limits:cpu:stringmemory:stringrequests:cpu:stringmemory:stringlivenessProbe:exec:command:[string]httpGet:path:stringport:inthost:stringscheme:stringhttpHeaders:- name:stringvalue:stringtcpSocket:port:int# 多久之后去检查initialDelaySeconds:number# 健康检查超时时间timeoutSeconds:number# 多长时间检查一次periodSeconds:number# 成功的阀值，检查几次成功才算成功successThreshold:0# 失败的阀值，检查几次失败才算失败failureThreshold:0securityContext:# 详细参见 pod_SecurityContext 章节# securityContext 可以配置pod 或者container 级别runAsUser:1000# 运行的用户runAsGroup:3000# 运行的用户组fsGroup:2000privileged:bool# 是否以privileged 权限运行，即这是这个进程拥有特权allowPrivilegeEscalation:bool# 控制一个进程是否能比其父进程获取更多的权限，如果一个容器以privileged权限运行或具有CAP_SYS_ADMIN权限，则AllowPrivilegeEscalation的值将总是truecapabilities:add:[\"NET_ADMIN\",\"SYS_TIME\",\"...\"]# 给某个特定的进程privileged权限，而不用给root用户所有的privileged权限terminationMessagePath:/dev/termination-log# 容器终止的日志文件terminationMessagePolicy:[File|FallbackToLogsOnError]# 默认为File, 容器终止消息输出到文件restartPolicy:[Always|Never|OnFailure]# 重启策略，默认为 AlwaysnodeSelector:object# 通过label 选取nodednsPolicy:ClusterFirst# pod 的 dns 策略 ,可以配置如下值# Default : 和宿主机的DNS完全一致# ClusterFirst: 把集群的DNS写入到Pod的DNS配置，但是如果设置了HostNetwork=true，就会强制设置为Default# ClusterFirstWithHostNet: 把集群的DNS写入到Pod的DNS配置，不管是否设置HostNetwork# None: 忽略所有的DNS配置，一般来说，设置了None之后会自己手动再设置dnsConfigenableServiceLinks:true# Kubernetes支持两种查找服务的主要模式: 环境变量和DNS, 如果不需要服务环境变量, 将 `enableServiceLinks` 标志设置为 `false` 来禁用此模式terminationGracePeriodSeconds:10# 发出删除pod指令后多久之后真正的删除podserviceAccountName:jenkins# pod 绑定的serviceAccountpriorityClassName:# 给pod 设置优先级，参考 : https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/schedulerName:default-scheduler# 如果不配置则使用kubernetes 默认的default-scheduler，如果这个不满足要求则可以自定义一个scheduler# https://kubernetes.io/zh/docs/tasks/administer-cluster/configure-multiple-schedulers/affinity:# 亲和性设置tolerations:- effect:NoExecutekey:node.kubernetes.io/not-readyoperator:ExiststolerationSeconds:300- effect:NoExecutekey:node.kubernetes.io/unreachableoperator:ExiststolerationSeconds:300# 容忍设置imagePullSecrets:- name:string# 镜像拉取策略hostNetwork:false# 是否使用主机网络，默认为false，如果为true，pod直接用主机网络，在pod中可以看到主机的网络接口volumes:- name:stringemptyDir:{}hostPath:path:stringsecret:secretName:stringitems:- key:stringpath:stringconfigMap:name:stringitems:- key:stringpath:string# 目录挂载 ","date":"2020-06-18","objectID":"/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/:1:0","tags":["kubernetes","pod"],"title":"pod 配置文件说明","uri":"/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/"},{"categories":null,"content":"pod 具体的样例 apiVersion:v1kind:Podmetadata:labels:app:elastic-clustername:enode-0spec:containers:- env:- name:ES_JAVA_OPTSvalueFrom:configMapKeyRef:key:ES_JAVA_OPTSname:es-configimage:elasticsearch:6.7.2imagePullPolicy:IfNotPresentlivenessProbe:failureThreshold:3httpGet:path:/_cluster/health?local=trueport:9200scheme:HTTPperiodSeconds:600successThreshold:1timeoutSeconds:1name:elasticsearchports:- containerPort:9200name:es-httpprotocol:TCP- containerPort:9300name:es-transportprotocol:TCPreadinessProbe:failureThreshold:3httpGet:path:/_cluster/health?local=trueport:9200scheme:HTTPinitialDelaySeconds:30periodSeconds:20successThreshold:1timeoutSeconds:1resources:limits:cpu:\"2\"memory:10Girequests:cpu:\"1\"memory:8GisecurityContext:capabilities:add:- IPC_LOCK- SYS_RESOURCEprivileged:truerunAsUser:1000terminationMessagePath:/dev/termination-logterminationMessagePolicy:FilevolumeMounts:- mountPath:/usr/share/elasticsearch/dataname:es-data- mountPath:/usr/share/elasticsearch/logsname:es-logs- mountPath:/usr/share/elasticsearch/config/elasticsearch.ymlname:elasticsearch-configsubPath:elasticsearch.yml- mountPath:/var/run/secrets/kubernetes.io/serviceaccountname:default-token-k4r6freadOnly:truednsPolicy:ClusterFirstenableServiceLinks:truehostname:enode-0initContainers:- command:- sysctl- -w- vm.max_map_count=262144image:busyboximagePullPolicy:IfNotPresentname:init-sysctlresources:{}securityContext:privileged:trueterminationMessagePath:/dev/termination-logterminationMessagePolicy:FilevolumeMounts:- mountPath:/var/run/secrets/kubernetes.io/serviceaccountname:default-token-k4r6freadOnly:truepriority:0restartPolicy:AlwaysschedulerName:default-schedulersecurityContext:fsGroup:1000serviceAccount:defaultserviceAccountName:defaultsubdomain:elasticsearch-clusterterminationGracePeriodSeconds:30tolerations:- effect:NoExecutekey:node.kubernetes.io/not-readyoperator:ExiststolerationSeconds:300- effect:NoExecutekey:node.kubernetes.io/unreachableoperator:ExiststolerationSeconds:300volumes:- name:es-datapersistentVolumeClaim:claimName:es-data-enode-0- name:es-logspersistentVolumeClaim:claimName:es-logs-enode-0- configMap:defaultMode:420items:- key:elasticsearch.ymlpath:elasticsearch.ymlname:es-configname:elasticsearch-config- name:default-token-k4r6fsecret:defaultMode:420secretName:default-token-k4r6f ","date":"2020-06-18","objectID":"/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/:2:0","tags":["kubernetes","pod"],"title":"pod 配置文件说明","uri":"/pod%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E/"},{"categories":null,"content":"位运算合集","date":"2020-05-30","objectID":"/argorithm/bit/","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位运算 计算机中的数在内存中都是以二进制形式进行存储的，用位运算就是直接对整数在内存中的二进制位进行操作，因此其执行效率非常高，在程序中尽量使用位运算进行操作，这会大大提高程序的性能。 ","date":"2020-05-30","objectID":"/argorithm/bit/:0:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位操作符 ","date":"2020-05-30","objectID":"/argorithm/bit/:1:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"\u0026 与运算 \u0026 与运算 两个位都是 1 时，结果才为 1，否则为 0，如 1 0 0 1 1 \u0026 1 1 0 0 1 ------------------------------ 1 0 0 0 1 ","date":"2020-05-30","objectID":"/argorithm/bit/:1:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"| 或运算 两个位都是 0 时，结果才为 0，否则为 1，如 1 0 0 1 1 | 1 1 0 0 1 ------------------------------ 1 1 0 1 1 ","date":"2020-05-30","objectID":"/argorithm/bit/:1:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"^ 异或运算 两个位相同则为 0，不同则为 1，如 1 0 0 1 1 ^ 1 1 0 0 1 ----------------------------- 0 1 0 1 0 ","date":"2020-05-30","objectID":"/argorithm/bit/:1:3","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"~ 取反运算 0 则变为 1，1 则变为 0，如 ~ 1 0 0 1 1 ----------------------------- 0 1 1 0 0 ","date":"2020-05-30","objectID":"/argorithm/bit/:1:4","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"« 左移运算 向左进行移位操作，高位丢弃，低位补 0,如 int a = 8; a \u003c\u003c 3; 移位前：0000 0000 0000 0000 0000 0000 0000 1000 移位后：0000 0000 0000 0000 0000 0000 0100 0000 左移n为的值即为当前值*2^n, 如: a = 8 b = a\u003c\u003c3 # 64 c = a * (2 ** 3) # 64 ","date":"2020-05-30","objectID":"/argorithm/bit/:1:5","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"»右移运算 向右进行移位操作，对无符号数，高位补 0，对于有符号数，高位补符号位，如 unsigned int a = 8; a \u003e\u003e 3; 移位前：0000 0000 0000 0000 0000 0000 0000 1000 移位后：0000 0000 0000 0000 0000 0000 0000 0001 ​ int a = -8; a \u003e\u003e 3; 移位前：1111 1111 1111 1111 1111 1111 1111 1000 移位前：1111 1111 1111 1111 1111 1111 1111 1111 ","date":"2020-05-30","objectID":"/argorithm/bit/:1:6","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"有符号数和无符号数 ","date":"2020-05-30","objectID":"/argorithm/bit/:2:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"有符号数 有符号数的定义是：字节的最高位作为符号位，其余的是数值位。例如一个字节中存储的二进制数为1100 1000，最高位1作为符号位，其余的7为 100 1000 作为数值为。 那么，符号位占据1位，就有0和1这样的两种数值，就有： 如果符号位为0，那么字节中存储的数值是正数 如果符号位为1，那么字节中存储的数值是负数 对于1100 1000这样的二进制数据，符号位是1，就表示负数。 在有符号数中，表示负数的算法是： 把数值位中存储的二进制数据，每个位都取反，就是原来为0的值变为1，原来为1的值变为0； 给对取反后的二进制数据加1，得到的数值就得到负数值； ","date":"2020-05-30","objectID":"/argorithm/bit/:2:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"无符号数 无符号数的定义是：没有符号位，所有的位数都是数值位。所以表示的都是正数。 ","date":"2020-05-30","objectID":"/argorithm/bit/:2:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"例子 例一 1100 1000这个数值，如果作为有符号数看待，那么符号位是1，数值位是100 1000。所以，符号位是1，所以，这个数据是负数。然后，表示成十进制时，对数值位的操作是： 数值位取反，得到011 0111； 对取反后的数值 011 0111加1得到011 1000，数值位的值为56； 那么，1100 1000这个二进制数据表示为“有符号数”时，就是-56这个数值。 如果作为无符号数看待，那么，就没有符号位，所有的位数都是数值位，所以11001000都作为数值位，表示的十进制数值是200 例二 例如，0111 0011这个数值，如果当做“有符号数”看待，那么，其符号位是0，所以，表示整数，数值位是115，所以，表示正115这个数值。如果当做无符号数看待，所有位都是数值位，计算得到115这个数值，所以，表示正115。所以我们可以总结 ","date":"2020-05-30","objectID":"/argorithm/bit/:2:3","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"总结 无符号数，总是表示正数。所有位数都表示数值位。 有符号数，可以表示正数和负数，最高位是符号位，其余位都是数值位。如果符号位是0，则表示正数；如果符号位是1，则表示负数。对于负数的表示方法是：数值位全部取反，再加1，得到的数值就是负数值。 ","date":"2020-05-30","objectID":"/argorithm/bit/:2:4","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"原码、反码、补码 ","date":"2020-05-30","objectID":"/argorithm/bit/:3:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"原码 原码的表示范围-127~-0, +0~+127, 共256个数字 正0的原码是0000 0000, 负0的原码是1000 0000, 有正0负0之分, 不符合人的习惯, 待解决. 原码有几个缺点，零分两种 +0 和 -0 。还有，在进行不同符号的加法运算或者同符号的减法运算的时候，不能直接判断出结果的正负。你需要将两个值的绝对值进行比较，然后进行加减操作 ，最后符号位由绝对值大的决定。于是反码就产生了。 ","date":"2020-05-30","objectID":"/argorithm/bit/:3:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"反码 除符号位, 原码其余位取反而得 +0：0000 0000，-0：1111 1111 仍然有正0负0之分。 正数的反码就是原码，负数的反码等于原码除符号位以外所有的位取反 举例说明： int类型的 3 的反码是 00000000 00000000 00000000 00000011 和原码一样没什么可说的 int类型的 -3 的反码是 11111111 11111111 11111111 11111100 除开符号位 所有位 取反 解决了加减运算的问题，但还是有正负零之分，然后就到补码了 ","date":"2020-05-30","objectID":"/argorithm/bit/:3:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"补码 在反码的基础上加1而得 对原码的两种0同时末位加1 +0：0000 0000，-0：0000 0000(因为溢出导致8位全0) 消除了正0负0之别, 如此一来, 便节省出一个数值表示方式1000 0000, 不能浪费, 用来表示-128, -128特殊之处在于没有相应的反码原码。也可以这样考虑: -1： 1111 1111 -2： 1111 1110（在-1的基础上减1，直接将补码减1即可） -3： 1111 1101（在-2补码基础上减1，以下类似） -4： 1111 1100 …… -127：1000 0001 -128：1000 0000 如此以来：8位补码表示范围是-128~+127因为0只有一种形式所以，仍然是256个数 若8位代表无符号数, 则表示范围是 : 0~255, 这就是为什么高级语言讲到数据类型， 正数的补码与原码相同，负数的补码为 其原码除符号位外所有位取反（得到反码了），然后最低位加1 ","date":"2020-05-30","objectID":"/argorithm/bit/:3:3","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"原码，反码，补码总结 正数的反码和补码都与原码相同。 负数的反码为对该数的原码除符号位外各位取反。 负数的补码为对该数的原码除符号位外各位取反，然后在最后一位加1　 优缺点: 原码最好理解了，但是加减法不够方便，还有两个零。。 反码稍微困难一些，解决了加减法的问题，但还是有有个零 补码理解困难，其他就没什么缺点了 ","date":"2020-05-30","objectID":"/argorithm/bit/:3:4","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"存储 计算机中的整数是用补码存储的，最高位为符号位 如果最高位为0则为正数，求值的时候，直接转为10进制即可。 最高位如果为1代表为负数，求值的时候，需要先把二进制的值按位取反，然后加1得到负数绝对值(相反数)的二进制码，然后转为10进制，加上负号即可。 ","date":"2020-05-30","objectID":"/argorithm/bit/:3:5","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"原码，反码，补码的应用 ","date":"2020-05-30","objectID":"/argorithm/bit/:3:6","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"负数的十进制和二进制转换 ","date":"2020-05-30","objectID":"/argorithm/bit/:4:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"十进制转二进制 方法为: 先转换为二进制 对二进制数求反 再将该二进制数加一 总而言之: 十进制数转换为二进制数求补码即为结果 例子 -32 转换为二进制 第一步：32（10）=00100000（2） 第二步：求反：11011111 第三步：加1:11100000 所以-32（10）=11100000（2） ","date":"2020-05-30","objectID":"/argorithm/bit/:4:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"二进制转十进制 方法为: 数值为取反 对该二进制加一 转换为10进制 例子 11001000 转换为十进制 第一步（数值位取反）： 10110111 第二步（加一）：10111000 第三步（十进制）：-56 所以11001000（2）=-56（10） ","date":"2020-05-30","objectID":"/argorithm/bit/:4:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"十进制数求反的规律 下面都是以10进制表示: ","date":"2020-05-30","objectID":"/argorithm/bit/:5:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"负数求反 负数求反等于其绝对值 -1 如: num = -5 num1 = ~num # 4 ","date":"2020-05-30","objectID":"/argorithm/bit/:5:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"正数求反 正数求反等于其值 +1 的负数 如: num = 4 num1 = ~num # -5 ","date":"2020-05-30","objectID":"/argorithm/bit/:5:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"二进制的应用场景 ","date":"2020-05-30","objectID":"/argorithm/bit/:6:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位操作实现乘除法 数 a 向右移一位，相当于将 a 除以 2；数 a 向左移一位，相当于将 a 乘以 2 a = 2 a \u003e\u003e 1 # ---\u003e 1 a \u003c\u003c 1 # ---\u003e 4 ","date":"2020-05-30","objectID":"/argorithm/bit/:6:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位操作交换两数 位操作交换两数可以不需要第三个临时变量，虽然普通操作也可以做到，但是没有其效率高 # 普通操作 def swap(a: int, b: int) -\u003e(int,int): a = a + b b = a - b a = a - b return a,b # 位与操作 def swap(a: int, b: int) -\u003e (int, int): \"\"\" 交换两个数 :param a: :param b: :return: \"\"\" a ^= b # a = (a^b) b ^= a # b = b ^ a = b ^ a ^ b a ^= b # a = a ^ b = a ^ a ^ b return a, b ","date":"2020-05-30","objectID":"/argorithm/bit/:6:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位操作判断奇偶数 只要根据数的最后一位是 0 还是 1 来决定即可，为 0 就是偶数，为 1 就是奇数 if(0 == (a \u0026 1)) { //偶数 } ","date":"2020-05-30","objectID":"/argorithm/bit/:6:3","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位操作交换符号 交换符号将正数变成负数，负数变成正数 func reversal(a int) int { return ^a + 1 } def reversal(a: int) -\u003e int: \"\"\" 求相反数 :param a: :return: \"\"\" return ~a + 1 正数取反加1，正好变成其对应的负数(补码表示)；负数取反加一，则变为其原码，即正数 ","date":"2020-05-30","objectID":"/argorithm/bit/:6:4","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位操作求绝对值 正数的绝对值是其本身，负数的绝对值正好可以对其进行取反加一求得，即我们首先判断其符号位（整数右移 31 位得到 0，负数右移 31 位得到 -1,即 0xffffffff），然后根据符号进行相应的操作 def abs(a: int) -\u003e int: i = a \u003e\u003e 31 result = a if i == 0 else ~a + 1 return result 上面的操作可以进行优化，可以将 i == 0 的条件判断语句去掉。我们都知道符号位 i 只有两种情况，即 i = 0 为正，i = -1 为负。对于任何数与 0 异或都会保持不变，与 -1 即 0xffffffff 进行异或就相当于对此数进行取反,因此可以将上面三目元算符转换为((a^i)-i)，即整数时 a 与 0 异或得到本身，再减去 0，负数时与 0xffffffff 异或将 a 进行取反，然后在加上 1，即减去 i(i =-1) def abs(a: int) -\u003e int: \"\"\" 求绝对值 :param a: :return: \"\"\" i = a \u003e\u003e 31 result = (a ^ i) - i return result or func abs(a int) int { i := a \u003e\u003e 31 return (a ^ i) - i } ","date":"2020-05-30","objectID":"/argorithm/bit/:6:5","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位操作进行高低位交换 给定一个 16 位的无符号整数，将其高 8 位与低 8 位进行交换，求出交换后的值，如 从上面移位操作我们可以知道，只要将无符号数 a»8 即可得到其高 8 位移到低 8 位，高位补 0；将 a « 8 即可将 低 8 位移到高 8 位，低 8 位补 0，然后将 a » 8 和 a«8 进行或操作既可求得交换后的结果 。 unsigned short a = 34520; a = (a \u003e\u003e 8) | (a \u003c\u003c 8); ","date":"2020-05-30","objectID":"/argorithm/bit/:6:6","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位操作统计二进制中 1 的个数 统计二进制1的个数可以分别获取每个二进制位数，然后再统计其1的个数，此方法效率比较低。 这里介绍另外一种高效的方法，同样以 34520 为例， 我们计算其 a \u0026= (a-1)的结果： 第一次：计算前：1000 0110 1101 1000 计算后：1000 0110 1101 0000 第二次：计算前：1000 0110 1101 0000 计算后：1000 0110 1100 0000 第三次：计算前：1000 0110 1100 0000 计算后：1000 0110 1000 0000 我们发现，每计算一次二进制中就少了一个 1，则我们可以通过下面方法去统计：count = 0 def count_1(a: int) -\u003e int: \"\"\" 计算数值的二进制表示的1的数量 :param a: :return: \"\"\" count = 0 while (a): a = a \u0026 a - 1 count += 1 return count ","date":"2020-05-30","objectID":"/argorithm/bit/:6:7","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"求和 两数求和 func add(a int, b int) int { for b != 0 { sum := a ^ b carry := (a \u0026 b) \u003c\u003c 1 a = sum b = carry } return a } ","date":"2020-05-30","objectID":"/argorithm/bit/:6:8","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"比特位计数 给定一个非负整数 num。对于 0 ≤ i ≤ num 范围中的每个数字 i ，计算其二进制数中的 1 的数目并将它们作为数组返回。 示例 1: 输入: 2 输出: [0,1,1] 示例 2: 输入: 5 输出: [0,1,1,2,1,2] def countBits(num: int) -\u003e [int]: result = [0] * (num + 1) for i in range(1, num + 1): result[i] = result[i \u0026 i - 1] + 1 return result func countBits(num int) []int { result := make([]int, num+1) for i := 1; i \u003c num+1 ; i ++ { result[i] = result[i \u0026 (i-1)] + 1 } return result } ","date":"2020-05-30","objectID":"/argorithm/bit/:6:9","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"常用的特殊的数 0xaaaaaaaa = 10101010101010101010101010101010 (偶数位为1，奇数位为0） 0x55555555 = 1010101010101010101010101010101 (偶数位为0，奇数位为1） 0x33333333 = 110011001100110011001100110011 (1和0每隔两位交替出现) 0xcccccccc = 11001100110011001100110011001100 (0和1每隔两位交替出现) 0x0f0f0f0f = 00001111000011110000111100001111 (1和0每隔四位交替出现) 0xf0f0f0f0 = 11110000111100001111000011110000 (0和1每隔四位交替出现) 0xffffffff = 11111111111111111111111111111111 ","date":"2020-05-30","objectID":"/argorithm/bit/:7:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/argorithm/bit/"},{"categories":null,"content":"位运算合集","date":"2020-05-30","objectID":"/bit/","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"位运算 计算机中的数在内存中都是以二进制形式进行存储的，用位运算就是直接对整数在内存中的二进制位进行操作，因此其执行效率非常高，在程序中尽量使用位运算进行操作，这会大大提高程序的性能。 ","date":"2020-05-30","objectID":"/bit/:0:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"位操作符 ","date":"2020-05-30","objectID":"/bit/:1:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"\u0026 与运算 \u0026 与运算 两个位都是 1 时，结果才为 1，否则为 0，如 1 0 0 1 1 \u0026 1 1 0 0 1 ------------------------------ 1 0 0 0 1 ","date":"2020-05-30","objectID":"/bit/:1:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"| 或运算 两个位都是 0 时，结果才为 0，否则为 1，如 1 0 0 1 1 | 1 1 0 0 1 ------------------------------ 1 1 0 1 1 ","date":"2020-05-30","objectID":"/bit/:1:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"^ 异或运算 两个位相同则为 0，不同则为 1，如 1 0 0 1 1 ^ 1 1 0 0 1 ----------------------------- 0 1 0 1 0 ","date":"2020-05-30","objectID":"/bit/:1:3","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"~ 取反运算 0 则变为 1，1 则变为 0，如 ~ 1 0 0 1 1 ----------------------------- 0 1 1 0 0 ","date":"2020-05-30","objectID":"/bit/:1:4","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"« 左移运算 向左进行移位操作，高位丢弃，低位补 0,如 int a = 8; a \u003c\u003c 3; 移位前：0000 0000 0000 0000 0000 0000 0000 1000 移位后：0000 0000 0000 0000 0000 0000 0100 0000 左移n为的值即为当前值*2^n, 如: a = 8 b = a\u003c\u003c3 # 64 c = a * (2 ** 3) # 64 ","date":"2020-05-30","objectID":"/bit/:1:5","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"»右移运算 向右进行移位操作，对无符号数，高位补 0，对于有符号数，高位补符号位，如 unsigned int a = 8; a \u003e\u003e 3; 移位前：0000 0000 0000 0000 0000 0000 0000 1000 移位后：0000 0000 0000 0000 0000 0000 0000 0001 ​ int a = -8; a \u003e\u003e 3; 移位前：1111 1111 1111 1111 1111 1111 1111 1000 移位前：1111 1111 1111 1111 1111 1111 1111 1111 ","date":"2020-05-30","objectID":"/bit/:1:6","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"有符号数和无符号数 ","date":"2020-05-30","objectID":"/bit/:2:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"有符号数 有符号数的定义是：字节的最高位作为符号位，其余的是数值位。例如一个字节中存储的二进制数为1100 1000，最高位1作为符号位，其余的7为 100 1000 作为数值为。 那么，符号位占据1位，就有0和1这样的两种数值，就有： 如果符号位为0，那么字节中存储的数值是正数 如果符号位为1，那么字节中存储的数值是负数 对于1100 1000这样的二进制数据，符号位是1，就表示负数。 在有符号数中，表示负数的算法是： 把数值位中存储的二进制数据，每个位都取反，就是原来为0的值变为1，原来为1的值变为0； 给对取反后的二进制数据加1，得到的数值就得到负数值； ","date":"2020-05-30","objectID":"/bit/:2:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"无符号数 无符号数的定义是：没有符号位，所有的位数都是数值位。所以表示的都是正数。 ","date":"2020-05-30","objectID":"/bit/:2:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"例子 例一 1100 1000这个数值，如果作为有符号数看待，那么符号位是1，数值位是100 1000。所以，符号位是1，所以，这个数据是负数。然后，表示成十进制时，对数值位的操作是： 数值位取反，得到011 0111； 对取反后的数值 011 0111加1得到011 1000，数值位的值为56； 那么，1100 1000这个二进制数据表示为“有符号数”时，就是-56这个数值。 如果作为无符号数看待，那么，就没有符号位，所有的位数都是数值位，所以11001000都作为数值位，表示的十进制数值是200 例二 例如，0111 0011这个数值，如果当做“有符号数”看待，那么，其符号位是0，所以，表示整数，数值位是115，所以，表示正115这个数值。如果当做无符号数看待，所有位都是数值位，计算得到115这个数值，所以，表示正115。所以我们可以总结 ","date":"2020-05-30","objectID":"/bit/:2:3","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"总结 无符号数，总是表示正数。所有位数都表示数值位。 有符号数，可以表示正数和负数，最高位是符号位，其余位都是数值位。如果符号位是0，则表示正数；如果符号位是1，则表示负数。对于负数的表示方法是：数值位全部取反，再加1，得到的数值就是负数值。 ","date":"2020-05-30","objectID":"/bit/:2:4","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"原码、反码、补码 ","date":"2020-05-30","objectID":"/bit/:3:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"原码 原码的表示范围-127~-0, +0~+127, 共256个数字 正0的原码是0000 0000, 负0的原码是1000 0000, 有正0负0之分, 不符合人的习惯, 待解决. 原码有几个缺点，零分两种 +0 和 -0 。还有，在进行不同符号的加法运算或者同符号的减法运算的时候，不能直接判断出结果的正负。你需要将两个值的绝对值进行比较，然后进行加减操作 ，最后符号位由绝对值大的决定。于是反码就产生了。 ","date":"2020-05-30","objectID":"/bit/:3:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"反码 除符号位, 原码其余位取反而得 +0：0000 0000，-0：1111 1111 仍然有正0负0之分。 正数的反码就是原码，负数的反码等于原码除符号位以外所有的位取反 举例说明： int类型的 3 的反码是 00000000 00000000 00000000 00000011 和原码一样没什么可说的 int类型的 -3 的反码是 11111111 11111111 11111111 11111100 除开符号位 所有位 取反 解决了加减运算的问题，但还是有正负零之分，然后就到补码了 ","date":"2020-05-30","objectID":"/bit/:3:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"补码 在反码的基础上加1而得 对原码的两种0同时末位加1 +0：0000 0000，-0：0000 0000(因为溢出导致8位全0) 消除了正0负0之别, 如此一来, 便节省出一个数值表示方式1000 0000, 不能浪费, 用来表示-128, -128特殊之处在于没有相应的反码原码。也可以这样考虑: -1： 1111 1111 -2： 1111 1110（在-1的基础上减1，直接将补码减1即可） -3： 1111 1101（在-2补码基础上减1，以下类似） -4： 1111 1100 …… -127：1000 0001 -128：1000 0000 如此以来：8位补码表示范围是-128~+127因为0只有一种形式所以，仍然是256个数 若8位代表无符号数, 则表示范围是 : 0~255, 这就是为什么高级语言讲到数据类型， 正数的补码与原码相同，负数的补码为 其原码除符号位外所有位取反（得到反码了），然后最低位加1 ","date":"2020-05-30","objectID":"/bit/:3:3","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"原码，反码，补码总结 正数的反码和补码都与原码相同。 负数的反码为对该数的原码除符号位外各位取反。 负数的补码为对该数的原码除符号位外各位取反，然后在最后一位加1　 优缺点: 原码最好理解了，但是加减法不够方便，还有两个零。。 反码稍微困难一些，解决了加减法的问题，但还是有有个零 补码理解困难，其他就没什么缺点了 ","date":"2020-05-30","objectID":"/bit/:3:4","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"存储 计算机中的整数是用补码存储的，最高位为符号位 如果最高位为0则为正数，求值的时候，直接转为10进制即可。 最高位如果为1代表为负数，求值的时候，需要先把二进制的值按位取反，然后加1得到负数绝对值(相反数)的二进制码，然后转为10进制，加上负号即可。 ","date":"2020-05-30","objectID":"/bit/:3:5","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"原码，反码，补码的应用 ","date":"2020-05-30","objectID":"/bit/:3:6","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"负数的十进制和二进制转换 ","date":"2020-05-30","objectID":"/bit/:4:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"十进制转二进制 方法为: 先转换为二进制 对二进制数求反 再将该二进制数加一 总而言之: 十进制数转换为二进制数求补码即为结果 例子 -32 转换为二进制 第一步：32（10）=00100000（2） 第二步：求反：11011111 第三步：加1:11100000 所以-32（10）=11100000（2） ","date":"2020-05-30","objectID":"/bit/:4:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"二进制转十进制 方法为: 数值为取反 对该二进制加一 转换为10进制 例子 11001000 转换为十进制 第一步（数值位取反）： 10110111 第二步（加一）：10111000 第三步（十进制）：-56 所以11001000（2）=-56（10） ","date":"2020-05-30","objectID":"/bit/:4:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"十进制数求反的规律 下面都是以10进制表示: ","date":"2020-05-30","objectID":"/bit/:5:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"负数求反 负数求反等于其绝对值 -1 如: num = -5 num1 = ~num # 4 ","date":"2020-05-30","objectID":"/bit/:5:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"正数求反 正数求反等于其值 +1 的负数 如: num = 4 num1 = ~num # -5 ","date":"2020-05-30","objectID":"/bit/:5:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"二进制的应用场景 ","date":"2020-05-30","objectID":"/bit/:6:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"位操作实现乘除法 数 a 向右移一位，相当于将 a 除以 2；数 a 向左移一位，相当于将 a 乘以 2 a = 2 a \u003e\u003e 1 # ---\u003e 1 a \u003c\u003c 1 # ---\u003e 4 ","date":"2020-05-30","objectID":"/bit/:6:1","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"位操作交换两数 位操作交换两数可以不需要第三个临时变量，虽然普通操作也可以做到，但是没有其效率高 # 普通操作 def swap(a: int, b: int) -\u003e(int,int): a = a + b b = a - b a = a - b return a,b # 位与操作 def swap(a: int, b: int) -\u003e (int, int): \"\"\" 交换两个数 :param a: :param b: :return: \"\"\" a ^= b # a = (a^b) b ^= a # b = b ^ a = b ^ a ^ b a ^= b # a = a ^ b = a ^ a ^ b return a, b ","date":"2020-05-30","objectID":"/bit/:6:2","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"位操作判断奇偶数 只要根据数的最后一位是 0 还是 1 来决定即可，为 0 就是偶数，为 1 就是奇数 if(0 == (a \u0026 1)) { //偶数 } ","date":"2020-05-30","objectID":"/bit/:6:3","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"位操作交换符号 交换符号将正数变成负数，负数变成正数 func reversal(a int) int { return ^a + 1 } def reversal(a: int) -\u003e int: \"\"\" 求相反数 :param a: :return: \"\"\" return ~a + 1 正数取反加1，正好变成其对应的负数(补码表示)；负数取反加一，则变为其原码，即正数 ","date":"2020-05-30","objectID":"/bit/:6:4","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"位操作求绝对值 正数的绝对值是其本身，负数的绝对值正好可以对其进行取反加一求得，即我们首先判断其符号位（整数右移 31 位得到 0，负数右移 31 位得到 -1,即 0xffffffff），然后根据符号进行相应的操作 def abs(a: int) -\u003e int: i = a \u003e\u003e 31 result = a if i == 0 else ~a + 1 return result 上面的操作可以进行优化，可以将 i == 0 的条件判断语句去掉。我们都知道符号位 i 只有两种情况，即 i = 0 为正，i = -1 为负。对于任何数与 0 异或都会保持不变，与 -1 即 0xffffffff 进行异或就相当于对此数进行取反,因此可以将上面三目元算符转换为((a^i)-i)，即整数时 a 与 0 异或得到本身，再减去 0，负数时与 0xffffffff 异或将 a 进行取反，然后在加上 1，即减去 i(i =-1) def abs(a: int) -\u003e int: \"\"\" 求绝对值 :param a: :return: \"\"\" i = a \u003e\u003e 31 result = (a ^ i) - i return result or func abs(a int) int { i := a \u003e\u003e 31 return (a ^ i) - i } ","date":"2020-05-30","objectID":"/bit/:6:5","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"位操作进行高低位交换 给定一个 16 位的无符号整数，将其高 8 位与低 8 位进行交换，求出交换后的值，如 从上面移位操作我们可以知道，只要将无符号数 a»8 即可得到其高 8 位移到低 8 位，高位补 0；将 a « 8 即可将 低 8 位移到高 8 位，低 8 位补 0，然后将 a » 8 和 a«8 进行或操作既可求得交换后的结果 。 unsigned short a = 34520; a = (a \u003e\u003e 8) | (a \u003c\u003c 8); ","date":"2020-05-30","objectID":"/bit/:6:6","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"位操作统计二进制中 1 的个数 统计二进制1的个数可以分别获取每个二进制位数，然后再统计其1的个数，此方法效率比较低。 这里介绍另外一种高效的方法，同样以 34520 为例， 我们计算其 a \u0026= (a-1)的结果： 第一次：计算前：1000 0110 1101 1000 计算后：1000 0110 1101 0000 第二次：计算前：1000 0110 1101 0000 计算后：1000 0110 1100 0000 第三次：计算前：1000 0110 1100 0000 计算后：1000 0110 1000 0000 我们发现，每计算一次二进制中就少了一个 1，则我们可以通过下面方法去统计：count = 0 def count_1(a: int) -\u003e int: \"\"\" 计算数值的二进制表示的1的数量 :param a: :return: \"\"\" count = 0 while (a): a = a \u0026 a - 1 count += 1 return count ","date":"2020-05-30","objectID":"/bit/:6:7","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"求和 两数求和 func add(a int, b int) int { for b != 0 { sum := a ^ b carry := (a \u0026 b) \u003c\u003c 1 a = sum b = carry } return a } ","date":"2020-05-30","objectID":"/bit/:6:8","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"比特位计数 给定一个非负整数 num。对于 0 ≤ i ≤ num 范围中的每个数字 i ，计算其二进制数中的 1 的数目并将它们作为数组返回。 示例 1: 输入: 2 输出: [0,1,1] 示例 2: 输入: 5 输出: [0,1,1,2,1,2] def countBits(num: int) -\u003e [int]: result = [0] * (num + 1) for i in range(1, num + 1): result[i] = result[i \u0026 i - 1] + 1 return result func countBits(num int) []int { result := make([]int, num+1) for i := 1; i \u003c num+1 ; i ++ { result[i] = result[i \u0026 (i-1)] + 1 } return result } ","date":"2020-05-30","objectID":"/bit/:6:9","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"常用的特殊的数 0xaaaaaaaa = 10101010101010101010101010101010 (偶数位为1，奇数位为0） 0x55555555 = 1010101010101010101010101010101 (偶数位为0，奇数位为1） 0x33333333 = 110011001100110011001100110011 (1和0每隔两位交替出现) 0xcccccccc = 11001100110011001100110011001100 (0和1每隔两位交替出现) 0x0f0f0f0f = 00001111000011110000111100001111 (1和0每隔四位交替出现) 0xf0f0f0f0 = 11110000111100001111000011110000 (0和1每隔四位交替出现) 0xffffffff = 11111111111111111111111111111111 ","date":"2020-05-30","objectID":"/bit/:7:0","tags":["算法","位运算","bit"],"title":"位运算","uri":"/bit/"},{"categories":null,"content":"二叉搜索树","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"定义及特点 二叉查找树（英语：Binary Search Tree），也称为 二叉搜索树、有序二叉树（Ordered Binary Tree）或排序二叉树（Sorted Binary Tree），是指一棵空树或者具有下列性质的二叉树： 若任意节点的左子树不空，则左子树上所有节点的值均小于它的根节点的值； 若任意节点的右子树不空，则右子树上所有节点的值均大于它的根节点的值； 任意节点的左、右子树也分别为二叉查找树； 没有键值相等的节点。 二叉查找树相比于其他数据结构的优势在于查找、插入的时间复杂度较低。为 O(logn)。二叉查找树是基础性数据结构，用于构建更为抽象的数据结构，如集合、多重集、关联数组等。 二叉查找树的查找过程和次优二叉树类似，通常采取二叉链表作为二叉查找树的存储结构。中序遍历二叉查找树可得到一个关键字的有序序列，一个无序序列可以通过构造一棵二叉查找树变成一个有序序列，构造树的过程即为对无序序列进行查找的过程。每次插入的新的结点都是二叉查找树上新的叶子结点，在进行插入操作时，不必移动其它结点，只需改动某个结点的指针，由空变为非空即可。搜索、插入、删除的复杂度等于树高，期望 O(\\log n)O(logn)，最坏 O(n)O(n)（数列有序，树退化成线性表）。 虽然二叉查找树的最坏效率是 O(n)O(n)，但它支持动态查询，且有很多改进版的二叉查找树可以使树高为 O(\\log n)O(logn)，从而将最坏效率降至 O(\\log n)O(logn)，如 AVL 树、红黑树等。 ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:1:0","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"常用操作 ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:2:0","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"树节点定义: class TreeNode: def __init__(self, val): self.val = val self.left = None self.right = None or type TreeNode struct { Val int Left *TreeNode Right *TreeNode } ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:2:1","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"查找 在二叉搜索树b中查找x的过程为： 若b是空树，则搜索失败，否则： 若x等于b的根节点的数据域之值，则查找成功；否则： 若x小于b的根节点的数据域之值，则递归搜索左子树；否则: 递归查找右子树 ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:2:2","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"插入 向一个二叉搜索树b中插入一个节点s的算法，过程为： 若b是空树，则将s所指结点作为根节点插入，否则： 若s.val等于b的根节点的数据域之值，则返回，否则： 若s.val小于b的根节点的数据域之值，则把s所指节点插入到左子树中，否则： 把s所指节点插入到右子树中（新插入节点总是叶子节点） ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:2:3","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"删除 二叉搜索树的删除操作分三种情况讨论: 如果待删除的节点是叶子节点，那么可以立即被删除，如下图所示： 例：删除数据为16的节点，是叶子节点，可以直接删除 如果有一个子节点，要将下一个子节点上移到当前节点，即替换之 例：删除数据为25的节点，它下面有唯一一个子节点35, 上移到替换之 如果有两个子节点，则将其右子树的最小数据代替此节点的数据，并将其右子树的最小数据删除，如下图所示 例：删除节点数据为5的节点，找到被删除节点右子树的最小节点。需要一个临时变量successor，将11节点下面的子节点进行查询，找到右子树最小节点7，并把右子树最小节点7替换被删除节点，维持二叉树结构。如下图 ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:2:4","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"遍历 可以采用前序，中序，后序来遍历该二叉搜索树，或者使用广度优先搜索的方式。这里用中序遍历来实现，可以保证按从小到大的顺序打印。 ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:2:5","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"构造一颗二叉查找树 用一组数值建造一棵二叉查找树的同时，也把这组数值进行了排序。其最差时间复杂度为 O(n2)。例如，若该组数值经是有序的（从小到大），则建造出来的二叉查找树的所有节点，都没有左子树 ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:2:6","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"常用操作的实现 python版 # 节点定义 class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None # 查找 def search(root: TreeNode, val: int) -\u003e (bool, TreeNode): if root == None: return False, None elif val \u003e root.val: return search(root.right, val) elif val \u003c root.val: return search(root.left, val) else: return True, root # 插入 def insert(root: TreeNode, node: TreeNode) -\u003e TreeNode: \"\"\"insert inplace\"\"\" if root == None: root = node return root elif node.val \u003e root.val: root.right = insert(root.right, node) else: root.left = insert(root.left, node) return root # 删除 def deleteNode(root: TreeNode, key: int) -\u003e TreeNode: \"\"\" :type root: TreeNode :type key: int :rtype: TreeNode \"\"\" if root == None: return None if key \u003c root.val: root.left = deleteNode(root.left, key) elif key \u003e root.val: root.right = deleteNode(root.right, key) else: if root.left == None: return root.right elif root.right == None: return root.left else: min_node = findMinNode(root.right) root.val = min_node.val root.right = deleteNode(root.right, root.val) return root def findMinNode(node: TreeNode) -\u003e TreeNode: while node.left: node = node.left return node # 中序遍历 def traverse_binary_tree(root: TreeNode): if root is None: return traverse_binary_tree(root.left) print(root.val) traverse_binary_tree(root.right) # 构建二叉树 def build_binary_tree(values: [int]): tree = None for v in values: tree = insert(tree, TreeNode(v)) return tree if __name__ == \"__main__\": values = [17, 5, 35, 2, 11, 29, 38, 9, 16, 7] # 构造二叉树 node = build_binary_tree(values) # 查找 node_7 = search(node, 35) # 遍历 traverse_binary_tree(node) # 删除 a = deleteNode(node, 5) print() golang版: package main import \"fmt\" type TreeNode struct { Val int Left *TreeNode Right *TreeNode } func main() { values := []int{17, 5, 35, 2, 11, 29, 38, 9, 16, 7} // 测试构造二叉树 node := buildBinarySearchTree(values) // 遍历 traverseBinarySearchTree(node) // 搜索 ok, child := search(node, 11) // 删除 new_node := deleteTreenode(node, 35) fmt.Println(new_node) fmt.Println(ok, child) } // 查找 func search(root *TreeNode, val int) (bool, *TreeNode) { if root == nil { return false, nil } if root.Val == val { return true, root } else if root.Val \u003c val { return search(root.Right, val) } else { return search(root.Left, val) } } // 插入 func insert(root, node *TreeNode) *TreeNode { if root == nil { root = node return root } if root.Val \u003e node.Val { root.Left = insert(root.Left, node) } else { root.Right = insert(root.Right, node) } return root } // 删除 func deleteTreenode(root *TreeNode, val int) *TreeNode { if root == nil { return nil } if root.Val \u003e val { root.Left = deleteTreenode(root.Left, val) } else if root.Val \u003c val { root.Right = deleteTreenode(root.Right, val) } else { if root.Left == nil { return root.Right } else if root.Right == nil { return root.Left } else { min_node := findMinNode(root.Right) root.Val = min_node.Val root.Right = deleteTreenode(root.Right, min_node.Val) } } return root } func findMinNode(root *TreeNode) *TreeNode { for root.Left != nil { root = root.Left } return root } // 中序遍历 func traverseBinarySearchTree(root *TreeNode) { if root == nil { return } traverseBinarySearchTree(root.Left) fmt.Println(root.Val) traverseBinarySearchTree(root.Right) } // 构建二叉搜索树 func buildBinarySearchTree(values []int) *TreeNode { var node *TreeNode = nil for _, value := range values { node = insert(node, \u0026TreeNode{Val: value}) } return node } ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:3:0","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"性能分析 查找：最佳情况Olog(n), 最坏情况O(n) 插入：最佳情况Olog(n), 最坏情况O(n) 删除：最佳情况Olog(n), 最坏情况O(n) ","date":"2020-05-05","objectID":"/argorithm/binarysearchtree/:4:0","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/argorithm/binarysearchtree/"},{"categories":null,"content":"二叉搜索树","date":"2020-05-05","objectID":"/binarysearchtree/","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"定义及特点 二叉查找树（英语：Binary Search Tree），也称为 二叉搜索树、有序二叉树（Ordered Binary Tree）或排序二叉树（Sorted Binary Tree），是指一棵空树或者具有下列性质的二叉树： 若任意节点的左子树不空，则左子树上所有节点的值均小于它的根节点的值； 若任意节点的右子树不空，则右子树上所有节点的值均大于它的根节点的值； 任意节点的左、右子树也分别为二叉查找树； 没有键值相等的节点。 二叉查找树相比于其他数据结构的优势在于查找、插入的时间复杂度较低。为 O(logn)。二叉查找树是基础性数据结构，用于构建更为抽象的数据结构，如集合、多重集、关联数组等。 二叉查找树的查找过程和次优二叉树类似，通常采取二叉链表作为二叉查找树的存储结构。中序遍历二叉查找树可得到一个关键字的有序序列，一个无序序列可以通过构造一棵二叉查找树变成一个有序序列，构造树的过程即为对无序序列进行查找的过程。每次插入的新的结点都是二叉查找树上新的叶子结点，在进行插入操作时，不必移动其它结点，只需改动某个结点的指针，由空变为非空即可。搜索、插入、删除的复杂度等于树高，期望 O(\\log n)O(logn)，最坏 O(n)O(n)（数列有序，树退化成线性表）。 虽然二叉查找树的最坏效率是 O(n)O(n)，但它支持动态查询，且有很多改进版的二叉查找树可以使树高为 O(\\log n)O(logn)，从而将最坏效率降至 O(\\log n)O(logn)，如 AVL 树、红黑树等。 ","date":"2020-05-05","objectID":"/binarysearchtree/:1:0","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"常用操作 ","date":"2020-05-05","objectID":"/binarysearchtree/:2:0","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"树节点定义: class TreeNode: def __init__(self, val): self.val = val self.left = None self.right = None or type TreeNode struct { Val int Left *TreeNode Right *TreeNode } ","date":"2020-05-05","objectID":"/binarysearchtree/:2:1","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"查找 在二叉搜索树b中查找x的过程为： 若b是空树，则搜索失败，否则： 若x等于b的根节点的数据域之值，则查找成功；否则： 若x小于b的根节点的数据域之值，则递归搜索左子树；否则: 递归查找右子树 ","date":"2020-05-05","objectID":"/binarysearchtree/:2:2","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"插入 向一个二叉搜索树b中插入一个节点s的算法，过程为： 若b是空树，则将s所指结点作为根节点插入，否则： 若s.val等于b的根节点的数据域之值，则返回，否则： 若s.val小于b的根节点的数据域之值，则把s所指节点插入到左子树中，否则： 把s所指节点插入到右子树中（新插入节点总是叶子节点） ","date":"2020-05-05","objectID":"/binarysearchtree/:2:3","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"删除 二叉搜索树的删除操作分三种情况讨论: 如果待删除的节点是叶子节点，那么可以立即被删除，如下图所示： 例：删除数据为16的节点，是叶子节点，可以直接删除 如果有一个子节点，要将下一个子节点上移到当前节点，即替换之 例：删除数据为25的节点，它下面有唯一一个子节点35, 上移到替换之 如果有两个子节点，则将其右子树的最小数据代替此节点的数据，并将其右子树的最小数据删除，如下图所示 例：删除节点数据为5的节点，找到被删除节点右子树的最小节点。需要一个临时变量successor，将11节点下面的子节点进行查询，找到右子树最小节点7，并把右子树最小节点7替换被删除节点，维持二叉树结构。如下图 ","date":"2020-05-05","objectID":"/binarysearchtree/:2:4","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"遍历 可以采用前序，中序，后序来遍历该二叉搜索树，或者使用广度优先搜索的方式。这里用中序遍历来实现，可以保证按从小到大的顺序打印。 ","date":"2020-05-05","objectID":"/binarysearchtree/:2:5","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"构造一颗二叉查找树 用一组数值建造一棵二叉查找树的同时，也把这组数值进行了排序。其最差时间复杂度为 O(n2)。例如，若该组数值经是有序的（从小到大），则建造出来的二叉查找树的所有节点，都没有左子树 ","date":"2020-05-05","objectID":"/binarysearchtree/:2:6","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"常用操作的实现 python版 # 节点定义 class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None # 查找 def search(root: TreeNode, val: int) -\u003e (bool, TreeNode): if root == None: return False, None elif val \u003e root.val: return search(root.right, val) elif val \u003c root.val: return search(root.left, val) else: return True, root # 插入 def insert(root: TreeNode, node: TreeNode) -\u003e TreeNode: \"\"\"insert inplace\"\"\" if root == None: root = node return root elif node.val \u003e root.val: root.right = insert(root.right, node) else: root.left = insert(root.left, node) return root # 删除 def deleteNode(root: TreeNode, key: int) -\u003e TreeNode: \"\"\" :type root: TreeNode :type key: int :rtype: TreeNode \"\"\" if root == None: return None if key \u003c root.val: root.left = deleteNode(root.left, key) elif key \u003e root.val: root.right = deleteNode(root.right, key) else: if root.left == None: return root.right elif root.right == None: return root.left else: min_node = findMinNode(root.right) root.val = min_node.val root.right = deleteNode(root.right, root.val) return root def findMinNode(node: TreeNode) -\u003e TreeNode: while node.left: node = node.left return node # 中序遍历 def traverse_binary_tree(root: TreeNode): if root is None: return traverse_binary_tree(root.left) print(root.val) traverse_binary_tree(root.right) # 构建二叉树 def build_binary_tree(values: [int]): tree = None for v in values: tree = insert(tree, TreeNode(v)) return tree if __name__ == \"__main__\": values = [17, 5, 35, 2, 11, 29, 38, 9, 16, 7] # 构造二叉树 node = build_binary_tree(values) # 查找 node_7 = search(node, 35) # 遍历 traverse_binary_tree(node) # 删除 a = deleteNode(node, 5) print() golang版: package main import \"fmt\" type TreeNode struct { Val int Left *TreeNode Right *TreeNode } func main() { values := []int{17, 5, 35, 2, 11, 29, 38, 9, 16, 7} // 测试构造二叉树 node := buildBinarySearchTree(values) // 遍历 traverseBinarySearchTree(node) // 搜索 ok, child := search(node, 11) // 删除 new_node := deleteTreenode(node, 35) fmt.Println(new_node) fmt.Println(ok, child) } // 查找 func search(root *TreeNode, val int) (bool, *TreeNode) { if root == nil { return false, nil } if root.Val == val { return true, root } else if root.Val \u003c val { return search(root.Right, val) } else { return search(root.Left, val) } } // 插入 func insert(root, node *TreeNode) *TreeNode { if root == nil { root = node return root } if root.Val \u003e node.Val { root.Left = insert(root.Left, node) } else { root.Right = insert(root.Right, node) } return root } // 删除 func deleteTreenode(root *TreeNode, val int) *TreeNode { if root == nil { return nil } if root.Val \u003e val { root.Left = deleteTreenode(root.Left, val) } else if root.Val \u003c val { root.Right = deleteTreenode(root.Right, val) } else { if root.Left == nil { return root.Right } else if root.Right == nil { return root.Left } else { min_node := findMinNode(root.Right) root.Val = min_node.Val root.Right = deleteTreenode(root.Right, min_node.Val) } } return root } func findMinNode(root *TreeNode) *TreeNode { for root.Left != nil { root = root.Left } return root } // 中序遍历 func traverseBinarySearchTree(root *TreeNode) { if root == nil { return } traverseBinarySearchTree(root.Left) fmt.Println(root.Val) traverseBinarySearchTree(root.Right) } // 构建二叉搜索树 func buildBinarySearchTree(values []int) *TreeNode { var node *TreeNode = nil for _, value := range values { node = insert(node, \u0026TreeNode{Val: value}) } return node } ","date":"2020-05-05","objectID":"/binarysearchtree/:3:0","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"性能分析 查找：最佳情况Olog(n), 最坏情况O(n) 插入：最佳情况Olog(n), 最坏情况O(n) 删除：最佳情况Olog(n), 最坏情况O(n) ","date":"2020-05-05","objectID":"/binarysearchtree/:4:0","tags":["算法","binarySearchTree","二叉搜索树"],"title":"二叉搜索树","uri":"/binarysearchtree/"},{"categories":null,"content":"How to run jenkins on kubernetes","date":"2020-02-08","objectID":"/opensrouce/k8s_jenkins/","tags":["kubernetes","jenkins"],"title":"How to run jenkins on kubernetes","uri":"/opensrouce/k8s_jenkins/"},{"categories":null,"content":"作用 如何在Kubernetes环境中运行jenkins ","date":"2020-02-08","objectID":"/opensrouce/k8s_jenkins/:1:0","tags":["kubernetes","jenkins"],"title":"How to run jenkins on kubernetes","uri":"/opensrouce/k8s_jenkins/"},{"categories":null,"content":"项目地址 https://github.com/russellgao/k8s_jenkins ","date":"2020-02-08","objectID":"/opensrouce/k8s_jenkins/:2:0","tags":["kubernetes","jenkins"],"title":"How to run jenkins on kubernetes","uri":"/opensrouce/k8s_jenkins/"},{"categories":null,"content":"参考 https://mp.weixin.qq.com/s/7YFlmcUH5iOB2XOBIZ2_rA ","date":"2020-02-08","objectID":"/opensrouce/k8s_jenkins/:3:0","tags":["kubernetes","jenkins"],"title":"How to run jenkins on kubernetes","uri":"/opensrouce/k8s_jenkins/"},{"categories":null,"content":"在k8s 中运行EKL","date":"2020-01-04","objectID":"/opensrouce/k8s_elk/","tags":["kubernetes","ELK"],"title":"ELK stack on kubernetes","uri":"/opensrouce/k8s_elk/"},{"categories":null,"content":"作用 如何在Kubernetes环境中运行ELK Stack ","date":"2020-01-04","objectID":"/opensrouce/k8s_elk/:1:0","tags":["kubernetes","ELK"],"title":"ELK stack on kubernetes","uri":"/opensrouce/k8s_elk/"},{"categories":null,"content":"项目地址 https://github.com/russellgao/k8s_elk ","date":"2020-01-04","objectID":"/opensrouce/k8s_elk/:2:0","tags":["kubernetes","ELK"],"title":"ELK stack on kubernetes","uri":"/opensrouce/k8s_elk/"},{"categories":null,"content":"用法 manifests Can run in production environment experimental In the experimental stage 使用之前需要修改各个yaml文件的Storage，Service相关的参数，根据实际情况选择合适的介质，修改完之后执行kubectl -f manifests 即可。 详细信息参考 github ","date":"2020-01-04","objectID":"/opensrouce/k8s_elk/:3:0","tags":["kubernetes","ELK"],"title":"ELK stack on kubernetes","uri":"/opensrouce/k8s_elk/"},{"categories":null,"content":"支持的组件 es logstash kibana kafka zookeeper ","date":"2020-01-04","objectID":"/opensrouce/k8s_elk/:4:0","tags":["kubernetes","ELK"],"title":"ELK stack on kubernetes","uri":"/opensrouce/k8s_elk/"},{"categories":null,"content":"More https://mp.weixin.qq.com/s/93_Jf8P69Q0nkw1Ip7MsFQ ","date":"2020-01-04","objectID":"/opensrouce/k8s_elk/:5:0","tags":["kubernetes","ELK"],"title":"ELK stack on kubernetes","uri":"/opensrouce/k8s_elk/"}]